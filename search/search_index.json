{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"HydraFlow Seamlessly integrate Hydra and MLflow to streamline machine learning experiment workflows.        Getting Started             View on GitHub"},{"location":"#what-is-hydraflow","title":"What is HydraFlow?","text":"<p>HydraFlow seamlessly integrates Hydra and MLflow to create a comprehensive machine learning experiment management framework. It provides a complete workflow from defining experiments to execution and analysis, streamlining machine learning projects from research to production.</p>"},{"location":"#key-integration-features","title":"Key Integration Features","text":"<ul> <li> Automatic Configuration Tracking: Hydra configurations are automatically   saved as MLflow artifacts, ensuring complete reproducibility of experiments.</li> <li> Type-safe Configuration: Leverage Python dataclasses for type-safe   experiment configuration with full IDE support.</li> <li> Unified Workflow: Connect configuration management and experiment tracking   in a single, coherent workflow.</li> <li> Powerful Analysis Tools: Analyze and compare experiments using   configuration parameters captured from Hydra.</li> </ul>"},{"location":"#where-to-go-next","title":"Where to go next?","text":"<ul> <li> <p> Getting Started</p> <p>Install HydraFlow and learn the core concepts and design principles to get you up and running quickly.</p> <p> Learn the basics</p> </li> <li> <p> Practical Tutorials</p> <p>Follow hands-on examples to understand HydraFlow in practice and learn to create your first application.</p> <p> Try the tutorials</p> </li> <li> <p> User Guide</p> <p>Dive deep into the features of HydraFlow, from running applications to automating workflows and analyzing results.</p> <p> Read the guide</p> </li> </ul>"},{"location":"getting-started/","title":"Getting Started with HydraFlow","text":"<p>Welcome to HydraFlow, a framework designed to streamline machine learning workflows by integrating Hydra's configuration management with MLflow's experiment tracking capabilities.</p>"},{"location":"getting-started/#overview","title":"Overview","text":"<p>This section provides everything you need to begin using HydraFlow effectively:</p> <ul> <li>Installation: Step-by-step instructions for installing   HydraFlow and its dependencies</li> <li>Core Concepts: An introduction to the fundamental concepts   that underpin HydraFlow's design and functionality</li> </ul>"},{"location":"getting-started/#why-hydraflow","title":"Why HydraFlow?","text":"<p>Managing machine learning experiments involves numerous challenges, including:</p> <ul> <li>Configuration Management: Tracking hyperparameters and settings across   multiple experiment runs</li> <li>Reproducibility: Ensuring experiments can be reliably reproduced</li> <li>Result Analysis: Efficiently comparing and analyzing experiment outcomes</li> <li>Workflow Automation: Organizing and managing experiment workflows</li> </ul> <p>HydraFlow addresses these challenges by providing:</p> <ol> <li>Type-safe Configuration: Using Python's native dataclasses for    robust configuration management</li> <li>Seamless Integration: Bridging Hydra and MLflow to combine their    respective strengths</li> <li>Analysis Tools: Providing powerful APIs for filtering, grouping,    and analyzing results</li> <li>Workflow Automation: Simplifying the organization and execution of    machine learning experiments</li> </ol>"},{"location":"getting-started/#quick-example","title":"Quick Example","text":"<p>Here's a simple example to demonstrate HydraFlow's basic usage:</p> <pre><code>from dataclasses import dataclass\nfrom mlflow.entities import Run\nimport hydraflow\n\n@dataclass\nclass Config:\n    learning_rate: float = 0.01\n    batch_size: int = 32\n    epochs: int = 10\n\n@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Your training code here\n    print(f\"Training with lr={cfg.learning_rate}, batch_size={cfg.batch_size}\")\n\n    # Log metrics\n    hydraflow.log_metric(\"accuracy\", 0.95)\n\nif __name__ == \"__main__\":\n    train()\n</code></pre> <p>Run this example with:</p> <pre><code>python train.py learning_rate=0.001 batch_size=64\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>After installing HydraFlow and understanding its core concepts, you're ready to:</p> <ol> <li>Follow our Practical Tutorials to see HydraFlow in action</li> <li>Explore the detailed User Guide to learn more about HydraFlow's capabilities</li> <li>Check the API Reference for detailed documentation of HydraFlow's API</li> </ol> <p>Continue to the Installation Guide to get started with HydraFlow.</p>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":"<p>This page introduces the fundamental concepts of HydraFlow that form the foundation of the framework.</p>"},{"location":"getting-started/concepts/#design-principles","title":"Design Principles","text":"<p>HydraFlow is built on the following design principles:</p> <ol> <li>Type Safety - Utilizing Python dataclasses for configuration     type checking and IDE support</li> <li>Reproducibility - Automatically tracking all experiment configurations     for fully reproducible experiments</li> <li>Workflow Integration - Creating a cohesive workflow by integrating     Hydra's configuration management with MLflow's experiment tracking</li> <li>Analysis Capabilities - Providing powerful APIs for easily     analyzing experiment results</li> </ol>"},{"location":"getting-started/concepts/#key-components","title":"Key Components","text":"<p>HydraFlow consists of the following key components:</p>"},{"location":"getting-started/concepts/#configuration-management","title":"Configuration Management","text":"<p>HydraFlow uses a hierarchical configuration system based on OmegaConf and Hydra. This provides:</p> <ul> <li>Type-safe configuration using Python dataclasses</li> <li>Schema validation to ensure configuration correctness</li> <li>Configuration composition from multiple sources</li> <li>Command-line overrides</li> </ul> <p>Example configuration:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Config:\n    learning_rate: float = 0.001\n    batch_size: int = 32\n    epochs: int = 10\n</code></pre> <p>This configuration class defines the structure and default values for your experiment, enabling type checking and auto-completion.</p>"},{"location":"getting-started/concepts/#main-decorator","title":"Main Decorator","text":"<p>The <code>@hydraflow.main</code> decorator defines the entry point for a HydraFlow application:</p> <pre><code>import hydraflow\nfrom mlflow.entities import Run\n\n@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Your experiment code\n    print(f\"Training with lr={cfg.learning_rate}, batch_size={cfg.batch_size}\")\n\n    # Log metrics\n    hydraflow.log_metric(\"accuracy\", 0.95)\n</code></pre> <p>This decorator provides:</p> <ul> <li>Automatic registration of your config class with Hydra's <code>ConfigStore</code></li> <li>Automatic setup of an MLflow experiment (including tracking URI if provided)</li> <li>Storage of Hydra configurations and logs as MLflow artifacts</li> <li>Support for type-safe APIs and IDE integration</li> </ul>"},{"location":"getting-started/concepts/#workflow-automation","title":"Workflow Automation","text":"<p>HydraFlow allows you to automate experiment workflows using a YAML-based job definition system:</p> <pre><code>jobs:\n  train_models:\n    run: python train.py\n    sets:\n      - each: model=small,medium,large\n        all: learning_rate=0.001,0.01,0.1\n</code></pre> <p>This enables:</p> <ul> <li>Defining reusable experiment workflows</li> <li>Efficient configuration of parameter sweeps</li> <li>Organization of complex experiment campaigns</li> </ul> <p>You can also define more complex parameter spaces using extended sweep syntax:</p> <pre><code># Ranges (start:end:step)\npython train.py -m \"learning_rate=0.01:0.03:0.01\"\n\n# SI prefixes\npython train.py -m \"batch_size=1k,2k,4k\"\n# 1000, 2000, 4000\n\n# Grid within a single parameter\npython train.py -m \"model=(small,large)_(v1,v2)\"\n# small_v1, small_v2, large_v1, large_v2\n</code></pre>"},{"location":"getting-started/concepts/#analysis-tools","title":"Analysis Tools","text":"<p>After running experiments, HydraFlow provides powerful tools for accessing and analyzing results. These tools help you track, compare, and derive insights from your experiments.</p>"},{"location":"getting-started/concepts/#working-with-individual-runs","title":"Working with Individual Runs","text":"<p>For individual experiment analysis, HydraFlow provides the <code>Run</code> class, which represents a single experiment run:</p> <pre><code>from hydraflow import Run\n\n# Load an existing run\nrun = Run.load(\"path/to/run\")\n\n# Access configuration values\nlearning_rate = run.get(\"learning_rate\")\n</code></pre> <p>The <code>Run</code> class provides:</p> <ul> <li>Access to experiment configurations used during the run</li> <li>Methods for loading and analyzing experiment results</li> <li>Support for custom implementations through the factory pattern</li> <li>Type-safe access to configuration values</li> </ul> <p>You can use type parameters for more powerful IDE support:</p> <pre><code>from dataclasses import dataclass\nfrom hydraflow import Run\n\n@dataclass\nclass MyConfig:\n    learning_rate: float\n    batch_size: int\n\n# Load a Run with type information\nrun = Run[MyConfig].load(\"path/to/run\")\nprint(run.cfg.learning_rate)  # IDE auto-completion works\n</code></pre>"},{"location":"getting-started/concepts/#comparing-multiple-runs","title":"Comparing Multiple Runs","text":"<p>For comparing multiple runs, HydraFlow offers the <code>RunCollection</code> class, which enables efficient analysis across runs:</p> <pre><code># Load multiple runs\nruns = Run.load([\"path/to/run1\", \"path/to/run2\", \"path/to/run3\"])\n\n# Filter runs by parameter value\nfiltered_runs = runs.filter(model_type=\"lstm\")\n\n# Group runs by a parameter\ngrouped_runs = runs.group_by(\"dataset_name\")\n\n# Convert to DataFrame for analysis\ndf = runs.to_frame(\"learning_rate\", \"batch_size\", \"accuracy\")\n</code></pre> <p>Key features of experiment comparison:</p> <ul> <li>Filtering runs based on configuration parameters</li> <li>Grouping runs by common attributes</li> <li>Aggregating data across runs</li> <li>Converting to Polars DataFrames for advanced analysis</li> </ul>"},{"location":"getting-started/concepts/#summary","title":"Summary","text":"<p>These core concepts work together to provide a comprehensive framework for managing machine learning experiments:</p> <ol> <li>Configuration Management - Type-safe configuration with Python dataclasses</li> <li>Main Decorator - The entry point that integrates Hydra and MLflow</li> <li>Workflow Automation - Reusable experiment definitions and advanced parameter sweeps</li> <li>Analysis Tools - Access, filter, and analyze experiment results</li> </ol> <p>Understanding these fundamental concepts will help you leverage the full power of HydraFlow for your machine learning projects.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide walks you through installing HydraFlow and its dependencies.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>HydraFlow requires:</p> <ul> <li>Python 3.13 or higher</li> <li>A package manager (pip or uv)</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>You can install HydraFlow using your preferred package manager:</p>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install hydraflow\n</code></pre>"},{"location":"getting-started/installation/#using-uv","title":"Using uv","text":"<p>uv is a modern, fast Python package manager:</p> <pre><code>uv pip install hydraflow\n</code></pre> <p>These commands install the core framework with minimal dependencies.</p>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify that HydraFlow is correctly installed by running the CLI command:</p> <pre><code>hydraflow --help\n</code></pre> <p>This should display the help message and available commands, confirming that HydraFlow is properly installed and accessible from your terminal.</p>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":"<p>While not required, we recommend using a virtual environment:</p>"},{"location":"getting-started/installation/#using-venv","title":"Using venv","text":"<pre><code>python -m venv hydraflow-env\nsource hydraflow-env/bin/activate  # On Windows: hydraflow-env\\Scripts\\activate\npip install hydraflow  # or use uv pip\n</code></pre>"},{"location":"getting-started/installation/#using-uv_1","title":"Using uv","text":"<pre><code>uv venv hydraflow-env\nsource hydraflow-env/bin/activate  # On Windows: hydraflow-env\\Scripts\\activate\nuv pip install hydraflow\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during installation:</p> <ol> <li>Ensure your Python version is 3.13 or higher</li> <li>Update your package manager:</li> <li>For pip: <code>pip install --upgrade pip</code></li> <li>For uv: <code>uv self update</code></li> <li>If installing from source, ensure you have the necessary build tools    installed for your platform</li> </ol> <p>For persistent issues, please check the GitHub issues or open a new issue with details about your environment and the error message.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have installed HydraFlow, proceed to Core Concepts to understand the framework's fundamental principles.</p>"},{"location":"part1-applications/","title":"Running Applications with HydraFlow","text":"<p>This section covers the fundamentals of defining and running HydraFlow applications - the first step in your machine learning experiment workflow.</p>"},{"location":"part1-applications/#overview","title":"Overview","text":"<p>HydraFlow applications combine Hydra's configuration management with MLflow's experiment tracking capabilities. This allows you to:</p> <ul> <li>Define type-safe configurations using Python dataclasses</li> <li>Override configurations via command-line arguments</li> <li>Launch multiple runs with different parameter combinations</li> <li>Automatically track all configurations, metrics, and artifacts</li> </ul>"},{"location":"part1-applications/#key-components","title":"Key Components","text":"<p>The core elements of a HydraFlow application are:</p> <ol> <li> <p>Configuration Class: A Python dataclass that defines the structure    and default values of your application's configuration.</p> </li> <li> <p>Main Function: The entry point of your application, decorated with    <code>@hydraflow.main</code> and accepting the configuration as    an argument.</p> </li> <li> <p>Experiment Logic: Your machine learning code that utilizes the    configuration and logs results.</p> </li> </ol>"},{"location":"part1-applications/#basic-application-structure","title":"Basic Application Structure","text":"<pre><code>from dataclasses import dataclass\nfrom mlflow.entities import Run\nimport hydraflow\n\n@dataclass\nclass Config:\n    learning_rate: float = 0.01\n    batch_size: int = 32\n    epochs: int = 10\n\n@hydraflow.main(Config)\ndef app(run: Run, cfg: Config) -&gt; None:\n    # Your experiment code here\n    print(f\"Training with lr={cfg.learning_rate}, batch_size={cfg.batch_size}\")\n\n    # Log metrics and artifacts\n    hydraflow.log_metric(\"accuracy\", 0.95)\n    hydraflow.log_artifact(\"model.pkl\", \"Model checkpoint\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>"},{"location":"part1-applications/#practical-examples","title":"Practical Examples","text":"<p>If you prefer learning by example, check out our Practical Tutorials section, which includes:</p> <ul> <li>Creating Your First HydraFlow Application: A step-by-step guide to building a basic application</li> <li>Automating Complex Workflows: How to define and execute complex experiment workflows</li> <li>Analyzing Experiment Results: Working with experiment results</li> </ul>"},{"location":"part1-applications/#whats-next","title":"What's Next","text":"<p>In the following pages, we'll explore each aspect of HydraFlow applications in detail:</p> <ul> <li> <p>Main Decorator: Learn how to use the   <code>@hydraflow.main</code> decorator to create HydraFlow applications.</p> </li> <li> <p>Configuration: Discover how to define, compose, and   validate configurations using dataclasses and Hydra's powerful   configuration system.</p> </li> <li> <p>Execution: Understand how to run applications, override   configurations, and perform parameter sweeps for experimentation.</p> </li> </ul>"},{"location":"part1-applications/#advanced-application-features","title":"Advanced Application Features","text":"<p>Once you've mastered the basics, you may want to explore HydraFlow's more advanced features:</p> <ul> <li> <p>Extended Sweep Syntax: Define complex parameter spaces using   HydraFlow's powerful syntax for numerical ranges, combinations, and more.</p> </li> <li> <p>Job Configuration: Create reusable job definitions for   repeated experiment workflows using a declarative YAML format.</p> </li> </ul> <p>These advanced features are covered in detail in Part 2: Automating Workflows.</p>"},{"location":"part1-applications/configuration/","title":"Configuration Management","text":"<p>HydraFlow uses a powerful configuration management system based on Python's dataclasses and Hydra's composition capabilities. This approach provides type safety, IDE auto-completion, and flexible parameter specification.</p>"},{"location":"part1-applications/configuration/#basic-configuration","title":"Basic Configuration","text":"<p>The simplest way to define a configuration for a HydraFlow application is using a Python dataclass:</p> <pre><code>from dataclasses import dataclass\nfrom mlflow.entities import Run\nimport hydraflow\n\n@dataclass\nclass Config:\n    learning_rate: float = 0.01\n    batch_size: int = 32\n    epochs: int = 10\n    model_name: str = \"transformer\"\n\n@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Access configuration parameters\n    print(f\"Training {cfg.model_name} for {cfg.epochs} epochs\")\n    print(f\"Learning rate: {cfg.learning_rate}, Batch size: {cfg.batch_size}\")\n</code></pre>"},{"location":"part1-applications/configuration/#type-hints","title":"Type Hints","text":"<p>Adding type hints to your configuration class provides several benefits:</p> <ol> <li>Static Type Checking: Tools like mypy can catch configuration errors    before runtime.</li> <li>IDE Auto-completion: Your IDE can provide suggestions as you work with    configuration objects.</li> <li>Documentation: Type hints serve as implicit documentation for your    configuration parameters.</li> </ol>"},{"location":"part1-applications/configuration/#nested-configurations","title":"Nested Configurations","text":"<p>For more complex applications, you can use nested dataclasses to organize related parameters:</p> <pre><code>@dataclass\nclass ModelConfig:\n    name: str = \"transformer\"\n    hidden_size: int = 512\n    num_layers: int = 6\n    dropout: float = 0.1\n\n@dataclass\nclass OptimizerConfig:\n    name: str = \"adam\"\n    learning_rate: float = 0.001\n    weight_decay: float = 0.0\n\n@dataclass\nclass DataConfig:\n    batch_size: int = 32\n    num_workers: int = 4\n    train_path: str = \"data/train\"\n    val_path: str = \"data/val\"\n\n@dataclass\nclass Config:\n    model: ModelConfig = ModelConfig()\n    optimizer: OptimizerConfig = OptimizerConfig()\n    data: DataConfig = DataConfig()\n    seed: int = 42\n    max_epochs: int = 10\n\n@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Access nested configuration\n    model_name = cfg.model.name\n    lr = cfg.optimizer.learning_rate\n    batch_size = cfg.data.batch_size\n</code></pre>"},{"location":"part1-applications/configuration/#hydra-integration","title":"Hydra Integration","text":"<p>HydraFlow integrates closely with Hydra for configuration management. For detailed explanations of Hydra's capabilities, please refer to the Hydra documentation.</p> <p>HydraFlow leverages the following Hydra features, but does not modify their behavior:</p> <ul> <li>Configuration Files: Organize configurations in YAML files</li> <li>Command-line Overrides: Change parameters without modifying code</li> <li>Configuration Groups: Swap entire configuration blocks</li> <li>Configuration Composition: Combine configurations from multiple sources</li> <li>Interpolation: Reference other configuration values</li> <li>Multi-run Sweeps: Run experiments with different parameter combinations</li> </ul> <p>When using HydraFlow, remember that:</p> <ol> <li>Your configuration structure comes from your dataclass definitions</li> <li>HydraFlow automatically registers your top-level dataclass with Hydra</li> <li><code>@hydraflow.main</code> sets up the connection between your dataclass and Hydra</li> </ol> <p>For advanced Hydra features and detailed usage examples, we recommend consulting the official Hydra documentation after you become familiar with the basic HydraFlow concepts.</p>"},{"location":"part1-applications/configuration/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Type Hints: Always include type hints for all configuration parameters.</p> </li> <li> <p>Set Sensible Defaults: Provide reasonable default values to make your    application usable with minimal configuration.</p> </li> <li> <p>Group Related Parameters: Use nested dataclasses to organize related    parameters logically.</p> </li> <li> <p>Document Parameters: Add docstrings to your dataclasses and parameters    to explain their purpose and valid values.</p> </li> <li> <p>Validate Configurations: Add validation logic to catch invalid    configurations early.</p> </li> </ol>"},{"location":"part1-applications/configuration/#summary","title":"Summary","text":"<p>HydraFlow's configuration system combines the type safety of Python dataclasses with the flexibility of Hydra's composition and override capabilities. This approach makes your machine learning experiments more maintainable, reproducible, and easier to debug.</p>"},{"location":"part1-applications/execution/","title":"Executing Applications","text":"<p>Once you've defined your HydraFlow application, you need to execute it to run experiments. This page covers how to run applications, configure them via command-line arguments, and perform parameter sweeps.</p>"},{"location":"part1-applications/execution/#basic-execution","title":"Basic Execution","text":"<p>To run a HydraFlow application, simply execute the Python script:</p> <pre><code>python train.py\n</code></pre> <p>This will:</p> <ol> <li>Set up the MLflow tracking URI (if provided in the decorator) and an     MLflow experiment with the same name as the Hydra job name. If the     experiment doesn't exist, it will be created automatically</li> <li>Create a new MLflow run or reuse an existing one based on the configuration</li> <li>Save the Hydra configuration as an MLflow artifact</li> <li>Execute your function decorated with <code>@hydraflow.main</code></li> <li>Save only <code>*.log</code> files from Hydra's output directory as MLflow artifacts</li> </ol> <p>Note that any other artifacts (models, data files, etc.) must be explicitly saved by your code using MLflow's logging functions. The <code>chdir</code> option in the <code>@hydraflow.main</code> decorator can help with this by changing the working directory to the run's artifact directory, making file operations more convenient.</p>"},{"location":"part1-applications/execution/#command-line-override-syntax","title":"Command-line Override Syntax","text":"<p>Hydra provides a powerful syntax for overriding configuration values:</p> <pre><code># Override simple parameters\npython train.py learning_rate=0.001 batch_size=64\n\n# Override nested parameters\npython train.py model.hidden_size=1024 optimizer.learning_rate=0.0001\n\n# Override using configuration groups\npython train.py model=cnn optimizer=sgd\n\n# Load specific configurations\npython train.py --config-name=my_experiment\n</code></pre>"},{"location":"part1-applications/execution/#working-with-multirun-mode","title":"Working with Multirun Mode","text":"<p>Hydra's multirun mode allows you to execute your application with multiple configurations in a single command:</p> <pre><code># Run with different learning rates\npython train.py -m optimizer.learning_rate=0.1,0.01,0.001\n\n# Run with different model configurations\npython train.py -m model=transformer,cnn,lstm\n\n# Sweep over combinations of parameters\npython train.py -m optimizer.learning_rate=0.1,0.01 model=transformer,cnn\n</code></pre> <p>The above command will run your application 6 times with all combinations of the specified parameters (2 learning rates \u00d7 3 model types).</p>"},{"location":"part1-applications/execution/#advanced-parameter-sweeps","title":"Advanced Parameter Sweeps","text":"<p>For more complex parameter spaces, HydraFlow provides an extended sweep syntax that goes beyond Hydra's basic capabilities:</p> <pre><code># Define numerical ranges with start:stop:step\npython train.py -m batch_size=16:128:16\n\n# Use SI prefixes for scientific notation\npython train.py -m learning_rate=1:5:m  # 0.001 to 0.005\n\n# Create parameter combinations\npython train.py -m model=(cnn,transformer)_(small,large)\n</code></pre> <p>See Extended Sweep Syntax for a complete reference on these powerful features.</p>"},{"location":"part1-applications/execution/#managing-complex-experiment-workflows","title":"Managing Complex Experiment Workflows","text":"<p>HydraFlow provides CLI tools to work with multirun mode more efficiently than using long command lines:</p> <pre><code># Define jobs in hydraflow.yaml\njobs:\n  train:\n    run: python train.py\n    sets:\n      - each: model=small,large\n      - all: seed=42,43\n\n# Run the defined job\nhydraflow run train\n</code></pre> <p>This approach helps you organize complex experiments, track execution history, and make experiments more reproducible. For details on these advanced capabilities, see Job Configuration in Part 2.</p>"},{"location":"part1-applications/execution/#output-organization","title":"Output Organization","text":"<p>By default, Hydra organizes outputs in the following directory structure for HydraFlow applications:</p> <pre><code>ROOT_DIR/\n\u251c\u2500\u2500 outputs/                 # Created in single run mode\n\u2502   \u2514\u2500\u2500 YYYY-MM-DD/          # Date\n\u2502       \u2514\u2500\u2500 HH-MM-SS/        # Time\n\u2502           \u2514\u2500\u2500 .hydra/      # Hydra configuration\n\u2514\u2500\u2500 multirun/                # Created in multirun mode\n    \u2514\u2500\u2500 YYYY-MM-DD/          # Date\n        \u2514\u2500\u2500 HH-MM-SS/        # Time\n            \u251c\u2500\u2500 0/           # Sweep run 0\n            \u2502   \u2514\u2500\u2500 .hydra/\n            \u251c\u2500\u2500 1/           # Sweep run 1\n            \u2502   \u2514\u2500\u2500 .hydra/\n            \u2514\u2500\u2500 ...\n</code></pre> <p>Each run also creates an entry in the MLflow tracking directory:</p> <pre><code>mlruns/\n\u251c\u2500\u2500 experiment_id_1/         # Experiment ID\n\u2502   \u251c\u2500\u2500 run_id_1/            # Run ID\n\u2502   \u2502   \u251c\u2500\u2500 artifacts/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 .hydra/      # Hydra config copied here\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 config.yaml\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 hydra.yaml\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 overrides.yaml\n\u2502   \u2502   \u251c\u2500\u2500 metrics/\n\u2502   \u2502   \u251c\u2500\u2500 params/\n\u2502   \u2502   \u2514\u2500\u2500 tags/\n\u2502   \u251c\u2500\u2500 run_id_2/\n\u2502   \u2502   \u2514\u2500\u2500 artifacts/\n\u2502   \u2502       \u2514\u2500\u2500 .hydra/\n:   \u2514\u2500\u2500 meta.yaml            # Contains Hydra job name\n</code></pre> <p>Important: In HydraFlow, the MLflow run's <code>artifacts</code> directory serves as the single source of truth for your experiment data. The Hydra-generated output directories are only used as temporary locations to store configuration files and logs, which are then copied to the MLflow artifacts. After your application completes, you can safely delete the Hydra output directories without losing any essential information, as everything important is preserved in the MLflow run.</p> <p>Users can always access Hydra-generated configuration files and logs by navigating to the <code>run_dir/artifacts/.hydra/</code> directory within the MLflow run. This directory contains all the configuration files, overrides, and other Hydra-related files that were used for that specific run, making it easy to review and reproduce the experiment settings even after the original Hydra output directories have been deleted.</p>"},{"location":"part1-applications/execution/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Sensible Defaults: Design your configuration to have sensible defaults    so that running without arguments produces meaningful results.</p> </li> <li> <p>Document Command-line Options: Include a README or help text that    describes the available command-line options for your application.</p> </li> <li> <p>Version Control Configurations: Store configuration files in version    control to ensure reproducibility.</p> </li> <li> <p>Organize Complex Sweeps: For complex parameter sweeps, use HydraFlow's    extended sweep syntax and job configurations rather than long command lines.</p> </li> <li> <p>Monitor Resource Usage: Be mindful of resource usage when running large    parameter sweeps, especially with parallel execution.</p> </li> </ol>"},{"location":"part1-applications/execution/#summary","title":"Summary","text":"<p>HydraFlow provides a streamlined way to leverage both Hydra's configuration management and MLflow's experiment tracking. The execution system is built on standard Hydra functionality, while experiment tracking utilizes MLflow's capabilities. By combining these tools, HydraFlow enables reproducible and efficient machine learning workflows while maintaining compatibility with the underlying libraries' documentation and ecosystem.</p> <p>For more advanced execution capabilities, see: - Extended Sweep Syntax - Job Configuration in Part 2</p>"},{"location":"part1-applications/main-decorator/","title":"Main Decorator","text":"<p>The <code>hydraflow.main</code> decorator is the central component for creating HydraFlow applications. It bridges Hydra's configuration management with MLflow's experiment tracking, automatically setting up the experiment environment.</p>"},{"location":"part1-applications/main-decorator/#basic-usage","title":"Basic Usage","text":"<p>Here's how to use the main decorator in its simplest form:</p> <pre><code>from dataclasses import dataclass\nfrom mlflow.entities import Run\nimport hydraflow\n\n@dataclass\nclass Config:\n    learning_rate: float = 0.01\n    batch_size: int = 32\n\n@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    print(f\"Training with learning_rate={cfg.learning_rate}\")\n    # Your training code here\n\nif __name__ == \"__main__\":\n    train()\n</code></pre>"},{"location":"part1-applications/main-decorator/#function-signature","title":"Function Signature","text":"<p>The function decorated with <code>@hydraflow.main</code> must accept two parameters:</p> <ol> <li> <p><code>run</code>: The current run object of type <code>mlflow.entities.Run</code>, which can be used to access run    information and log additional metrics or artifacts.</p> </li> <li> <p><code>cfg</code>: The configuration object containing all parameters, populated from    Hydra's configuration system and command-line overrides.</p> </li> </ol>"},{"location":"part1-applications/main-decorator/#type-annotations","title":"Type Annotations","text":"<p>The <code>cfg</code> parameter should be annotated with your configuration class for type checking and IDE auto-completion. This is particularly useful when working with complex configurations:</p> <pre><code>@dataclass\nclass TrainingConfig:\n    learning_rate: float\n    batch_size: int\n\n@dataclass\nclass DataConfig:\n    path: str\n    validation_split: float\n\n@dataclass\nclass Config:\n    training: TrainingConfig\n    data: DataConfig\n    seed: int = 42\n\n@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Type-checked access to nested configuration\n    lr = cfg.training.learning_rate\n    data_path = cfg.data.path\n\n    # Your training code here\n</code></pre>"},{"location":"part1-applications/main-decorator/#using-mlflow-apis","title":"Using MLflow APIs","text":"<p>Within a function decorated with <code>@hydraflow.main</code>, you have access to standard MLflow logging functions:</p> <pre><code>import mlflow\n\n@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", 0.95)\n\n    # Log a set of metrics\n    mlflow.log_metrics({\n        \"precision\": 0.92,\n        \"recall\": 0.89,\n        \"f1_score\": 0.90\n    })\n\n    # Log artifacts\n    mlflow.log_artifact(\"model.pkl\")\n\n    # Log parameters not included in the config\n    mlflow.log_param(\"custom_param\", \"value\")\n</code></pre>"},{"location":"part1-applications/main-decorator/#run-identification-and-reuse","title":"Run Identification and Reuse","text":"<p>One of HydraFlow's key features is automatic run identification and reuse. By default, if a run with the same configuration already exists within an experiment, HydraFlow will reuse that existing run instead of creating a new one.</p> <p>This behavior is particularly valuable in computation clusters where preemption (forced termination by the system) can occur. If your job is preempted before completion, you can simply restart it, and HydraFlow will automatically continue with the existing run, allowing you to resume from checkpoints.</p> <pre><code>from pathlib import Path\n\n@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # If this exact configuration was run before but interrupted,\n    # the same Run object will be reused\n    checkpoint_path = Path(\"checkpoint.pt\")\n\n    if checkpoint_path.exists():\n        print(f\"Resuming from checkpoint in run: {run.info.run_id}\")\n        # Load checkpoint and continue training\n    else:\n        print(f\"Starting new training in run: {run.info.run_id}\")\n        # Start training from scratch\n</code></pre> <p>This default behavior improves efficiency by:</p> <ul> <li>Avoiding duplicate experiments with identical configurations</li> <li>Enabling graceful recovery from system interruptions</li> <li>Reducing wasted computation when jobs are preempted</li> <li>Supporting iterative development with checkpointing</li> </ul>"},{"location":"part1-applications/main-decorator/#automatic-skipping-of-completed-runs","title":"Automatic Skipping of Completed Runs","text":"<p>HydraFlow automatically skips runs that have already completed successfully. This is especially valuable in environments where jobs are automatically restarted after preemption. Without requiring any additional configuration, HydraFlow will:</p> <ol> <li>Identify already completed runs with the same configuration</li> <li>Skip re-execution of those runs</li> <li>Proceed only with runs that were interrupted or not yet executed</li> </ol> <pre><code>@hydraflow.main(Config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # If this configuration was already successfully run before,\n    # the function won't even be called - HydraFlow automatically\n    # skips it and returns immediately\n\n    print(f\"This run is either new or was previously interrupted: {run.info.run_id}\")\n    # Your training code here\n</code></pre> <p>This automatic skipping behavior:</p> <ul> <li>Prevents redundant computation in multi-job or batch scenarios</li> <li>Handles preemption recovery efficiently in cluster environments</li> <li>Reduces resource usage by avoiding unnecessary re-execution</li> <li>Works seamlessly without requiring explicit handling in your code</li> </ul>"},{"location":"part1-applications/main-decorator/#advanced-features","title":"Advanced Features","text":"<p>The <code>hydraflow.main</code> decorator supports several keyword arguments that enhance its functionality. All these options are set to <code>None</code> or <code>False</code> by default and must be explicitly enabled when needed:</p>"},{"location":"part1-applications/main-decorator/#tracking-uri-management-tracking_uri","title":"Tracking URI Management (<code>tracking_uri</code>)","text":"<p>You can set the MLflow tracking URI directly within the decorator. HydraFlow calls <code>mlflow.set_tracking_uri()</code> with the provided URI before each job execution.</p> <pre><code>@hydraflow.main(Config, tracking_uri=\"sqlite:///mlflow.db\")\ndef train(run: Run, cfg: Config) -&gt; None:\n    # The tracking URI is set before the experiment is created for each job.\n    ...\n</code></pre>"},{"location":"part1-applications/main-decorator/#working-directory-management-chdir","title":"Working Directory Management (<code>chdir</code>)","text":"<p>Change the current working directory to the run's artifact directory during execution:</p> <pre><code>@hydraflow.main(Config, chdir=True)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Working directory is now the run's artifact directory\n    # Useful for relative path references\n    with open(\"results.txt\", \"w\") as f:\n        f.write(\"Results will be saved as an artifact in the run\")\n</code></pre> <p>This option is beneficial when:</p> <ul> <li>You need to save or access files using relative paths</li> <li>Your code relies on local file operations within the experiment directory</li> <li>You want artifacts to be automatically associated with the current run</li> <li>You're working with libraries that expect files in the current directory</li> </ul>"},{"location":"part1-applications/main-decorator/#forcing-new-runs-force_new_run","title":"Forcing New Runs (<code>force_new_run</code>)","text":"<p>Override the default run identification and reuse behavior by always creating a new run, even when identical configurations exist:</p> <pre><code>@hydraflow.main(Config, force_new_run=True)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # This will always create a new run, even if identical\n    # configurations exist in the experiment\n    print(f\"Fresh run created: {run.info.run_id}\")\n</code></pre> <p>This option is useful when:</p> <ul> <li>You want to test the reproducibility of your experiments</li> <li>You need to compare results across multiple identical runs</li> <li>You've made changes to external dependencies not captured in the configuration</li> <li>You want to avoid the run identification mechanism for debugging purposes</li> </ul>"},{"location":"part1-applications/main-decorator/#rerunning-finished-experiments-rerun_finished","title":"Rerunning Finished Experiments (<code>rerun_finished</code>)","text":"<p>Override the automatic skipping of completed runs by explicitly allowing rerunning of experiments that have already finished:</p> <pre><code>@hydraflow.main(Config, rerun_finished=True)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Runs that have FINISHED status will be rerun instead of skipped\n    # The same run ID will be reused\n    print(f\"Run may be rerunning even if it completed successfully: {run.info.run_id}\")\n</code></pre> <p>This option is valuable when:</p> <ul> <li>You need to regenerate artifacts or metrics from a successful run</li> <li>You've improved your logging or analysis and want to apply it to previous runs</li> <li>You're iteratively refining experiments without changing their configuration</li> <li>You suspect that a \"successful\" run may have had undetected issues</li> </ul>"},{"location":"part1-applications/main-decorator/#matching-based-on-overrides-match_overrides","title":"Matching Based on Overrides (<code>match_overrides</code>)","text":"<p>Match runs based on command-line overrides instead of the full configuration:</p> <pre><code>@hydraflow.main(Config, match_overrides=True)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Runs will be matched based on CLI overrides\n    # rather than the complete configuration contents\n    print(f\"Run ID: {run.info.run_id}\")\n</code></pre> <p>This option is particularly useful when:</p> <ul> <li>You have large default configurations but only care about specific parameters</li> <li>You want to group runs by the parameters that were explicitly overridden</li> <li>You're iterating on experiments with command-line variations</li> <li>Your configuration contains volatile or automatically generated values</li> </ul>"},{"location":"part1-applications/main-decorator/#dynamic-configuration-updates-update","title":"Dynamic Configuration Updates (<code>update</code>)","text":"<p>Modify or enhance the configuration after it has been loaded by Hydra but before the run starts:</p> <pre><code>def update_config(cfg: Config) -&gt; Config:\n    # Calculate derived values or add runtime information\n    if cfg.width &gt; 0 and cfg.height &gt; 0:\n        cfg.area = cfg.width * cfg.height\n    return cfg\n\n@hydraflow.main(Config, update=update_config)\ndef train(run: Run, cfg: Config) -&gt; None:\n    # Configuration has been updated with calculated area\n    print(f\"Area: {cfg.area}\")\n</code></pre> <p>This option is powerful when you need to:</p> <ul> <li>Calculate derived parameters based on existing configuration values</li> <li>Apply conditional logic to adjust parameters based on their relationships</li> <li>Ensure consistency between related parameters</li> <li>Adapt configurations to the current environment (e.g., hardware capabilities)</li> </ul> <p>The <code>update</code> function should accept a configuration object and return the same object (or None). Any changes made to the configuration will be saved to the run's configuration file, ensuring that the stored configuration accurately reflects all updates.</p>"},{"location":"part1-applications/main-decorator/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Keep Configuration Classes Focused: Break down complex configurations    into logical components using nested dataclasses.</p> </li> <li> <p>Use Type Annotations: Always annotate your function parameters for    better IDE support and type checking.</p> </li> <li> <p>Log Important Information: Log all relevant metrics, parameters, and    artifacts to ensure reproducibility.</p> </li> <li> <p>Handle Errors Gracefully: Implement proper error handling inside your    main function to avoid losing experiment data.</p> </li> </ol>"},{"location":"part1-applications/main-decorator/#summary","title":"Summary","text":"<p>The <code>hydraflow.main</code> decorator simplifies the integration of Hydra and MLflow, handling configuration management and experiment tracking automatically. This allows you to focus on your experiment implementation while ensuring that all relevant information is properly tracked and organized.</p>"},{"location":"part2-advanced/","title":"Automating Workflows","text":"<p>This section covers advanced techniques for automating and structuring multiple experiments in HydraFlow. It provides tools for defining complex parameter spaces and reusable experiment definitions.</p>"},{"location":"part2-advanced/#overview","title":"Overview","text":"<p>After creating your basic HydraFlow applications, the next step is to automate your experiment workflows. This includes:</p> <ul> <li>Creating parameter sweeps across complex combinations</li> <li>Defining reusable experiment configurations</li> <li>Organizing large-scale experiment campaigns</li> </ul>"},{"location":"part2-advanced/#key-components","title":"Key Components","text":"<p>The main components for workflow automation in HydraFlow are:</p> <ol> <li>Extended Sweep Syntax: A powerful syntax for defining parameter     spaces beyond simple comma-separated values.</li> <li>Job Configuration: A YAML-based definition system for creating     reusable experiment workflows.</li> </ol>"},{"location":"part2-advanced/#practical-examples","title":"Practical Examples","text":"<p>For hands-on examples of workflow automation, see our Practical Tutorials section, specifically:</p> <ul> <li>Automating Complex Workflows: A tutorial   that demonstrates how to use <code>hydraflow.yaml</code> to define and execute   various types of workflows</li> <li>Analyzing Experiment Results: Learn   how to work with results from automated experiment runs</li> </ul>"},{"location":"part2-advanced/#extended-sweep-syntax","title":"Extended Sweep Syntax","text":"<p>HydraFlow extends Hydra's sweep syntax to provide more powerful ways to define parameter spaces:</p> <pre><code># Range of values (inclusive)\npython train.py -m \"learning_rate=0.001:0.1:0.001\"  # start:end:step\n\n# SI prefixes\npython train.py -m \"batch_size=1k,2k,4k\"  # 1000, 2000, 4000\n\n# Logarithmic spacing\npython train.py -m \"learning_rate=log(0.0001:0.1:10)\"  # 10 points log-spaced\n</code></pre> <p>Learn more about these capabilities in Sweep Syntax.</p>"},{"location":"part2-advanced/#job-configuration","title":"Job Configuration","text":"<p>For more complex experiment workflows, you can use HydraFlow's job configuration system:</p> <pre><code>jobs:\n  train_models:\n    run: python train.py\n    sets:\n      - each: model=small,medium,large\n        all: seed=42 epochs=10\n\n  evaluate_models:\n    run: python evaluate.py\n    sets:\n      - each: model=small,medium,large\n        all: test_data=validation\n</code></pre> <p>This approach allows you to define reusable experiment definitions that can be executed with a single command. Learn more in Job Configuration.</p>"},{"location":"part2-advanced/#executing-workflows","title":"Executing Workflows","text":"<p>Once defined, workflows can be executed using the <code>hydraflow run</code> command:</p> <pre><code># Execute a job defined in hydraflow.yaml\nhydraflow run train_models\n\n# Preview execution with dry run\nhydraflow run train_models --dry-run\n\n# Run a job with additional overrides\nhydraflow run train_models seed=123\n</code></pre>"},{"location":"part2-advanced/#whats-next","title":"What's Next","text":"<p>In the following pages, we'll explore workflow automation in detail:</p> <ul> <li>Sweep Syntax: Learn about HydraFlow's extended   syntax for defining parameter spaces.</li> <li>Job Configuration: Discover how to create   reusable job definitions for your experiments.</li> </ul> <p>After automating your experiments, you'll want to analyze the results using the tools covered in Part 3: Analyzing Results.</p>"},{"location":"part2-advanced/job-configuration/","title":"Job Configuration","text":"<p>HydraFlow job configuration allows you to define reusable experiment definitions that can be executed with a single command. This page explains how to create and use job configurations.</p>"},{"location":"part2-advanced/job-configuration/#basic-job-configuration","title":"Basic Job Configuration","text":"<p>HydraFlow reads job definitions from a <code>hydraflow.yaml</code> file in your project directory. A basic job configuration looks like this:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: &gt;-\n          model=small,large\n          learning_rate=0.1,0.01\n</code></pre>"},{"location":"part2-advanced/job-configuration/#configuration-structure","title":"Configuration Structure","text":"<p>The configuration file uses the following structure:</p> <ul> <li><code>jobs</code>: The top-level key containing all job definitions</li> <li><code>&lt;job_name&gt;</code>: Name of the job (e.g., \"train\")<ul> <li><code>run</code>: The command to execute</li> <li><code>add</code>: Global configuration arguments appended to each command</li> <li><code>sets</code>: List of parameter sets for the job</li> </ul> </li> </ul> <p>Each job must have either a <code>run</code>, <code>call</code>, or <code>submit</code> key, and at least one parameter set.</p>"},{"location":"part2-advanced/job-configuration/#execution-commands","title":"Execution Commands","text":"<p>HydraFlow supports three types of execution commands:</p>"},{"location":"part2-advanced/job-configuration/#run","title":"<code>run</code>","text":"<p>The <code>run</code> command executes the specified command directly:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: model=small,large\n</code></pre>"},{"location":"part2-advanced/job-configuration/#call","title":"<code>call</code>","text":"<p>The <code>call</code> command executes a Python function:</p> <pre><code>jobs:\n  train:\n    call: my_module.train_function\n    sets:\n      - each: model=small,large\n</code></pre> <p>The specified function will be imported and called with the parameters.</p>"},{"location":"part2-advanced/job-configuration/#submit","title":"<code>submit</code>","text":"<p>The <code>submit</code> command collects all parameter combinations into a text file and passes this file to the specified command:</p> <pre><code>jobs:\n  train:\n    submit: python submit_handler.py\n    sets:\n      - each: model=small,large\n</code></pre> <p>When executed, this will:</p> <ol> <li>Generate all parameter combinations from the sets</li> <li>Write these combinations to a text file (one combination per line)</li> <li>Execute the specified command once, passing the text file as an argument</li> </ol> <p>The command (e.g., <code>submit_handler.py</code> in the example) is responsible for:</p> <ol> <li>Reading the parameter file</li> <li>Processing the parameter sets in any way it chooses</li> <li>Optionally distributing the work (via cluster jobs, local parallelization, etc.)</li> </ol> <p>The key difference between <code>run</code> and <code>submit</code>:</p> <ul> <li><code>run</code>: Executes the command once per parameter combination</li> <li><code>submit</code>: Executes the command once, with all parameter combinations provided in a file</li> </ul> <p>This gives you complete flexibility in how parameter combinations are processed. Your handler script can implement any logic - from simple sequential processing to complex distributed execution across a cluster.</p>"},{"location":"part2-advanced/job-configuration/#parameter-sets","title":"Parameter Sets","text":"<p>Each job contains one or more parameter sets under the <code>sets</code> key. Each set can include the following types of parameters:</p>"},{"location":"part2-advanced/job-configuration/#each","title":"<code>each</code>","text":"<p>The <code>each</code> parameter defines a grid of parameter combinations. Each combination will be executed as a separate command:</p> <pre><code>sets:\n  - each: &gt;-\n      model=small,large\n      learning_rate=0.1,0.01\n</code></pre> <p>This will generate four separate executions, one for each combination of model and learning rate.</p>"},{"location":"part2-advanced/job-configuration/#all","title":"<code>all</code>","text":"<p>The <code>all</code> parameter defines parameters that will be included in each execution from the set:</p> <pre><code>sets:\n  - each: model=small,large\n  - all: seed=42 debug=true\n</code></pre> <p>This will include <code>seed=42 debug=true</code> in every execution for the set.</p>"},{"location":"part2-advanced/job-configuration/#add","title":"<code>add</code>","text":"<p>The <code>add</code> parameter adds additional arguments that are appended to the end of each command. This is primarily used for Hydra configuration settings:</p> <pre><code>sets:\n  - each: model=small,large\n  - add: &gt;-\n      hydra/launcher=joblib\n      hydra.launcher.n_jobs=4\n</code></pre> <p>This will append Hydra configuration to each command from the set. If a set has its own <code>add</code> parameter, it completely overrides the job-level <code>add</code> parameter (they are not merged). The job-level <code>add</code> is entirely ignored for that set.</p>"},{"location":"part2-advanced/job-configuration/#multiple-parameter-sets","title":"Multiple Parameter Sets","text":"<p>A job can have multiple parameter sets, each executed independently:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      # First set: Train models with different architectures\n      - each: &gt;-\n          model=small,large\n          optimizer=adam\n\n      # Second set: Train models with different learning rates\n      - each: &gt;-\n          model=medium\n          learning_rate=0.1,0.01,0.001\n</code></pre> <p>Each set is completely independent and does not build upon the others. The sets are executed sequentially in the order they are defined.</p>"},{"location":"part2-advanced/job-configuration/#combining-parameter-types","title":"Combining Parameter Types","text":"<p>You can combine different parameter types within a single set:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    add: hydra/launcher=joblib hydra.launcher.n_jobs=2\n    sets:\n      # First set: uses job-level add\n      - each: model=small\n      - all: seed=42 debug=true\n\n      # Second set: merges with job-level add (set-level parameters take precedence)\n      - each: model=large\n      - all: seed=43\n      - add: hydra/launcher=submitit hydra.launcher.submitit.cpus_per_task=4\n</code></pre> <p>This will execute:</p> <pre><code># First set: with job-level add\npython train.py model=small seed=42 debug=true hydra/launcher=joblib hydra.launcher.n_jobs=2\n\n# Second set: merges job-level and set-level add (hydra/launcher is overridden by set-level)\npython train.py model=large seed=43 hydra/launcher=submitit hydra.launcher.n_jobs=2 hydra.launcher.submitit.cpus_per_task=4\n</code></pre>"},{"location":"part2-advanced/job-configuration/#job-level-and-set-level-add","title":"Job-level and Set-level <code>add</code>","text":"<p>You can specify <code>add</code> at both the job level and set level:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    add: hydra/launcher=joblib hydra.launcher.n_jobs=2\n    sets:\n      # Uses job-level add\n      - each: model=small,medium\n\n      # Merges with job-level add (set-level takes precedence for the same keys)\n      - each: model=large,xlarge\n        add: hydra/launcher=submitit hydra.launcher.submitit.cpus_per_task=8\n</code></pre> <p>When a set has its own <code>add</code> parameter, it is merged with the job-level <code>add</code> parameter. If the same parameter key exists in both the job-level and set-level <code>add</code>, the set-level value takes precedence.</p> <p>For example, with the configuration above:</p> <ul> <li>The first set uses: <code>hydra/launcher=joblib hydra.launcher.n_jobs=2</code></li> <li>The second set uses: <code>hydra/launcher=submitit hydra.launcher.n_jobs=2 hydra.launcher.submitit.cpus_per_task=8</code></li> </ul> <p>Notice how <code>hydra/launcher</code> is overridden by the set-level value, while <code>hydra.launcher.n_jobs</code> from the job-level is retained.</p> <p>This behavior allows you to:</p> <ol> <li>Define common parameters at the job level</li> <li>Override or add specific parameters at the set level</li> <li>Keep all non-conflicting parameters from both levels</li> </ol> <p>This merging behavior makes it easy to maintain common configuration options while customizing specific aspects for different parameter sets.</p>"},{"location":"part2-advanced/job-configuration/#summary","title":"Summary","text":"<p>HydraFlow's job configuration system provides a powerful way to define and manage complex parameter sweeps:</p> <ol> <li> <p>Execution Commands:</p> <ul> <li><code>run</code>: Executes a command once per parameter combination (most common usage)</li> <li><code>call</code>: Calls a Python function once per parameter combination</li> <li><code>submit</code>: Passes all parameter combinations as a text file to a handler script, executed once</li> </ul> </li> <li> <p>Parameter Types:</p> <ul> <li><code>each</code>: Generates a grid of parameter combinations (cartesian product)</li> <li><code>all</code>: Specifies parameters included in every command</li> <li><code>add</code>: Arguments appended to the end of each command (primarily for Hydra configuration)</li> </ul> </li> <li> <p>Multiple Sets and Merging Behavior:</p> <ul> <li>Define multiple independent parameter sets</li> <li>Job-level and set-level <code>add</code> parameters are merged</li> <li>Set-level values take precedence for the same keys</li> </ul> </li> </ol> <p>These features combined allow you to define complex experiment configurations concisely and execute them efficiently. Reusing configurations ensures reproducibility and consistency across your experiments.</p>"},{"location":"part2-advanced/sweep-syntax/","title":"Extended Sweep Syntax","text":"<p>HydraFlow provides an extended syntax for defining parameter spaces. This syntax is more powerful than Hydra's basic comma-separated lists, allowing you to define ranges, logarithmic spaces, and more.</p>"},{"location":"part2-advanced/sweep-syntax/#basic-syntax","title":"Basic Syntax","text":"<p>The core of HydraFlow's sweep syntax is a comma-separated list:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: model=small,medium,large\n</code></pre> <p>This generates commands for each parameter value:</p> <pre><code>python train.py -m model=small\npython train.py -m model=medium\npython train.py -m model=large\n</code></pre> <p>When using multiple parameters with <code>each</code>, all possible combinations (cartesian product) will be generated:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: &gt;-\n          model=small,medium\n          learning_rate=0.1,0.01\n</code></pre> <p>This generates all four combinations:</p> <pre><code>python train.py -m model=small learning_rate=0.1\npython train.py -m model=small learning_rate=0.01\npython train.py -m model=medium learning_rate=0.1\npython train.py -m model=medium learning_rate=0.01\n</code></pre>"},{"location":"part2-advanced/sweep-syntax/#numerical-ranges","title":"Numerical Ranges","text":"<p>For numerical parameters, you can use range notation with colons:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: batch_size=16:128:16\n</code></pre> <p>This generates:</p> <pre><code>python train.py -m batch_size=16\npython train.py -m batch_size=32\npython train.py -m batch_size=48\n...\npython train.py -m batch_size=128\n</code></pre> <p>The format is <code>start:stop:step</code>, similar to Python's range notation. Note that unlike Python's range, the stop value is inclusive - the range includes both the start and stop values if they align with the step size.</p> <p>You can omit the start value to default to 0:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: steps=:5  # Equivalent to steps=0:5:1\n</code></pre> <p>Generates:</p> <pre><code>python train.py -m steps=0\npython train.py -m steps=1\npython train.py -m steps=2\npython train.py -m steps=3\npython train.py -m steps=4\npython train.py -m steps=5\n</code></pre> <p>You can also use negative steps to create descending ranges:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: lr=5:1:-1\n</code></pre> <p>Generates:</p> <pre><code>python train.py -m lr=5\npython train.py -m lr=4\npython train.py -m lr=3\npython train.py -m lr=2\npython train.py -m lr=1\n</code></pre>"},{"location":"part2-advanced/sweep-syntax/#si-prefixes-engineering-notation","title":"SI Prefixes (Engineering Notation)","text":"<p>You can use SI prefixes to represent large or small numbers concisely:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: &gt;-\n          weight_decay=1:3:n     # nano (1e-9)\n          max_tokens=1:3:k       # kilo (1e3)\n          model_dim=1:3:M        # mega (1e6)\n</code></pre> <p>This generates all combinations (total of 27 different commands).</p> <p>Supported SI prefixes:</p> <ul> <li><code>f</code>: femto (1e-15)</li> <li><code>p</code>: pico (1e-12)</li> <li><code>n</code>: nano (1e-9)</li> <li><code>u</code>: micro (1e-6)</li> <li><code>m</code>: milli (1e-3)</li> <li><code>k</code>: kilo (1e3)</li> <li><code>M</code>: mega (1e6)</li> <li><code>G</code>: giga (1e9)</li> <li><code>T</code>: tera (1e12)</li> </ul> <p>You can also use fractional steps with SI prefixes:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: learning_rate=0.1:0.4:0.1:m  # From 0.1e-3 to 0.4e-3 by 0.1e-3\n</code></pre>"},{"location":"part2-advanced/sweep-syntax/#prefix-notation","title":"Prefix Notation","text":"<p>You can apply an SI prefix to all values in a parameter using the prefix notation:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: &gt;-\n          lr/m=1,2,5,10           # Applies milli (1e-3) to all values\n          batch_size/k=4,8        # Applies kilo (1e3) to all values\n</code></pre> <p>This is useful when all values for a parameter share the same exponent.</p>"},{"location":"part2-advanced/sweep-syntax/#grouping-with-parentheses","title":"Grouping with Parentheses","text":"<p>You can use parentheses to create combinations of values:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: &gt;-\n          model=(cnn,transformer)_(small,large)  # Combines model types and sizes\n</code></pre> <p>This generates:</p> <pre><code>python train.py -m model=cnn_small\npython train.py -m model=cnn_large\npython train.py -m model=transformer_small\npython train.py -m model=transformer_large\n</code></pre> <p>Parentheses are particularly useful for combining values with SI prefixes:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: &gt;-\n          weight_decay=(1,4)k,(6,8)M  # Combines values with different prefixes\n</code></pre>"},{"location":"part2-advanced/sweep-syntax/#pipe-operator-for-multiple-parameter-sets","title":"Pipe Operator for Multiple Parameter Sets","text":"<p>The pipe operator (<code>|</code>) allows you to specify completely different parameter sets that are executed independently:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: &gt;-\n          model=small,large|lr=0.1,0.2|dropout=1:5:2 decay=1,2\n</code></pre> <p>This generates separate Hydra multirun commands:</p> <pre><code># The pipe operator creates separate Hydra multirun commands\npython train.py -m model=small,large decay=1\npython train.py -m model=small,large decay=2\npython train.py -m lr=0.1,0.2 decay=1\npython train.py -m lr=0.1,0.2 decay=2\npython train.py -m dropout=1,3,5 decay=1\npython train.py -m dropout=1,3,5 decay=2\n</code></pre> <p>The pipe operator splits the expression into separate Hydra multirun commands. Each section before a pipe becomes a separate command with its own grid sweep. Parameters that appear after all pipes (like <code>decay=1,2</code> in the example) are applied to every section and expanded as well.</p> <p>This is fundamentally different from <code>each</code> without pipes, which would create a single grid of all combinations. The pipe operator allows you to run completely independent parameter sweeps in the same job.</p> <p>A practical use case is to group similar configurations while separating dissimilar ones:</p> <pre><code>jobs:\n  train:\n    run: python train.py\n    sets:\n      - each: model=small,medium|large\n</code></pre> <p>This is equivalent to <code>model=small,medium|model=large</code> and would generate two separate command sets:</p> <pre><code>python train.py -m model=small,medium\npython train.py -m model=large\n</code></pre> <p>This allows you to group smaller models (small and medium) in one job while running the large model in a separate job, which is useful when you want to allocate resources differently based on model size.</p>"},{"location":"part2-advanced/sweep-syntax/#summary","title":"Summary","text":"<p>HydraFlow's extended sweep syntax provides several powerful features for parameter space exploration:</p> <ol> <li>Basic comma-separated lists - Simple way to enumerate discrete parameter values</li> <li>Numerical ranges - Define continuous ranges with start:stop:step notation (inclusive of stop value)</li> <li>SI prefixes - Use scientific notation shortcuts (n, u, m, k, M, G, etc.) for large/small numbers</li> <li>Prefix notation - Apply SI prefixes to all values in a parameter list</li> <li>Parentheses grouping - Create combinations of values and nested structures</li> <li>Pipe operator - Run multiple independent parameter sweeps in the same job</li> </ol> <p>All of these can be combined to create complex, expressive parameter sweeps with minimal configuration. Remember that using the <code>each</code> keyword creates a cartesian product of all parameters (all possible combinations), while the pipe operator (<code>|</code>) creates separate, independent parameter sweeps.</p> <p>When using these features, HydraFlow will automatically generate the appropriate Hydra multirun commands with the <code>-m</code> flag.</p>"},{"location":"part3-analysis/","title":"Analyzing Results with HydraFlow","text":"<p>After running experiments, the next critical step is analyzing and comparing results to derive insights. This section covers HydraFlow's powerful tools for accessing, analyzing, and visualizing experiment data.</p>"},{"location":"part3-analysis/#overview","title":"Overview","text":"<p>Part 3 provides a comprehensive API for working with experiment results, enabling you to:</p> <ul> <li>Load and access experiment data from MLflow runs</li> <li>Filter and group experiments based on configuration parameters</li> <li>Transform experiment data into structured formats for analysis</li> </ul>"},{"location":"part3-analysis/#key-components","title":"Key Components","text":"<p>The main components of HydraFlow's analysis tools are:</p> <ol> <li> <p><code>Run</code> Class: Represents a single experiment    run, providing access to configuration and artifacts.</p> </li> <li> <p><code>Collection</code> Class: A generic base class    implementing the <code>Sequence</code> protocol with powerful filtering, grouping, and data    extraction capabilities.</p> </li> <li> <p><code>RunCollection</code> Class:    A collection of <code>Run</code> instances with specialized tools for filtering, grouping, and    aggregating results, built on top of the <code>Collection</code> class.</p> </li> <li> <p>Data Analysis Integration: Tools to convert experiment data into    Polars DataFrames for advanced analysis.</p> </li> </ol>"},{"location":"part3-analysis/#practical-examples","title":"Practical Examples","text":"<p>For hands-on examples of experiment analysis, check out our Practical Tutorials section, specifically:</p> <ul> <li>Analyzing Experiment Results: A detailed tutorial demonstrating how to load, filter, group, and analyze experiment data using HydraFlow's APIs</li> </ul>"},{"location":"part3-analysis/#basic-analysis-workflow","title":"Basic Analysis Workflow","text":"<pre><code>from hydraflow import Run\n\n# Load experiment runs\nruns = Run.load([\"path/to/run1\", \"path/to/run2\", \"path/to/run3\"])\n\n# Filter runs based on configuration\nfiltered_runs = runs.filter(learning_rate=0.01, model_type=\"transformer\")\n\n# Group runs by a parameter\ngrouped_runs = runs.group_by(\"batch_size\")\n\n# Aggregate grouped data\ndf_aggregated = grouped_runs.agg(\n    count=lambda runs: len(runs),\n    avg_accuracy=lambda runs: sum(run.get(\"accuracy\", 0) for run in runs) / len(runs)\n)\n\n# Convert to DataFrame for analysis\ndf = runs.to_frame(\"learning_rate\", \"batch_size\", accuracy=lambda run: run.get(\"accuracy\"))\n\n# Perform analysis on the DataFrame\nbest_run = df.sort(\"accuracy\", descending=True).first()\n</code></pre>"},{"location":"part3-analysis/#finding-and-loading-runs","title":"Finding and Loading Runs","text":"<p>HydraFlow provides utilities to easily find and load runs from your MLflow tracking directory:</p> <pre><code>from hydraflow import Run\nfrom hydraflow.core.io import iter_run_dirs\n\n# Find all runs in the tracking directory\nruns = Run.load(iter_run_dirs())\n\n# Load runs from specific experiments\nruns = Run.load(iter_run_dirs(\"my_experiment\"))\n\n# Load runs from multiple experiments using patterns\nruns = Run.load(iter_run_dirs([\"training_*\", \"finetuning_*\"]))\n</code></pre> <p>This approach makes it easy to gather all relevant runs for analysis without having to manually specify each run directory.</p>"},{"location":"part3-analysis/#type-safe-analysis","title":"Type-Safe Analysis","text":"<p>HydraFlow supports type-safe analysis through type parameters:</p> <pre><code>from dataclasses import dataclass\nfrom hydraflow import Run\n\n@dataclass\nclass MyConfig:\n    learning_rate: float\n    batch_size: int\n    model_type: str\n\n# Load runs with type information\nruns = Run[MyConfig].load([\"path/to/run1\", \"path/to/run2\"])\n\n# Type-checked access to configuration\nfor run in runs:\n    # IDE will provide auto-completion for cfg properties\n    print(f\"LR: {run.cfg.learning_rate}, Model: {run.cfg.model_type}\")\n</code></pre>"},{"location":"part3-analysis/#implementation-support","title":"Implementation Support","text":"<p>HydraFlow can integrate with custom implementation classes to provide domain-specific functionality:</p> <pre><code>from hydraflow import Run\nfrom pathlib import Path\n\nclass ModelAnalyzer:\n    def __init__(self, artifacts_dir: Path, cfg: MyConfig | None = None):\n        self.artifacts_dir = artifacts_dir\n        self.cfg = cfg\n\n    def load_model(self):\n        return torch.load(self.artifacts_dir / \"model.pt\")\n\n    def analyze_performance(self):\n        # Custom analysis logic\n        pass\n\n# Load a run with implementation\nrun = Run[MyConfig, ModelAnalyzer].load(\"path/to/run\", ModelAnalyzer)\n\n# Access implementation methods\nmodel = run.impl.load_model()\nresults = run.impl.analyze_performance()\n</code></pre> <p>The analysis capabilities covered in Part 3 are designed to work seamlessly with the experiment definitions from Part 1 and the advanced workflow automation from Part 2.</p>"},{"location":"part3-analysis/#whats-next","title":"What's Next","text":"<p>In the following pages, we'll explore HydraFlow's analysis tools in detail:</p> <ul> <li> <p>Run Class: Learn how to use the <code>Run</code>   class to access and analyze individual experiment runs.</p> </li> <li> <p>Run Collection: Discover the powerful features of the   <code>RunCollection</code> class for   working with multiple runs.</p> </li> <li> <p>Updating Runs: Learn how to update existing runs with   new metrics, tags, and artifacts.</p> </li> </ul>"},{"location":"part3-analysis/run-class/","title":"Run Class","text":"<p>The <code>Run</code> class is a fundamental component of HydraFlow's analysis toolkit, representing a single execution of an experiment. It provides structured access to all data associated with a run, including configuration and artifacts.</p>"},{"location":"part3-analysis/run-class/#basic-usage","title":"Basic Usage","text":"<p>To work with a run, first load it using either the constructor or the <code>Run.load</code> class method:</p> <pre><code>from hydraflow import Run\nfrom pathlib import Path\n\n# Using constructor with Path object\nrun_dir = Path(\"mlruns/exp_id/run_id\")\nrun = Run(run_dir)\n\n# Using load method with string path\nrun = Run.load(\"mlruns/exp_id/run_id\")\n</code></pre>"},{"location":"part3-analysis/run-class/#access-run-data","title":"Access Run Data","text":"<p>The <code>Run</code> class provides access to run information and configuration.</p>"},{"location":"part3-analysis/run-class/#run-information","title":"Run Information","text":"<p>The <code>info</code> attribute provides the following information:</p> <pre><code>print(f\"Run ID: {run.info.run_id}\")\nprint(f\"Run Directory: {run.info.run_dir}\")\nprint(f\"Job name: {run.info.job_name}\")\n</code></pre>"},{"location":"part3-analysis/run-class/#run-configuration","title":"Run Configuration","text":"<p>The <code>cfg</code> attribute provides the entire configuration:</p> <pre><code># Access entire configuration\nprint(f\"Configuration: {run.cfg}\")\n</code></pre> <p>You can also access configuration values by key using the <code>get</code> method:</p> <pre><code># Access configuration by key\nlearning_rate = run.get(\"learning_rate\")\n\n# Nested access with dot notation\nmodel_type = run.get(\"model.type\")\n\n# Alternatively, use double underscore notation for nested access\nmodel_type = run.get(\"model__type\")  # Equivalent to \"model.type\"\n\n# Access implementation attributes or run info\nmetric_value = run.get(\"accuracy\")  # From impl or cfg\nrun_id = run.get(\"run_id\")  # From RunInfo\n\n# Access special object keys\ncfg = run.get(\"cfg\")    # Returns the complete configuration object\nimpl = run.get(\"impl\")  # Returns the implementation object\ninfo = run.get(\"info\")  # Returns the run information object\n\n# Provide a default value if the key doesn't exist\nbatch_size = run.get(\"batch_size\", 32)\n\n# Use a callable as default to dynamically generate values based on the run\n# This is useful for derived parameters or conditional defaults\nlr = run.get(\"learning_rate\", default=lambda r: r.get(\"base_lr\", 0.01) / 10)\n</code></pre> <p>The <code>get</code> method searches for values in the following order:</p> <ol> <li>In the configuration (<code>cfg</code>)</li> <li>In the implementation instance (<code>impl</code>)</li> <li>In the run information (<code>info</code>)</li> <li>In the run object itself (<code>self</code>)</li> </ol> <p>This provides a unified access interface regardless of where the data is stored.</p> <p>The double underscore notation (<code>__</code>) is automatically converted to dot notation (<code>.</code>) internally, making it useful for nested parameter access, especially when using keyword arguments in methods that don't allow dots in parameter names.</p> <p>When providing a default value, you can use either a static value or a callable function. If you provide a callable, it will receive the Run instance as an argument, allowing you to create context-dependent default values that can access other run parameters or properties. This is particularly useful for:</p> <ul> <li>Creating derived parameters that don't exist in the original configuration</li> <li>Handling schema evolution across different experiment iterations</li> <li>Providing fallbacks that depend on other configuration values</li> <li>Implementing conditional logic for parameter defaults</li> </ul>"},{"location":"part3-analysis/run-class/#type-safe-configuration-access","title":"Type-Safe Configuration Access","text":"<p>For better IDE integration and type checking, you can specify the configuration type as a type parameter:</p> <pre><code>from dataclasses import dataclass\nfrom hydraflow import Run\n\n@dataclass\nclass ModelConfig:\n    type: str\n    hidden_size: int\n\n@dataclass\nclass TrainingConfig:\n    learning_rate: float\n    batch_size: int\n    epochs: int\n\n@dataclass\nclass Config:\n    model: ModelConfig\n    training: TrainingConfig\n    seed: int = 42\n\n# Create a typed Run instance\nrun = Run[Config](run_dir)\n\n# Type-safe access with IDE auto-completion\nmodel_type = run.cfg.model.type\nlr = run.cfg.training.learning_rate\nseed = run.cfg.seed\n</code></pre>"},{"location":"part3-analysis/run-class/#custom-implementation-classes","title":"Custom Implementation Classes","text":"<p>The <code>Run</code> class can be extended with custom implementation classes to add domain-specific functionality:</p> <pre><code>from pathlib import Path\nfrom hydraflow import Run\n\nclass ModelLoader:\n    def __init__(self, artifacts_dir: Path):\n        self.artifacts_dir = artifacts_dir\n\n    def load_weights(self):\n        \"\"\"Load the model weights from the artifacts directory.\"\"\"\n        return torch.load(self.artifacts_dir / \"weights.pt\")\n\n    def evaluate(self, test_data):\n        \"\"\"Evaluate the model on test data.\"\"\"\n        model = self.load_weights()\n        return model.evaluate(test_data)\n\n# Create a Run with implementation\nrun = Run[Config, ModelLoader](run_dir, ModelLoader)\n</code></pre> <p>The <code>impl</code> attribute provides access to the implementation class instance:</p> <pre><code># Access implementation methods\nweights = run.impl.load_weights()\nresults = run.impl.evaluate(test_data)\n</code></pre>"},{"location":"part3-analysis/run-class/#configuration-aware-implementations","title":"Configuration-Aware Implementations","text":"<p>Implementation classes can optionally accept the run's configuration:</p> <pre><code>class AdvancedModelLoader:\n    def __init__(self, artifacts_dir: Path, cfg: Config | None = None):\n        self.artifacts_dir = artifacts_dir\n        self.cfg = cfg\n\n    def load_model(self):\n        \"\"\"Load model using configuration parameters.\"\"\"\n        model_type = self.cfg.model.type\n        model_path = self.artifacts_dir / f\"{model_type}_model.pt\"\n        return torch.load(model_path)\n\n# The implementation will receive both artifacts_dir and cfg\nrun = Run[Config, AdvancedModelLoader](run_dir, AdvancedModelLoader)\nmodel = run.impl.load_model()  # Uses configuration information\n</code></pre>"},{"location":"part3-analysis/run-class/#converting-to-dataframe","title":"Converting to DataFrame","text":"<p>To convert a Run instance to a Polars DataFrame, use the <code>to_frame</code> method. This method adds the Run's information as columns to the DataFrame.</p> <pre><code># Basic usage\ndf = run.to_frame(\n    lambda r: DataFrame({\"value\": [1, 2, 3]}),\n    \"run_id\",\n    \"experiment_name\"\n)\n\n# With default values\ndf = run.to_frame(\n    lambda r: DataFrame({\"value\": [1, 2, 3]}),\n    \"run_id\",\n    (\"status\", lambda r: \"completed\")\n)\n</code></pre> <p>The <code>to_frame</code> method accepts the following parameters:</p> <ul> <li><code>function</code>: A function that takes a Run instance and returns a DataFrame</li> <li><code>*keys</code>: Keys for the Run's information to add. Accepts the following formats:<ul> <li>String: A simple key (e.g., \"run_id\")</li> <li>Tuple: A tuple of (key, default value or function returning default value)</li> </ul> </li> </ul>"},{"location":"part3-analysis/run-class/#loading-multiple-runs","title":"Loading Multiple Runs","text":"<p>The <code>load</code> class method can load both individual runs and collections of runs:</p> <pre><code># Load a single run\nrun = Run.load(\"mlruns/exp_id/run_id\")\n\n# Load multiple runs to create a RunCollection\nrun_dirs = [\"mlruns/exp_id/run_id1\", \"mlruns/exp_id/run_id2\"]\nruns = Run.load(run_dirs)\n\n# Load runs with parallel processing\nruns = Run.load(run_dirs, n_jobs=4)  # Use 4 parallel jobs for loading\nruns = Run.load(run_dirs, n_jobs=-1)  # Use all available CPU cores\n</code></pre>"},{"location":"part3-analysis/run-class/#finding-runs-with-iter_run_dirs","title":"Finding Runs with <code>iter_run_dirs</code>","text":"<p>HydraFlow provides the <code>iter_run_dirs</code> function to easily discover runs in your MLflow tracking directory:</p> <pre><code>from hydraflow.core.io import iter_run_dirs\nfrom hydraflow import Run\n\n# Find all runs in the tracking directory\nrun_dirs = list(iter_run_dirs())\nruns = Run.load(run_dirs)\n\n# Filter runs by experiment name\n# - Use a single experiment name\nruns = Run.load(iter_run_dirs(\"my_experiment\"))\n\n# - Use multiple experiment names (with pattern matching)\nruns = Run.load(iter_run_dirs([\"train_*\", \"eval_*\"]))\n\n# - Use a custom filtering function\ndef filter_experiments(name: str) -&gt; bool:\n    return name.startswith(\"train_\") and \"v2\" in name\n\nruns = Run.load(iter_run_dirs(filter_experiments))\n</code></pre> <p>The <code>iter_run_dirs</code> function yields paths to run directories that can be directly passed to <code>Run.load</code>. This makes it easy to find and load runs based on experiment names or custom filtering criteria.</p>"},{"location":"part3-analysis/run-class/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Type Parameters: Specify configuration types with <code>Run[Config]</code>    for better IDE support and type checking.</p> </li> <li> <p>Leverage Custom Implementations: Create domain-specific implementation    classes to encapsulate analysis logic.</p> </li> <li> <p>Use Parallel Loading: For large numbers of runs, use the    <code>n_jobs</code> parameter with <code>load</code> to speed up loading.</p> </li> <li> <p>Unified Data Access: Use the <code>get</code> method as a unified interface    to access data from all components (configuration, implementation, and run info).    It provides a consistent way to retrieve values regardless of where they are stored,    with a clear precedence order (cfg \u2192 impl \u2192 info).</p> </li> <li> <p>Default Values: When accessing potentially missing keys, use the    <code>get</code> method's default parameter: <code>run.get(\"key\", default_value)</code>.</p> </li> </ol>"},{"location":"part3-analysis/run-class/#summary","title":"Summary","text":"<p>The <code>Run</code> class provides a powerful interface for working with experiment runs in HydraFlow. Its type-safe configuration access, custom implementation support, and convenient loading mechanisms make it easy to analyze and compare experiment results effectively.</p>"},{"location":"part3-analysis/run-collection/","title":"Run Collection","text":"<p>The <code>RunCollection</code> class is a powerful tool for working with multiple experiment runs. It provides methods for filtering, grouping, and analyzing sets of <code>Run</code> instances, making it easy to compare and extract insights from your experiments.</p>"},{"location":"part3-analysis/run-collection/#architecture","title":"Architecture","text":"<p><code>RunCollection</code> is built on top of the more general <code>Collection</code> class, which provides a flexible foundation for working with sequences of items. This architecture offers several benefits:</p> <ol> <li>Consistent Interface: All collection-based classes in HydraFlow     share a common interface and behavior</li> <li>Code Reuse: Core functionality is implemented once in the base     class and inherited by specialized collections</li> <li>Extensibility: New collection types can easily be created     for different item types</li> <li>Type Safety: Generic type parameters ensure type checking     throughout the collection hierarchy</li> </ol> <p>The <code>Collection</code> class implements the Python <code>Sequence</code> protocol, allowing it to be used like standard Python collections (lists, tuples) while providing specialized methods for filtering, grouping, and data extraction.</p> <p><code>RunCollection</code> extends this foundation with run-specific functionality, particularly for working with MLflow experiment data. This layered design separates generic collection behavior from domain-specific operations.</p>"},{"location":"part3-analysis/run-collection/#creating-a-run-collection","title":"Creating a Run Collection","text":"<p>There are several ways to create a <code>RunCollection</code>:</p> <pre><code>from hydraflow import Run, RunCollection\nfrom pathlib import Path\n\n# Method 1: Using Run.load with multiple paths\nrun_dirs = [\"mlruns/exp_id/run_id1\", \"mlruns/exp_id/run_id2\"]\nruns = Run.load(run_dirs)\n\n# Method 2: Using a generator expression\nrun_dirs = Path(\"mlruns/exp_id\").glob(\"*\")\nruns = Run.load(run_dirs)\n\n# Method 3: Creating from a list of Run instances\nrun1 = Run(Path(\"mlruns/exp_id/run_id1\"))\nrun2 = Run(Path(\"mlruns/exp_id/run_id2\"))\nruns = RunCollection([run1, run2])\n\n# Method 4: Using iter_run_dirs to find runs dynamically\nfrom hydraflow import iter_run_dirs\n\n# Find all runs in a tracking directory\nruns = Run.load(iter_run_dirs())\n\n# Find runs from specific experiments\nruns = Run.load(iter_run_dirs([\"experiment1\", \"experiment2\"]))\n\n# Use pattern matching for experiment names\nruns = Run.load(iter_run_dirs(\"transformer_*\"))\n\n# Use a custom filter function for experiment names\ndef is_recent_version(name: str) -&gt; bool:\n    return name.startswith(\"model_\") and \"v2\" in name\n\nruns = Run.load(iter_run_dirs(is_recent_version))\n</code></pre>"},{"location":"part3-analysis/run-collection/#basic-operations","title":"Basic Operations","text":"<p>The <code>RunCollection</code> class supports common operations for working with collections:</p> <pre><code># Check the number of runs\nprint(f\"Number of runs: {len(runs)}\")\n\n# Iterate over runs\nfor run in runs:\n    print(f\"Run ID: {run.info.run_id}\")\n\n# Access individual runs by index\nfirst_run = runs[0]\nlast_run = runs[-1]\n\n# Slice the collection\nsubset = runs[1:4]  # Get runs 1, 2, and 3\n</code></pre>"},{"location":"part3-analysis/run-collection/#filtering-runs","title":"Filtering Runs","text":"<p>One of the most powerful features of <code>RunCollection</code> is the ability to filter runs based on configuration parameters or other criteria:</p> <pre><code># Filter by exact parameter value\ntransformer_runs = runs.filter(model_type=\"transformer\")\n\n# Filter with multiple conditions (AND logic)\nspecific_runs = runs.filter(\n    model_type=\"transformer\",\n    learning_rate=0.001,\n    batch_size=32\n)\n\n# Filter with dot notation for nested parameters\n# Use a tuple to specify the parameter name and value\nnested_filter = runs.filter((\"model.hidden_size\", 512))\n\n# Filter with double underscore notation for nested parameters\n# This is often more convenient with keyword arguments\nnested_filter = runs.filter(model__hidden_size=512)  # Equivalent to \"model.hidden_size\"\nnested_filter = runs.filter(model__encoder__num_layers=6)  # For deeply nested parameters\n\n# Filter with tuple for range values (inclusive)\nlr_range = runs.filter(learning_rate=(0.0001, 0.01))\n\n# Filter with list for multiple allowed values (OR logic)\nmultiple_models = runs.filter(model_type=[\"transformer\", \"lstm\"])\n\n# Filter by a predicate function\ndef is_large_image(run: Run):\n    return run.get(\"width\") + run.get(\"height\") &gt; 100\n\ngood_runs = runs.filter(is_large_image)\n</code></pre> <p>The double underscore notation (<code>__</code>) is particularly useful for accessing nested configuration parameters with keyword arguments, as it's automatically converted to dot notation (<code>.</code>) internally. This allows you to write more natural and Pythonic filtering expressions, especially for deeply nested configurations.</p>"},{"location":"part3-analysis/run-collection/#advanced-filtering","title":"Advanced Filtering","text":"<p>The <code>filter</code> method supports more complex filtering patterns:</p> <pre><code># Combine different filter types\ncomplex_filter = runs.filter(\n    model_type=[\"transformer\", \"lstm\"],\n    learning_rate=(0.0001, 0.01),\n    batch_size=32\n)\n\n# Chained filtering\nfinal_runs = runs.filter(model_type=\"transformer\").filter(learning_rate=0.001)\n\n# Advanced filtering using predicate functions with callable defaults\n# This example filters runs based on learning rate efficiency (lr * batch_size)\n# Even if some runs are missing one parameter, the default logic provides values\ndef has_efficient_lr(run: Run) -&gt; bool:\n    lr = run.get(\"learning_rate\", default=lambda r: r.get(\"base_lr\", 0.01) * r.get(\"lr_multiplier\", 1.0))\n    batch_size = run.get(\"batch_size\", default=lambda r: r.get(\"default_batch_size\", 32))\n    return lr * batch_size &lt; 0.5\n\n# Apply the complex predicate\nefficient_runs = runs.filter(has_efficient_lr)\n</code></pre> <p>The combination of predicate functions with callable defaults in <code>get</code> enables sophisticated filtering logic that can handle missing parameters and varied configuration schemas across different experiment runs.</p>"},{"location":"part3-analysis/run-collection/#sorting-runs","title":"Sorting Runs","text":"<p>The <code>sort</code> method allows you to sort runs based on specific criteria:</p> <pre><code># Sort by accuracy in descending order\nruns.sort(\"learning_rate\", reverse=True)\n\n# Sort by multiple keys\nruns.sort(\"learning_rate\", \"model_type\")\n</code></pre>"},{"location":"part3-analysis/run-collection/#getting-individual-runs","title":"Getting Individual Runs","text":"<p>While <code>filter</code> returns a <code>RunCollection</code>, the <code>get</code> method returns a single <code>Run</code> instance that matches the criteria:</p> <pre><code># Get a specific run (raises error if multiple or no matches are found)\nbest_run = runs.get(model_type=\"transformer\", learning_rate=0.001)\n\n# Try to get a specific run. If no match is found, return None\nfallback_run = runs.try_get(model_type=\"transformer\")\n\n# Get the first matching run.\nfirst_match = runs.first(model_type=\"transformer\")\n\n# Get the last matching run.\nlast_match = runs.last(model_type=\"transformer\")\n</code></pre>"},{"location":"part3-analysis/run-collection/#extracting-data","title":"Extracting Data","text":"<p>RunCollection provides several methods to extract specific data from runs:</p> <pre><code># Extract values for a specific key as a list\nlearning_rates = runs.to_list(\"learning_rate\")\n\n# Extract values with a static default for missing values\nbatch_sizes = runs.to_list(\"batch_size\", default=32)\n\n# Extract values with a callable default that dynamically computes values\n# This is particularly useful for handling missing parameters or derived values\naccuracies = runs.to_list(\"accuracy\", default=lambda run: run.get(\"val_accuracy\", 0.0) * 0.9)\n\n# Extract values as a NumPy array\nbatch_sizes = runs.to_numpy(\"batch_size\")\n\n# Extract with callable default for complex scenarios\nlearning_rates = runs.to_numpy(\n    \"learning_rate\",\n    default=lambda run: run.get(\"base_lr\", 0.01) * run.get(\"lr_schedule_factor\", 1.0)\n)\n\n# Extract values as a Polars Series\nlr_series = runs.to_series(\"learning_rate\")\n\n# Extract with a custom name for the series\nmodel_series = runs.to_series(\"model_type\", name=\"Model Architecture\")\n\n# Extract with callable default and custom name\neffective_lr = runs.to_series(\n    \"learning_rate\",\n    default=lambda run: run.get(\"base_lr\", 0.01) * run.get(\"lr_multiplier\", 1.0),\n    name=\"Effective Learning Rate\"\n)\n\n# Use Series for further analysis and operations\nimport polars as pl\n# Combine multiple series into a DataFrame\ndf = pl.DataFrame([\n    runs.to_series(\"model_type\", name=\"Model\"),\n    runs.to_series(\"batch_size\", default=32, name=\"Batch Size\"),\n    effective_lr\n])\n# Perform operations between Series\nnormalized_acc = runs.to_series(\"accuracy\", default=0.0, name=\"Accuracy\")\nefficiency = normalized_acc / effective_lr  # Series division\n\n# Get unique values for a key\nmodel_types = runs.unique(\"model_type\")\n\n# Count unique values\nnum_model_types = runs.n_unique(\"model_type\")\n</code></pre> <p>All data extraction methods (<code>to_list</code>, <code>to_numpy</code>, <code>to_series</code>, etc.) support both static and callable default values, matching the behavior of the <code>Run.get</code> method. When using a callable default, the function receives the Run instance as an argument, allowing you to:</p> <ul> <li>Implement fallback logic for missing parameters</li> <li>Create derived values based on multiple parameters</li> <li>Handle varying configuration schemas across different experiments</li> <li>Apply transformations to the raw parameter values</li> </ul> <p>This makes it much easier to work with heterogeneous collections of runs that might have different parameter sets or evolving configuration schemas.</p>"},{"location":"part3-analysis/run-collection/#converting-to-dataframe","title":"Converting to DataFrame","text":"<p>For advanced analysis, you can convert your runs to a Polars DataFrame:</p> <pre><code># DataFrame with run information and entire configuration\ndf = runs.to_frame()\n\n# DataFrame with specific configuration parameters\ndf = runs.to_frame(\"model_type\", \"learning_rate\", \"batch_size\")\n\n# Include Run, configuration, or implementation objects as columns\ndf = runs.to_frame(\"model_type\", \"learning_rate\", \"run\")  # Include Run objects\ndf = runs.to_frame(\"model_type\", \"cfg\")  # Include configuration objects\ndf = runs.to_frame(\"run_id\", \"run\", \"cfg\", \"impl\")  # Include all objects\n\n# Specify default values for missing parameters using the defaults parameter\ndf = runs.to_frame(\n    \"model_type\",\n    \"learning_rate\",\n    \"batch_size\",\n    defaults={\"learning_rate\": 0.01, \"batch_size\": 32}\n)\n\n# Missing values without defaults are represented as None (null) in the DataFrame\n# This allows for standard handling of missing data in Polars\nmissing_values_df = runs.to_frame(\"model_type\", \"parameter_that_might_be_missing\")\n\n# Filter rows with non-null values\nimport polars as pl\nvalid_rows = missing_values_df.filter(pl.col(\"parameter_that_might_be_missing\").is_not_null())\n\n# Fill null values after creating the DataFrame\nfilled_df = missing_values_df.with_columns(\n    pl.col(\"parameter_that_might_be_missing\").fill_null(\"default_value\")\n)\n</code></pre>"},{"location":"part3-analysis/run-collection/#concatenating-multiple-runs","title":"Concatenating Multiple Runs","text":"<p>To convert and concatenate multiple Run instances into a DataFrame, use the <code>concat</code> method. This method adds each Run's information as columns to the DataFrame and concatenates them.</p> <pre><code># Basic usage\ndf = run_collection.concat(\n    lambda r: DataFrame({\"value\": [1, 2, 3]}),\n    \"run_id\",\n    \"experiment_name\"\n)\n\n# With default values\ndf = run_collection.concat(\n    lambda r: DataFrame({\"value\": [1, 2, 3]}),\n    \"run_id\",\n    (\"status\", lambda r: \"completed\")\n)\n</code></pre> <p>The <code>concat</code> method accepts the following parameters:</p> <ul> <li><code>function</code>: A function that takes each Run instance and returns a DataFrame</li> <li><code>*keys</code>: Keys for the Run's information to add. Accepts the following formats:<ul> <li>String: A simple key (e.g., \"run_id\")</li> <li>Tuple: A tuple of (key, default value or function returning default value)</li> </ul> </li> </ul>"},{"location":"part3-analysis/run-collection/#grouping-runs","title":"Grouping Runs","text":"<p>The <code>group_by</code> method allows you to organize runs based on parameter values:</p> <pre><code># Group by a single parameter\nmodel_groups = runs.group_by(\"model_type\")\n\n# Group by nested parameter using dot notation\narchitecture_groups = runs.group_by(\"model.architecture\")\n\n# Iterate through groups\nfor model_type, group in model_groups.items():\n    print(f\"Model type: {model_type}, Runs: {len(group)}\")\n\n# Group by multiple parameters\nparam_groups = runs.group_by(\"model_type\", \"learning_rate\")\n\n# Mix of regular and nested parameters using double underscore notation\nparam_groups = runs.group_by(\"model_type\", \"model__hidden_size\", \"optimizer__learning_rate\")\n\n# Access a specific group\ntransformer_001_group = param_groups[(\"transformer\", 0.001)]\n\n# Aggregating grouped runs using the agg method\n# This returns a DataFrame with the aggregated results\nmodel_counts = model_groups.agg(count=lambda runs: len(runs))\nmodel_avg_loss = model_groups.agg(\n    avg_loss=lambda runs: sum(run.get(\"loss\", 0) for run in runs) / len(runs),\n    min_loss=lambda runs: min(run.get(\"loss\", float(\"inf\")) for run in runs)\n)\n</code></pre> <p>The <code>group_by</code> method returns a <code>GroupBy</code> instance that maps keys to <code>RunCollection</code> instances. This design allows you to:</p> <ul> <li>Work with each group as a separate <code>RunCollection</code> with all the   filtering, sorting, and analysis capabilities</li> <li>Perform custom operations on each group that might not be expressible   as simple aggregation functions</li> <li>Chain additional operations on specific groups that interest you</li> <li>Implement multi-stage analysis workflows where you need to maintain   the full run information at each step</li> </ul> <p>To perform aggregations on the grouped data, use the <code>agg</code> method on the GroupBy instance. This transforms the grouped data into a DataFrame with aggregated results. You can define multiple aggregation functions to compute different metrics across each group.</p> <p>This approach preserves all information in each group, giving you maximum flexibility for downstream analysis.</p>"},{"location":"part3-analysis/run-collection/#type-safe-run-collections","title":"Type-Safe Run Collections","text":"<p>Like the <code>Run</code> class, <code>RunCollection</code> supports type parameters for better IDE integration:</p> <pre><code>from dataclasses import dataclass\nfrom hydraflow import Run, RunCollection\n\n@dataclass\nclass ModelConfig:\n    type: str\n    hidden_size: int\n\n@dataclass\nclass Config:\n    model: ModelConfig\n    learning_rate: float\n    batch_size: int\n\n# Create a typed RunCollection\nrun_dirs = [\"mlruns/exp_id/run_id1\", \"mlruns/exp_id/run_id2\"]\nruns = Run[Config].load(run_dirs)\n\n# Type-safe access in iterations\nfor run in runs:\n    # IDE will provide auto-completion\n    model_type = run.cfg.model.type\n    lr = run.cfg.learning_rate\n</code></pre>"},{"location":"part3-analysis/run-collection/#implementation-aware-collections","title":"Implementation-Aware Collections","text":"<p>You can also create collections with custom implementation classes:</p> <pre><code>class ModelAnalyzer:\n    def __init__(self, artifacts_dir: Path, cfg: Config | None = None):\n        self.artifacts_dir = artifacts_dir\n        self.cfg = cfg\n\n    def load_model(self):\n        # Load the model from artifacts\n        pass\n\n    def evaluate(self, data):\n        # Evaluate the model\n        pass\n\n# Create a collection with implementation\nruns = Run[Config, ModelAnalyzer].load(run_dirs, ModelAnalyzer)\n\n# Access implementation methods\nfor run in runs:\n    model = run.impl.load_model()\n    results = run.impl.evaluate(test_data)\n</code></pre>"},{"location":"part3-analysis/run-collection/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Filter Early: Apply filters as early as possible    to reduce the number of runs you're working with.</p> </li> <li> <p>Use Type Parameters: Specify    configuration/implementation types    with <code>Run[Config]</code> or <code>Run[Config, Impl]</code> and    use <code>load</code> method to collect runs for better IDE support and    type checking.</p> </li> <li> <p>Chain Operations: Combine filtering, grouping,    and object extraction for efficient analysis workflows.</p> </li> <li> <p>Use DataFrame Integration: Convert to DataFrames    for complex analysis and visualization needs.</p> </li> </ol>"},{"location":"part3-analysis/run-collection/#summary","title":"Summary","text":"<p>The <code>RunCollection</code> class is a powerful tool for comparative analysis of machine learning experiments. Its filtering, grouping, and data extraction capabilities enable efficient extraction of insights from large sets of experiments, helping you identify optimal configurations and understand performance trends.</p>"},{"location":"part3-analysis/updating-runs/","title":"Updating Run Configurations","text":"<p>As machine learning projects evolve, configuration structures often change. The <code>update</code> method in HydraFlow provides a powerful way to handle these changes and work with runs from different periods in your project's lifecycle.</p>"},{"location":"part3-analysis/updating-runs/#the-configuration-evolution-problem","title":"The Configuration Evolution Problem","text":"<p>A common challenge in ML experimentation is dealing with changing configuration schemas:</p> <ol> <li>You start with a specific configuration structure</li> <li>As your project evolves, you add new parameters</li> <li>Now you have a mix of old and new runs with different configuration schemas</li> <li>You want to analyze all runs together, but filtering becomes problematic</li> </ol> <p>For example, imagine you've been training image models with a fixed aspect ratio, but later decided to parameterize this aspect ratio:</p> <pre><code># Old configuration (fixed aspect ratio)\n@dataclass\nclass ModelConfig:\n    width: int = 256\n    height: int = 256\n    # aspect_ratio is implicitly 1:1\n\n# New configuration (parameterized aspect ratio)\n@dataclass\nclass ModelConfig:\n    width: int = 256\n    height: int = 256\n    aspect_ratio: float = 1.0  # New parameter!\n</code></pre> <p>When you try to filter runs by <code>aspect_ratio</code>, older runs will lack this parameter, making consistent analysis difficult.</p>"},{"location":"part3-analysis/updating-runs/#using-the-update-method","title":"Using the Update Method","text":"<p>The <code>update</code> method solves this problem by allowing you to add missing configuration parameters to runs without altering existing values:</p> <pre><code>from hydraflow import Run\n\n# Load a mix of old and new runs\nruns = Run.load([\"old_run_dir\", \"new_run_dir\"])\n\n# Add aspect_ratio to runs that don't have it\nfor run in runs:\n    run.update(\"aspect_ratio\", 1.0)  # Add default value if missing\n\n# You can also use nested parameters with dot notation\nrun.update(\"model.aspect_ratio\", 1.0)\n\n# Or use double underscore notation for nested parameters\nrun.update(\"model__aspect_ratio\", 1.0)  # Equivalent to \"model.aspect_ratio\"\n\n# Now you can filter by aspect_ratio\nsquare_runs = runs.filter(aspect_ratio=1.0)\n</code></pre> <p>The <code>update</code> method only adds values if the key doesn't already exist. For runs that already have an <code>aspect_ratio</code> parameter, the original value is preserved.</p> <p>The double underscore notation (<code>__</code>) is automatically converted to dot notation (<code>.</code>) internally, making it particularly useful for working with nested configurations.</p>"},{"location":"part3-analysis/updating-runs/#batch-updates-with-runcollection","title":"Batch Updates with RunCollection","text":"<p>To simplify updating multiple runs, you can use the <code>RunCollection.update</code> method:</p> <pre><code># Update all runs at once\nruns.update(\"aspect_ratio\", 1.0)\n\n# Now all runs have the aspect_ratio parameter\n</code></pre>"},{"location":"part3-analysis/updating-runs/#dynamic-updates-with-callables","title":"Dynamic Updates with Callables","text":"<p>You can provide a callable function instead of a fixed value to compute parameters dynamically:</p> <pre><code># Calculate aspect_ratio from width and height\ndef calculate_aspect_ratio(run: Run) -&gt; float:\n    width = run.get(\"width\", 0)  # Use default value if key doesn't exist\n    height = run.get(\"height\", 0)\n    if height == 0:\n        return 1.0\n    return width / height\n\n# Update with calculated values\nruns.update(\"aspect_ratio\", calculate_aspect_ratio)\n\n# Combine with callable defaults for more sophisticated logic\nruns.update(\"normalized_lr\", lambda run: run.get(\n    \"learning_rate\",\n    default=lambda r: r.get(\"base_lr\", 0.01) * r.get(\"lr_multiplier\", 1.0)\n) / 10)\n</code></pre> <p>The combination of dynamic updates with callable defaults in <code>get</code> provides a powerful mechanism for handling complex configuration scenarios and parameter dependencies. This approach allows you to:</p> <ol> <li>First look for existing values with custom fallback logic using <code>get</code> with callable defaults</li> <li>Then compute new parameters based on those values with <code>update</code></li> <li>Finally ensure all runs have consistent parameters for analysis</li> </ol>"},{"location":"part3-analysis/updating-runs/#updating-multiple-parameters","title":"Updating Multiple Parameters","text":"<p>You can update multiple related parameters at once by using a tuple of keys:</p> <pre><code># Update both width and height parameters\nruns.update(\n    (\"width\", \"height\"),\n    (640, 480)\n)\n\n# Update with calculated values\ndef calculate_dimensions(run: Run) -&gt; tuple[int, int]:\n    base_size = run.get(\"base_size\", 256)  # Default value if key doesn't exist\n    return (base_size, base_size)\n\nruns.update((\"width\", \"height\"), calculate_dimensions)\n</code></pre>"},{"location":"part3-analysis/updating-runs/#forcing-updates","title":"Forcing Updates","text":"<p>By default, <code>update</code> won't modify existing values. To override this behavior, use the <code>force</code> parameter:</p> <pre><code># Force update even if the parameter already exists\nruns.update(\"aspect_ratio\", 1.0, force=True)\n</code></pre>"},{"location":"part3-analysis/updating-runs/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Add Documentation: Comment your code to explain why updates are needed,    especially for future reference.</p> </li> <li> <p>Use Consistent Defaults: When adding missing parameters, use sensible    defaults that reflect the implicit values of older runs.</p> </li> <li> <p>Consider Dynamic Updates: When possible, compute missing values from    existing parameters to maintain consistency.</p> </li> <li> <p>Update Early: Apply updates early in your analysis pipeline, before    filtering or grouping.</p> </li> </ol>"},{"location":"part3-analysis/updating-runs/#summary","title":"Summary","text":"<p>The <code>update</code> method enables you to work with runs that have evolving configuration schemas. By adding missing parameters, you can treat old and new runs uniformly, enabling consistent analysis across your project's lifetime. This approach provides a form of \"duck typing\" for run configurations, allowing you to analyze runs based on their functional properties rather than their exact structure.</p>"},{"location":"practical-tutorials/","title":"Practical Tutorials","text":"<p>Welcome to the hands-on section of the HydraFlow documentation. These tutorials provide practical examples to help you understand how HydraFlow works in real-world scenarios.</p>"},{"location":"practical-tutorials/#what-youll-learn","title":"What You'll Learn","text":"<p>These tutorials demonstrate the core capabilities of HydraFlow through executable examples:</p> <ol> <li>Basic Application - Create and run a simple HydraFlow application</li> <li>Automated Workflows - Define and execute complex experiment workflows</li> <li>Results Analysis - Load, filter, and analyze experiment results</li> </ol> <p>Each tutorial includes:</p> <ul> <li>Complete working code examples</li> <li>Step-by-step explanations</li> <li>Real command outputs</li> <li>Practical insights</li> </ul>"},{"location":"practical-tutorials/#prerequisites","title":"Prerequisites","text":"<p>To follow these tutorials, you'll need:</p> <ul> <li>HydraFlow installed (Installation Guide)</li> <li>Basic understanding of Python</li> <li>Familiarity with machine learning experiment concepts</li> </ul>"},{"location":"practical-tutorials/#learning-path","title":"Learning Path","text":"<p>These tutorials are designed to be followed in sequence:</p> <ul> <li> Basic Application   Learn how to create and run a simple HydraFlow application</li> <li> Automated Workflows   Define complex experiment workflows using <code>hydraflow.yaml</code></li> <li> Results Analysis   Analyze experiment results using HydraFlow's powerful APIs</li> </ul> <p>By working through these tutorials, you'll gain a practical understanding of HydraFlow's capabilities and how it can streamline your machine learning experiments.</p>"},{"location":"practical-tutorials/#next-steps","title":"Next Steps","text":"<p>After completing these tutorials, you'll be ready to:</p> <ul> <li>Explore the detailed User Guide for in-depth explanations</li> <li>Apply HydraFlow to your own machine learning projects</li> <li>Customize HydraFlow's components for your specific needs</li> </ul>"},{"location":"practical-tutorials/advanced/","title":"Automating Complex Workflows","text":"<p>This tutorial demonstrates how to use HydraFlow's workflow automation capabilities to define, manage, and execute complex experiment workflows.</p> <p></p>"},{"location":"practical-tutorials/advanced/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this tutorial, you should:</p> <ol> <li>Understand basic HydraFlow applications (from the    Creating Your First Application tutorial)</li> <li>Have a basic understanding of YAML configuration files</li> </ol>"},{"location":"practical-tutorials/advanced/#project-structure","title":"Project Structure","text":"<p>First, let's examine our project structure:</p> <pre><code>./\n\u251c\u2500\u2500 example.py\n\u251c\u2500\u2500 hydraflow.yaml\n\u2514\u2500\u2500 submit.py\n</code></pre> <p>In this tutorial, we'll use:</p> <ul> <li><code>example.py</code>: Our basic HydraFlow application</li> <li><code>hydraflow.yaml</code>: A configuration file to define our experiment workflows</li> <li><code>submit.py</code>: A helper script for job submission</li> </ul>"},{"location":"practical-tutorials/advanced/#understanding-job-definitions","title":"Understanding Job Definitions","text":"<p>The <code>hydraflow.yaml</code> file allows you to define reusable experiment workflows:</p> hydraflow.yaml<pre><code>jobs:\n  job_sequential:\n    run: python example.py\n    sets:\n      - each: width=100,300\n        all: height=100:300:100\n  job_parallel:\n    run: python example.py\n    add: &gt;-\n      hydra/launcher=joblib\n      hydra.launcher.n_jobs=3\n    sets:\n      - each: width=200,400\n        all: height=100:300:100\n  job_submit:\n    submit: python submit.py example.py\n    sets:\n      - each: width=250:350:100\n        all: height=150,250\n</code></pre> <p>This configuration file defines three different types of jobs:</p> <ol> <li><code>job_sequential</code>: A job that runs sequentially</li> <li><code>job_parallel</code>: A job that runs with parallelization</li> <li><code>job_submit</code>: A job that uses a submit command for custom execution</li> </ol> <p>Each job demonstrates different execution patterns and parameter combinations.</p>"},{"location":"practical-tutorials/advanced/#using-the-hydraflow-cli","title":"Using the HydraFlow CLI","text":"<p>HydraFlow provides a command-line interface (CLI) for executing and managing jobs defined in your <code>hydraflow.yaml</code> file. The primary command is <code>hydraflow run</code>, which allows you to execute any job defined in your configuration.</p> <p>Basic usage: <pre><code>hydraflow run &lt;job_name&gt; [overrides]\n</code></pre></p> <p>Where:</p> <ul> <li><code>&lt;job_name&gt;</code> is the name of a job defined in <code>hydraflow.yaml</code></li> <li><code>[overrides]</code> are optional Hydra-style parameter overrides</li> </ul> <p>For more details on the CLI, see the Job Configuration documentation.</p>"},{"location":"practical-tutorials/advanced/#previewing-execution-with-dry-run","title":"Previewing Execution with Dry Run","text":"<p>Before executing our workflows, we can preview what will happen using the <code>--dry-run</code> flag:</p> <pre><code>$ hydraflow run job_sequential --dry-run\npython example.py --multirun width=100 height=100,200,300 hydra.job.name=job_sequential hydra.sweep.dir=multirun/01KHT4KQ4MK8A3V4KKNDKQXMKN\npython example.py --multirun width=300 height=100,200,300 hydra.job.name=job_sequential hydra.sweep.dir=multirun/01KHT4KQ4MK8A3V4KKNDKQXMKP\n</code></pre> <p>From the dry run output, we can observe:</p> <ul> <li>2 jobs will be executed (from the <code>each</code> parameter combinations)</li> <li>Each job contains 3 sweeps (from the <code>all</code> range values)</li> <li>Each job includes additional options:<ul> <li><code>hydra.job.name</code>: The name of the job defined in hydraflow.yaml</li> <li><code>hydra.sweep.dir</code>: A unique but time-ordered directory for each job created by HydraFlow</li> </ul> </li> </ul> <p>Standard Hydra creates directories based on the current date and time, which may cause duplication during parallel execution. HydraFlow solves this problem by creating unique, time-ordered directories for each job.</p>"},{"location":"practical-tutorials/advanced/#running-sequential-jobs","title":"Running Sequential Jobs","text":"<p>Let's examine the sequential job configuration:</p> <pre><code>job_sequential:\n  run: python example.py\n  sets:\n    - each: width=100,300\n      all: height=100:300:100\n</code></pre> <p>This job uses the <code>each</code> and <code>all</code> parameters to run multiple configuration combinations in sequence:</p> <pre><code>$ hydraflow run job_sequential\n[2026-02-19 05:05:25,293][HYDRA] Launching 3 jobs locally                       \n[2026-02-19 05:05:25,294][HYDRA]        #0 : width=100 height=100               \n2026/02/19 05:05:26 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:26 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:26 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:26 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:26 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:26 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:26 INFO mlflow.store.db.utils: Creating initial MLflow database\ntables...                                                                       \n2026/02/19 05:05:26 INFO mlflow.store.db.utils: Updating database tables        \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade  -&gt;         \n451aebb31d03, add metric step                                                   \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 451aebb31d03\n-&gt; 90e64c465722, migrate user column to tags                                    \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 90e64c465722\n-&gt; 181f10493468, allow nulls for metric values                                  \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 181f10493468\n-&gt; df50e92ffc5e, Add Experiment Tags Table                                      \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e\n-&gt; 7ac759974ad8, Update run tags with larger limit                              \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8\n-&gt; 89d4b8295536, create latest metrics table                                    \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 89d4b8295536\n-&gt; 2b4d017a5e9b, add model registry tables to db                                \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b\n-&gt; cfd24bdc0731, Update run status constraint with killed                       \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731\n-&gt; 0a8213491aaa, drop_duplicate_killed_constraint                               \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa\n-&gt; 728d730b5ebd, add registered model tags table                                \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd\n-&gt; 27a6a02d2cf1, add model version tags table                                   \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1\n-&gt; 84291f40a231, add run_link to model_version                                  \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 84291f40a231\n-&gt; a8c4a736bde6, allow nulls for run_id                                         \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6\n-&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary          \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05\n-&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql      \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade c48cb773bb87\n-&gt; bd07f7e963c5, create index on run_uuid                                       \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5\n-&gt; 0c779009ac13, add deleted_time field to runs table                           \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 0c779009ac13\n-&gt; cc1f77228345, change param value length to 500                               \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade cc1f77228345\n-&gt; 97727af70f4d, Add creation_time and last_update_time to experiments table    \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 97727af70f4d\n-&gt; 3500859a5d39, Add Model Aliases table                                        \n2026/02/19 05:05:26 INFO alembic.runtime.migration: Running upgrade 3500859a5d39\n-&gt; 7f2a7d5fae7d, add datasets inputs input_tags tables                          \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d\n-&gt; 2d6e25af4d3e, increase max param val length from 500 to 8000                 \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e\n-&gt; acf3f17fdcc7, add storage location field to model versions                   \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7\n-&gt; 867495a8f9d4, add trace tables                                               \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4\n-&gt; 5b0e9adcef9c, add cascade deletion to trace tables foreign keys              \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c\n-&gt; 4465047574b1, increase max dataset schema size                               \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 4465047574b1\n-&gt; f5a4f2784254, increase run tag value limit to 8000                           \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade f5a4f2784254\n-&gt; 0584bdc529eb, add cascading deletion to datasets from experiments            \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb\n-&gt; 400f98739977, add logged model tables                                        \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 400f98739977\n-&gt; 6953534de441, add step to inputs table                                       \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 6953534de441\n-&gt; bda7b8c39065, increase_model_version_tag_value_limit                         \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade bda7b8c39065\n-&gt; cbc13b556ace, add V3 trace schema columns                                    \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade cbc13b556ace\n-&gt; 770bee3ae1dd, add assessments table                                          \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd\n-&gt; a1b2c3d4e5f6, add spans table                                                \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6\n-&gt; de4033877273, create entity_associations table                               \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade de4033877273\n-&gt; 1a0cddfcaa16, Add webhooks and webhook_events tables                         \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16\n-&gt; 534353b11cbc, add scorer tables                                              \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 534353b11cbc\n-&gt; 71994744cf8e, add evaluation datasets                                        \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 71994744cf8e\n-&gt; 3da73c924c2f, add outputs to dataset record                                  \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f\n-&gt; bf29a5ff90ea, add jobs table                                                 \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea\n-&gt; 1bd49d398cd23, add secrets tables                                            \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade             \n1bd49d398cd23 -&gt; b7c8d9e0f1a2, add trace metrics table                          \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade b7c8d9e0f1a2\n-&gt; 5d2d30f0abce, update job table                                               \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 5d2d30f0abce\n-&gt; c9d4e5f6a7b8, add routing strategy to endpoints and linkage type to mappings \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade c9d4e5f6a7b8\n-&gt; 2c33131f4dae, add online_scoring_configs table                               \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Running upgrade 2c33131f4dae\n-&gt; d3e4f5a6b7c8, add display_name to endpoint_bindings                          \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:27 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n2026/02/19 05:05:27 INFO mlflow.tracking.fluent: Experiment with name           \n'job_sequential' does not exist. Creating a new experiment.                     \n[2026-02-19 05:05:27,392][__main__][INFO] - 93b36af24deb47ddb5a2b7c9cd341ede    \n[2026-02-19 05:05:27,393][__main__][INFO] - {'width': 100, 'height': 100}       \n[2026-02-19 05:05:27,401][HYDRA]        #1 : width=100 height=200               \n[2026-02-19 05:05:27,466][__main__][INFO] - edc0a0591f01491381e24279593e6032    \n[2026-02-19 05:05:27,466][__main__][INFO] - {'width': 100, 'height': 200}       \n[2026-02-19 05:05:27,471][HYDRA]        #2 : width=100 height=300               \n[2026-02-19 05:05:27,534][__main__][INFO] - dcb2da481c084feda02aff5db7e3a683    \n[2026-02-19 05:05:27,535][__main__][INFO] - {'width': 100, 'height': 300}       \n[2026-02-19 05:05:29,399][HYDRA] Launching 3 jobs locally                       \n[2026-02-19 05:05:29,399][HYDRA]        #0 : width=300 height=100               \n2026/02/19 05:05:29 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:29 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:29 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:29 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:29 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:29 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:29 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:29 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:29,945][__main__][INFO] - f3f540d194684be79aaca0358e4b21e1    \n[2026-02-19 05:05:29,945][__main__][INFO] - {'width': 300, 'height': 100}       \n[2026-02-19 05:05:29,953][HYDRA]        #1 : width=300 height=200               \n[2026-02-19 05:05:30,018][__main__][INFO] - aa5fb7f54ffc4b24971b9e6d5df1b5f5    \n[2026-02-19 05:05:30,018][__main__][INFO] - {'width': 300, 'height': 200}       \n[2026-02-19 05:05:30,022][HYDRA]        #2 : width=300 height=300               \n[2026-02-19 05:05:30,088][__main__][INFO] - cf72b5ebf7af4eb2b9d70d548035b13c    \n[2026-02-19 05:05:30,088][__main__][INFO] - {'width': 300, 'height': 300}       \n  0:00:06 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0:00:00 2/2 100%\n</code></pre> <p>Results of execution:</p> <ul> <li>An experiment named <code>job_sequential</code> is created</li> <li>2\u00d73=6 jobs are executed sequentially</li> <li>A progress bar is displayed to track completion</li> </ul>"},{"location":"practical-tutorials/advanced/#running-parallel-jobs","title":"Running Parallel Jobs","text":"<p>Now let's look at our parallel job configuration:</p> <pre><code>job_parallel:\n  run: python example.py\n  add: &gt;-\n    hydra/launcher=joblib\n    hydra.launcher.n_jobs=3\n  sets:\n    - each: width=200,400\n      all: height=100:300:100\n</code></pre> <p>This job leverages Hydra's parallel execution features using a joblib launcher via <code>add</code> parameter:</p> <pre><code>$ hydraflow run job_parallel --dry-run\npython example.py --multirun width=200 height=100,200,300 hydra.job.name=job_parallel hydra.sweep.dir=multirun/01KHT4KYAQ0ANTE0907W3357ET hydra/launcher=joblib hydra.launcher.n_jobs=3\npython example.py --multirun width=400 height=100,200,300 hydra.job.name=job_parallel hydra.sweep.dir=multirun/01KHT4KYAQ0ANTE0907W3357EV hydra/launcher=joblib hydra.launcher.n_jobs=3\n</code></pre> <pre><code>$ hydraflow run job_parallel\n[2026-02-19 05:05:32,769][HYDRA]                                                \nJoblib.Parallel(n_jobs=3,backend=loky,prefer=processes,require=None,verbose=0,ti\nmeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=Non\ne,mmap_mode=r) is launching 3 jobs                                              \n[2026-02-19 05:05:32,769][HYDRA] Launching jobs, sweep output dir :             \nmultirun/01KHT4KYQEPK0P9QZNRC22Y7R6                                             \n[2026-02-19 05:05:32,769][HYDRA]        #0 : width=200 height=100               \n[2026-02-19 05:05:32,769][HYDRA]        #1 : width=200 height=200               \n[2026-02-19 05:05:32,769][HYDRA]        #2 : width=200 height=300               \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:34 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:34 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n2026/02/19 05:05:34 INFO mlflow.tracking.fluent: Experiment with name           \n'job_parallel' does not exist. Creating a new experiment.                       \n[2026-02-19 05:05:34,721][__main__][INFO] - bb73b1977a6e47239f2b525e4b57fbf8    \n[2026-02-19 05:05:34,721][__main__][INFO] - {'width': 200, 'height': 200}       \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:34 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:35 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:35 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:35,237][__main__][INFO] - b7007d8c427548dc80195307e4b362c6    \n[2026-02-19 05:05:35,237][__main__][INFO] - {'width': 200, 'height': 100}       \n2026/02/19 05:05:35 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:35 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:35 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:35 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:35 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:35 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:35 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:35 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:35,693][__main__][INFO] - 286d4f065b274cc9b1fdb61069342c05    \n[2026-02-19 05:05:35,694][__main__][INFO] - {'width': 200, 'height': 300}       \n[2026-02-19 05:05:38,055][HYDRA]                                                \nJoblib.Parallel(n_jobs=3,backend=loky,prefer=processes,require=None,verbose=0,ti\nmeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=Non\ne,mmap_mode=r) is launching 3 jobs                                              \n[2026-02-19 05:05:38,055][HYDRA] Launching jobs, sweep output dir :             \nmultirun/01KHT4KYQEPK0P9QZNRC22Y7R7                                             \n[2026-02-19 05:05:38,055][HYDRA]        #0 : width=400 height=100               \n[2026-02-19 05:05:38,055][HYDRA]        #1 : width=400 height=200               \n[2026-02-19 05:05:38,055][HYDRA]        #2 : width=400 height=300               \n2026/02/19 05:05:39 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:39 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:39 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:39 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:39 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:39 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:39 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:39 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:40,001][__main__][INFO] - f3174e9983014f988ed6185b17a66a01    \n[2026-02-19 05:05:40,001][__main__][INFO] - {'width': 400, 'height': 200}       \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:40 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:40 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:40,531][__main__][INFO] - c4c6d2871bf44f39a2a35f487eff318f    \n[2026-02-19 05:05:40,531][__main__][INFO] - {'width': 400, 'height': 100}       \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:40 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:40 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:40 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:41,023][__main__][INFO] - 51b82388e65646638888c4983085f2e4    \n[2026-02-19 05:05:41,023][__main__][INFO] - {'width': 400, 'height': 300}       \n  0:00:10 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0:00:00 2/2 100%\n</code></pre> <p>Results of execution:</p> <ul> <li>An experiment named <code>job_parallel</code> is created</li> <li>The same Python script is used but with a different experiment name</li> <li>2 Python commands are executed sequentially</li> <li>Each Python command runs 3 jobs in parallel (using the <code>hydra/launcher=joblib</code> configuration)</li> </ul> <p>This demonstrates how HydraFlow makes Hydra's powerful parallel execution features easily accessible.</p>"},{"location":"practical-tutorials/advanced/#using-the-submit-command","title":"Using the Submit Command","text":"<p>For more complex execution patterns, HydraFlow provides the <code>submit</code> command. Here's our submit job configuration:</p> <pre><code>job_submit:\n  submit: python submit.py example.py\n  sets:\n    - each: width=250:350:100\n      all: height=150,250\n</code></pre> <p>The <code>submit</code> command requires two key components:</p> <ol> <li>Your HydraFlow application (<code>example.py</code> in this case)</li> <li>A command or script that will receive and process a parameter file</li> </ol> <p>Here's our implementation of the submit handler:</p> submit.py<pre><code>from __future__ import annotations\n\nimport shlex\nimport subprocess\nimport sys\nfrom pathlib import Path\n\n\ndef main() -&gt; None:\n    app_file, opt_file = sys.argv[1:]\n    text = Path(opt_file).read_text(encoding=\"utf-8\")\n\n    for line in text.splitlines():\n        opts = shlex.split(line)\n        args = [sys.executable, app_file, *opts]\n        print(args)\n        subprocess.run(args, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>How the <code>submit</code> command works:</p> <ol> <li>HydraFlow generates all parameter combinations based on your job configuration</li> <li>It writes these combinations to a temporary text file (one combination per line)</li> <li>It runs the command specified in the <code>submit</code> field of your <code>hydraflow.yaml</code></li> <li>It appends the temporary file path as the last argument to your command</li> </ol> <p>For example, with <code>submit: python submit.py example.py</code> in your configuration, the actual executed command will be something like: <pre><code>python submit.py example.py /tmp/hydraflow_parameters_12345.txt\n</code></pre></p> <p>Let's see it in action with a dry run:</p> <pre><code>$ hydraflow run job_submit --dry-run\npython submit.py example.py /home/runner/work/hydraflow/hydraflow/examples/tmperb4nggx\n--multirun width=250 height=150,250 hydra.job.name=job_submit hydra.sweep.dir=multirun/01KHT4M9K6RZTD9JKCBQKNQGFT\n--multirun width=350 height=150,250 hydra.job.name=job_submit hydra.sweep.dir=multirun/01KHT4M9K6RZTD9JKCBQKNQGFV\n</code></pre> <p>And now let's run it:</p> <pre><code>$ hydraflow run job_submit\n[2026-02-19 05:05:44,171][HYDRA] Launching 2 jobs locally\n[2026-02-19 05:05:44,172][HYDRA]    #0 : width=250 height=150\n2026/02/19 05:05:44 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n2026/02/19 05:05:44 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n2026/02/19 05:05:44 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n2026/02/19 05:05:44 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n2026/02/19 05:05:44 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n2026/02/19 05:05:44 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n2026/02/19 05:05:44 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2026/02/19 05:05:44 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n2026/02/19 05:05:44 INFO mlflow.tracking.fluent: Experiment with name 'job_submit' does not exist. Creating a new experiment.\n[2026-02-19 05:05:44,715][__main__][INFO] - 14d4b6c6ac824beaa16785c89eeee81a\n[2026-02-19 05:05:44,716][__main__][INFO] - {'width': 250, 'height': 150}\n[2026-02-19 05:05:44,723][HYDRA]    #1 : width=250 height=250\n[2026-02-19 05:05:44,788][__main__][INFO] - 4a758aaca4e94e70beae922a0aef786d\n[2026-02-19 05:05:44,788][__main__][INFO] - {'width': 250, 'height': 250}\n[2026-02-19 05:05:46,525][HYDRA] Launching 2 jobs locally\n[2026-02-19 05:05:46,525][HYDRA]    #0 : width=350 height=150\n2026/02/19 05:05:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n2026/02/19 05:05:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n2026/02/19 05:05:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n2026/02/19 05:05:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n2026/02/19 05:05:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n2026/02/19 05:05:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n2026/02/19 05:05:47 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2026/02/19 05:05:47 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n[2026-02-19 05:05:47,136][__main__][INFO] - fb45a76a45ef4838af0428b2fa6419d1\n[2026-02-19 05:05:47,136][__main__][INFO] - {'width': 350, 'height': 150}\n[2026-02-19 05:05:47,144][HYDRA]    #1 : width=350 height=250\n[2026-02-19 05:05:47,210][__main__][INFO] - 47dca7f385b24962a34c3dd25978c6dc\n[2026-02-19 05:05:47,210][__main__][INFO] - {'width': 350, 'height': 250}\n['/home/runner/work/hydraflow/hydraflow/.venv/bin/python', 'example.py', '--multirun', 'width=250', 'height=150,250', 'hydra.job.name=job_submit', 'hydra.sweep.dir=multirun/01KHT4M9Z80HPNARAQKR43Y0SQ']\n['/home/runner/work/hydraflow/hydraflow/.venv/bin/python', 'example.py', '--multirun', 'width=350', 'height=150,250', 'hydra.job.name=job_submit', 'hydra.sweep.dir=multirun/01KHT4M9Z80HPNARAQKR43Y0SR']\n</code></pre> <p>Our <code>submit.py</code> script implements a simple processor that:</p> <ol> <li>Accepts two arguments: the application file (<code>example.py</code>) and the parameter file</li> <li>Reads each line from the parameter file</li> <li>Runs the application with each set of parameters sequentially</li> </ol> <p>In real-world scenarios, you could customize this handler to:</p> <ul> <li>Submit jobs to compute clusters (SLURM, PBS, etc.)</li> <li>Implement custom scheduling logic</li> <li>Distribute workloads based on resource requirements</li> </ul>"},{"location":"practical-tutorials/advanced/#reviewing-results","title":"Reviewing Results","text":"<p>With HydraFlow, all important data is stored in MLflow, so we can safely delete the Hydra output directories:</p> <pre><code>$ rm -rf multirun\n</code></pre> <p>Let's check the directory structure:</p> <pre><code>./\n\u251c\u2500\u2500 mlruns/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 93b36af24deb47ddb5a2b7c9cd341ede/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 aa5fb7f54ffc4b24971b9e6d5df1b5f5/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 cf72b5ebf7af4eb2b9d70d548035b13c/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dcb2da481c084feda02aff5db7e3a683/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 edc0a0591f01491381e24279593e6032/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 f3f540d194684be79aaca0358e4b21e1/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 286d4f065b274cc9b1fdb61069342c05/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 51b82388e65646638888c4983085f2e4/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 b7007d8c427548dc80195307e4b362c6/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bb73b1977a6e47239f2b525e4b57fbf8/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 c4c6d2871bf44f39a2a35f487eff318f/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 f3174e9983014f988ed6185b17a66a01/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 3/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 14d4b6c6ac824beaa16785c89eeee81a/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 47dca7f385b24962a34c3dd25978c6dc/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 4a758aaca4e94e70beae922a0aef786d/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 fb45a76a45ef4838af0428b2fa6419d1/\n\u251c\u2500\u2500 example.py\n\u251c\u2500\u2500 hydraflow.yaml\n\u251c\u2500\u2500 mlflow.db\n\u2514\u2500\u2500 submit.py\n</code></pre> <p>After cleanup, we can observe:</p> <ul> <li>There are three experiments (one for each job type)</li> <li>Each experiment contains multiple runs</li> <li>A total of 16 runs were executed across all jobs</li> </ul>"},{"location":"practical-tutorials/advanced/#summary","title":"Summary","text":"<p>In this tutorial, you've learned how to:</p> <ol> <li>Define different types of experiment workflows in a <code>hydraflow.yaml</code> file</li> <li>Execute sequential and parallel job runs</li> <li>Use the <code>submit</code> command for custom execution patterns</li> <li>Preview jobs with dry runs</li> <li>Manage and organize experiment outputs</li> </ol> <p>These workflow automation capabilities allow you to efficiently manage complex experiment configurations, making your machine learning research more organized and reproducible.</p>"},{"location":"practical-tutorials/advanced/#next-steps","title":"Next Steps","text":"<p>Now that you've learned about workflow automation, try:</p> <ul> <li>Defining your own custom workflows</li> <li>Exploring more complex parameter sweep combinations</li> <li>Learning how to Analyze Results from your experiments</li> </ul> <p>For more detailed information, refer to:</p> <ul> <li>Part 1: Running Applications</li> <li>Part 2: Automating Workflows</li> <li>Part 3: Analyzing Results</li> </ul>"},{"location":"practical-tutorials/analysis/","title":"Analyzing Experiment Results","text":"<p>This tutorial demonstrates how to use HydraFlow's powerful analysis capabilities to work with your experiment results.</p> <p></p>"},{"location":"practical-tutorials/analysis/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this tutorial, you should:</p> <ol> <li>Understand the basic structure of a HydraFlow application    (from the Basic Application tutorial)</li> <li>Be familiar with the concept of job definitions    (from the Automated Workflows tutorial)</li> </ol>"},{"location":"practical-tutorials/analysis/#project-setup","title":"Project Setup","text":"<p>We'll start by running several experiments that we can analyze. We'll execute the three jobs defined in the Automated Workflows tutorial:</p> InputOutput <pre><code>$ hydraflow run job_sequential\n$ hydraflow run job_parallel\n$ hydraflow run job_submit\n</code></pre> <pre><code>[2026-02-19 05:05:49,434][HYDRA] Launching 3 jobs locally                       \n[2026-02-19 05:05:49,434][HYDRA]        #0 : width=100 height=100               \n2026/02/19 05:05:49 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:49 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:49 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:49 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:49 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:49 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:49 INFO mlflow.store.db.utils: Creating initial MLflow database\ntables...                                                                       \n2026/02/19 05:05:49 INFO mlflow.store.db.utils: Updating database tables        \n2026/02/19 05:05:49 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:49 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n2026/02/19 05:05:49 INFO alembic.runtime.migration: Running upgrade  -&gt;         \n451aebb31d03, add metric step                                                   \n2026/02/19 05:05:49 INFO alembic.runtime.migration: Running upgrade 451aebb31d03\n-&gt; 90e64c465722, migrate user column to tags                                    \n2026/02/19 05:05:49 INFO alembic.runtime.migration: Running upgrade 90e64c465722\n-&gt; 181f10493468, allow nulls for metric values                                  \n2026/02/19 05:05:49 INFO alembic.runtime.migration: Running upgrade 181f10493468\n-&gt; df50e92ffc5e, Add Experiment Tags Table                                      \n2026/02/19 05:05:49 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e\n-&gt; 7ac759974ad8, Update run tags with larger limit                              \n2026/02/19 05:05:49 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8\n-&gt; 89d4b8295536, create latest metrics table                                    \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 89d4b8295536\n-&gt; 2b4d017a5e9b, add model registry tables to db                                \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b\n-&gt; cfd24bdc0731, Update run status constraint with killed                       \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731\n-&gt; 0a8213491aaa, drop_duplicate_killed_constraint                               \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa\n-&gt; 728d730b5ebd, add registered model tags table                                \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd\n-&gt; 27a6a02d2cf1, add model version tags table                                   \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1\n-&gt; 84291f40a231, add run_link to model_version                                  \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 84291f40a231\n-&gt; a8c4a736bde6, allow nulls for run_id                                         \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6\n-&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary          \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05\n-&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql      \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade c48cb773bb87\n-&gt; bd07f7e963c5, create index on run_uuid                                       \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5\n-&gt; 0c779009ac13, add deleted_time field to runs table                           \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 0c779009ac13\n-&gt; cc1f77228345, change param value length to 500                               \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade cc1f77228345\n-&gt; 97727af70f4d, Add creation_time and last_update_time to experiments table    \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 97727af70f4d\n-&gt; 3500859a5d39, Add Model Aliases table                                        \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 3500859a5d39\n-&gt; 7f2a7d5fae7d, add datasets inputs input_tags tables                          \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d\n-&gt; 2d6e25af4d3e, increase max param val length from 500 to 8000                 \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e\n-&gt; acf3f17fdcc7, add storage location field to model versions                   \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7\n-&gt; 867495a8f9d4, add trace tables                                               \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4\n-&gt; 5b0e9adcef9c, add cascade deletion to trace tables foreign keys              \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c\n-&gt; 4465047574b1, increase max dataset schema size                               \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 4465047574b1\n-&gt; f5a4f2784254, increase run tag value limit to 8000                           \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade f5a4f2784254\n-&gt; 0584bdc529eb, add cascading deletion to datasets from experiments            \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb\n-&gt; 400f98739977, add logged model tables                                        \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 400f98739977\n-&gt; 6953534de441, add step to inputs table                                       \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 6953534de441\n-&gt; bda7b8c39065, increase_model_version_tag_value_limit                         \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade bda7b8c39065\n-&gt; cbc13b556ace, add V3 trace schema columns                                    \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade cbc13b556ace\n-&gt; 770bee3ae1dd, add assessments table                                          \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd\n-&gt; a1b2c3d4e5f6, add spans table                                                \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6\n-&gt; de4033877273, create entity_associations table                               \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade de4033877273\n-&gt; 1a0cddfcaa16, Add webhooks and webhook_events tables                         \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16\n-&gt; 534353b11cbc, add scorer tables                                              \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 534353b11cbc\n-&gt; 71994744cf8e, add evaluation datasets                                        \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 71994744cf8e\n-&gt; 3da73c924c2f, add outputs to dataset record                                  \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f\n-&gt; bf29a5ff90ea, add jobs table                                                 \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea\n-&gt; 1bd49d398cd23, add secrets tables                                            \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade             \n1bd49d398cd23 -&gt; b7c8d9e0f1a2, add trace metrics table                          \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade b7c8d9e0f1a2\n-&gt; 5d2d30f0abce, update job table                                               \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 5d2d30f0abce\n-&gt; c9d4e5f6a7b8, add routing strategy to endpoints and linkage type to mappings \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade c9d4e5f6a7b8\n-&gt; 2c33131f4dae, add online_scoring_configs table                               \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Running upgrade 2c33131f4dae\n-&gt; d3e4f5a6b7c8, add display_name to endpoint_bindings                          \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:50 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n2026/02/19 05:05:50 INFO mlflow.tracking.fluent: Experiment with name           \n'job_sequential' does not exist. Creating a new experiment.                     \n[2026-02-19 05:05:50,358][__main__][INFO] - 68a2b5edd5cc40618606d78f919fc02a    \n[2026-02-19 05:05:50,358][__main__][INFO] - {'width': 100, 'height': 100}       \n[2026-02-19 05:05:50,366][HYDRA]        #1 : width=100 height=200               \n[2026-02-19 05:05:50,431][__main__][INFO] - 3e7b4fe89b9844e5b44f7517ffe80d03    \n[2026-02-19 05:05:50,431][__main__][INFO] - {'width': 100, 'height': 200}       \n[2026-02-19 05:05:50,436][HYDRA]        #2 : width=100 height=300               \n[2026-02-19 05:05:50,498][__main__][INFO] - 9204a1aeb29e4f53b2fd5a3b92dbf682    \n[2026-02-19 05:05:50,498][__main__][INFO] - {'width': 100, 'height': 300}       \n[2026-02-19 05:05:52,471][HYDRA] Launching 3 jobs locally                       \n[2026-02-19 05:05:52,471][HYDRA]        #0 : width=300 height=100               \n2026/02/19 05:05:52 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:52 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:52 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:52 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:52 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:52 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:52 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:52 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:53,029][__main__][INFO] - 8482fedba1374b10ae80fc85ffa64547    \n[2026-02-19 05:05:53,029][__main__][INFO] - {'width': 300, 'height': 100}       \n[2026-02-19 05:05:53,037][HYDRA]        #1 : width=300 height=200               \n[2026-02-19 05:05:53,103][__main__][INFO] - 3e2bcaa2ebeb40249a7a4458464d1815    \n[2026-02-19 05:05:53,103][__main__][INFO] - {'width': 300, 'height': 200}       \n[2026-02-19 05:05:53,108][HYDRA]        #2 : width=300 height=300               \n[2026-02-19 05:05:53,174][__main__][INFO] - b43fbb2adfe8439f8c58801764107a58    \n[2026-02-19 05:05:53,174][__main__][INFO] - {'width': 300, 'height': 300}       \n  0:00:05 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0:00:00 2/2 100%\n[2026-02-19 05:05:55,436][HYDRA]                                                \nJoblib.Parallel(n_jobs=3,backend=loky,prefer=processes,require=None,verbose=0,ti\nmeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=Non\ne,mmap_mode=r) is launching 3 jobs                                              \n[2026-02-19 05:05:55,436][HYDRA] Launching jobs, sweep output dir :             \nmultirun/01KHT4MMXSKC9Z0184XXEK5D4J                                             \n[2026-02-19 05:05:55,436][HYDRA]        #0 : width=200 height=100               \n[2026-02-19 05:05:55,436][HYDRA]        #1 : width=200 height=200               \n[2026-02-19 05:05:55,436][HYDRA]        #2 : width=200 height=300               \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:57 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:57 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n2026/02/19 05:05:57 INFO mlflow.tracking.fluent: Experiment with name           \n'job_parallel' does not exist. Creating a new experiment.                       \n[2026-02-19 05:05:57,580][__main__][INFO] - f77a5f951642405182a52f169bcb11b5    \n[2026-02-19 05:05:57,580][__main__][INFO] - {'width': 200, 'height': 300}       \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:57 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:57 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:57 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:58,042][__main__][INFO] - e0e41fb39872471099902d01385f3152    \n[2026-02-19 05:05:58,043][__main__][INFO] - {'width': 200, 'height': 200}       \n2026/02/19 05:05:58 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:05:58 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:05:58 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:05:58 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:05:58 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:05:58 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:05:58 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:05:58 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:05:58,519][__main__][INFO] - e78289b325a54bddb8e281431a0070fd    \n[2026-02-19 05:05:58,519][__main__][INFO] - {'width': 200, 'height': 100}       \n[2026-02-19 05:06:01,038][HYDRA]                                                \nJoblib.Parallel(n_jobs=3,backend=loky,prefer=processes,require=None,verbose=0,ti\nmeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=Non\ne,mmap_mode=r) is launching 3 jobs                                              \n[2026-02-19 05:06:01,038][HYDRA] Launching jobs, sweep output dir :             \nmultirun/01KHT4MMXSKC9Z0184XXEK5D4K                                             \n[2026-02-19 05:06:01,038][HYDRA]        #0 : width=400 height=100               \n[2026-02-19 05:06:01,038][HYDRA]        #1 : width=400 height=200               \n[2026-02-19 05:06:01,038][HYDRA]        #2 : width=400 height=300               \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:06:03 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:06:03 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:06:03,317][__main__][INFO] - 3e84b17979904f9192046026c75220b2    \n[2026-02-19 05:06:03,317][__main__][INFO] - {'width': 400, 'height': 200}       \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:06:03 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:06:03 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:06:03 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:06:03,809][__main__][INFO] - 6150c233446c48959553f536c6be4867    \n[2026-02-19 05:06:03,809][__main__][INFO] - {'width': 400, 'height': 300}       \n2026/02/19 05:06:04 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.schemas                                                    \n2026/02/19 05:06:04 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.tables                                                     \n2026/02/19 05:06:04 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.types                                                      \n2026/02/19 05:06:04 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.constraints                                                \n2026/02/19 05:06:04 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.defaults                                                   \n2026/02/19 05:06:04 INFO alembic.runtime.plugins: setup plugin                  \nalembic.autogenerate.comments                                                   \n2026/02/19 05:06:04 INFO alembic.runtime.migration: Context impl SQLiteImpl.    \n2026/02/19 05:06:04 INFO alembic.runtime.migration: Will assume                 \nnon-transactional DDL.                                                          \n[2026-02-19 05:06:04,336][__main__][INFO] - 1c97b3a0bbcb4f669d0e2f05c0e4dcc2    \n[2026-02-19 05:06:04,336][__main__][INFO] - {'width': 400, 'height': 100}       \n  0:00:11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0:00:00 2/2 100%\n[2026-02-19 05:06:07,305][HYDRA] Launching 2 jobs locally\n[2026-02-19 05:06:07,305][HYDRA]    #0 : width=250 height=150\n2026/02/19 05:06:07 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n2026/02/19 05:06:07 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n2026/02/19 05:06:07 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n2026/02/19 05:06:07 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n2026/02/19 05:06:07 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n2026/02/19 05:06:07 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n2026/02/19 05:06:07 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2026/02/19 05:06:07 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n2026/02/19 05:06:07 INFO mlflow.tracking.fluent: Experiment with name 'job_submit' does not exist. Creating a new experiment.\n[2026-02-19 05:06:07,872][__main__][INFO] - 83bbb768556a4e9fb678d124f56ea4b9\n[2026-02-19 05:06:07,872][__main__][INFO] - {'width': 250, 'height': 150}\n[2026-02-19 05:06:07,880][HYDRA]    #1 : width=250 height=250\n[2026-02-19 05:06:07,945][__main__][INFO] - 04dc8df2f66245cf99d28c0d5d4d1e92\n[2026-02-19 05:06:07,946][__main__][INFO] - {'width': 250, 'height': 250}\n[2026-02-19 05:06:09,767][HYDRA] Launching 2 jobs locally\n[2026-02-19 05:06:09,767][HYDRA]    #0 : width=350 height=150\n2026/02/19 05:06:10 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n2026/02/19 05:06:10 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n2026/02/19 05:06:10 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n2026/02/19 05:06:10 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n2026/02/19 05:06:10 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n2026/02/19 05:06:10 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n2026/02/19 05:06:10 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2026/02/19 05:06:10 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n[2026-02-19 05:06:10,318][__main__][INFO] - 7a3670ff4d044db29a827c01d6e7fc1a\n[2026-02-19 05:06:10,318][__main__][INFO] - {'width': 350, 'height': 150}\n[2026-02-19 05:06:10,326][HYDRA]    #1 : width=350 height=250\n[2026-02-19 05:06:10,392][__main__][INFO] - 4f717bcf687440ae96c5dd3ca00a426f\n[2026-02-19 05:06:10,392][__main__][INFO] - {'width': 350, 'height': 250}\n['/home/runner/work/hydraflow/hydraflow/.venv/bin/python', 'example.py', '--multirun', 'width=250', 'height=150,250', 'hydra.job.name=job_submit', 'hydra.sweep.dir=multirun/01KHT4N0GP03DE8Z0WYEEACZ10']\n['/home/runner/work/hydraflow/hydraflow/.venv/bin/python', 'example.py', '--multirun', 'width=350', 'height=150,250', 'hydra.job.name=job_submit', 'hydra.sweep.dir=multirun/01KHT4N0GP03DE8Z0WYEEACZ11']\n</code></pre> <p></p> <p>After running these commands, our project structure looks like this:</p> <pre><code>./\n\u251c\u2500\u2500 mlruns/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 3e2bcaa2ebeb40249a7a4458464d1815/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 3e7b4fe89b9844e5b44f7517ffe80d03/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 68a2b5edd5cc40618606d78f919fc02a/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 8482fedba1374b10ae80fc85ffa64547/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 9204a1aeb29e4f53b2fd5a3b92dbf682/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 b43fbb2adfe8439f8c58801764107a58/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1c97b3a0bbcb4f669d0e2f05c0e4dcc2/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 3e84b17979904f9192046026c75220b2/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 6150c233446c48959553f536c6be4867/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 e0e41fb39872471099902d01385f3152/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 e78289b325a54bddb8e281431a0070fd/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 f77a5f951642405182a52f169bcb11b5/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 3/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 04dc8df2f66245cf99d28c0d5d4d1e92/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 4f717bcf687440ae96c5dd3ca00a426f/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 7a3670ff4d044db29a827c01d6e7fc1a/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 83bbb768556a4e9fb678d124f56ea4b9/\n\u251c\u2500\u2500 example.py\n\u251c\u2500\u2500 hydraflow.yaml\n\u251c\u2500\u2500 mlflow.db\n\u2514\u2500\u2500 submit.py\n</code></pre> <p>The <code>mlruns</code> directory contains all our experiment data. Let's explore how to access and analyze this data using HydraFlow's API.</p>"},{"location":"practical-tutorials/analysis/#discovering-runs","title":"Discovering Runs","text":""},{"location":"practical-tutorials/analysis/#finding-run-directories","title":"Finding Run Directories","text":"<p>HydraFlow provides the <code>iter_run_dirs</code> function to discover runs in your MLflow tracking directory:</p> <pre><code>&gt;&gt;&gt; from hydraflow import iter_run_dirs\n&gt;&gt;&gt; run_dirs = list(iter_run_dirs())\n&gt;&gt;&gt; print(len(run_dirs))\n&gt;&gt;&gt; for run_dir in run_dirs[:4]:\n...     print(run_dir)\n16\n/home/runner/work/hydraflow/hydraflow/examples/mlruns/3/7a3670ff4d044db29a827c01d6e7fc1a\n/home/runner/work/hydraflow/hydraflow/examples/mlruns/3/83bbb768556a4e9fb678d124f56ea4b9\n/home/runner/work/hydraflow/hydraflow/examples/mlruns/3/04dc8df2f66245cf99d28c0d5d4d1e92\n/home/runner/work/hydraflow/hydraflow/examples/mlruns/3/4f717bcf687440ae96c5dd3ca00a426f\n</code></pre> <p>This function finds all run directories in your MLflow tracking directory, making it easy to collect runs for analysis.</p>"},{"location":"practical-tutorials/analysis/#filtering-by-experiment-name","title":"Filtering by Experiment Name","text":"<p>You can filter runs by experiment name to focus on specific experiments:</p> <pre><code>&gt;&gt;&gt; print(len(list(iter_run_dirs(\"job_sequential\"))))\n&gt;&gt;&gt; names = [\"job_sequential\", \"job_parallel\"]\n&gt;&gt;&gt; print(len(list(iter_run_dirs(names))))\n&gt;&gt;&gt; print(len(list(iter_run_dirs(\"job_*\"))))\n6\n12\n16\n</code></pre> <p>As shown above, you can:</p> <ul> <li>Filter by a single experiment name</li> <li>Provide a list of experiment names</li> <li>Use pattern matching with wildcards</li> </ul>"},{"location":"practical-tutorials/analysis/#working-with-individual-runs","title":"Working with Individual Runs","text":""},{"location":"practical-tutorials/analysis/#loading-a-run","title":"Loading a Run","text":"<p>The <code>Run</code> class represents a single experiment run in HydraFlow:</p> <pre><code>&gt;&gt;&gt; from hydraflow import Run\n&gt;&gt;&gt; run_dirs = iter_run_dirs()\n&gt;&gt;&gt; run_dir = next(run_dirs)  # run_dirs is an iterator\n&gt;&gt;&gt; run = Run(run_dir)\n&gt;&gt;&gt; print(run)\n&gt;&gt;&gt; print(type(run))\nRun('7a3670ff4d044db29a827c01d6e7fc1a')\n&lt;class 'hydraflow.core.run.Run'&gt;\n</code></pre> <p>You can also use the <code>load</code> class method, which accepts both string paths and Path objects:</p> <pre><code>&gt;&gt;&gt; Run.load(str(run_dir))\n&gt;&gt;&gt; print(run)\nRun('7a3670ff4d044db29a827c01d6e7fc1a')\n</code></pre>"},{"location":"practical-tutorials/analysis/#accessing-run-information","title":"Accessing Run Information","text":"<p>Each Run instance provides access to run information and configuration:</p> <pre><code>&gt;&gt;&gt; print(run.info.run_dir)\n&gt;&gt;&gt; print(run.info.run_id)\n&gt;&gt;&gt; print(run.info.job_name)  # Hydra job name = MLflow experiment name\n/home/runner/work/hydraflow/hydraflow/examples/mlruns/3/7a3670ff4d044db29a827c01d6e7fc1a\n7a3670ff4d044db29a827c01d6e7fc1a\njob_submit\n</code></pre> <p>The configuration is available through the <code>cfg</code> attribute:</p> <pre><code>&gt;&gt;&gt; print(run.cfg)\n{'width': 350, 'height': 150}\n</code></pre>"},{"location":"practical-tutorials/analysis/#type-safe-configuration-access","title":"Type-Safe Configuration Access","text":"<p>For better IDE integration and type checking, you can specify the configuration type:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Config:\n    width: int = 1024\n    height: int = 768\n</code></pre> <pre><code>&gt;&gt;&gt; run = Run[Config](run_dir)\n&gt;&gt;&gt; print(run)\nRun('7a3670ff4d044db29a827c01d6e7fc1a')\n</code></pre> <p>When you use <code>Run[Config]</code>, your IDE will recognize <code>run.cfg</code> as having the specified type, enabling autocompletion and type checking.</p>"},{"location":"practical-tutorials/analysis/#accessing-configuration-values","title":"Accessing Configuration Values","text":"<p>The <code>get</code> method provides a unified interface to access values from a run:</p> <pre><code>&gt;&gt;&gt; print(run.get(\"width\"))\n&gt;&gt;&gt; print(run.get(\"height\"))\n350\n150\n</code></pre>"},{"location":"practical-tutorials/analysis/#adding-custom-implementations","title":"Adding Custom Implementations","text":""},{"location":"practical-tutorials/analysis/#basic-implementation","title":"Basic Implementation","text":"<p>You can extend runs with custom implementation classes to add domain-specific functionality:</p> <pre><code>from pathlib import Path\n\nclass Impl:\n    root_dir: Path\n\n    def __init__(self, root_dir: Path):\n        self.root_dir = root_dir\n\n    def __repr__(self) -&gt; str:\n        return f\"Impl({self.root_dir.stem!r})\"\n</code></pre> <pre><code>&gt;&gt;&gt; run = Run[Config, Impl](run_dir, Impl)\n&gt;&gt;&gt; print(run)\nRun[Impl]('7a3670ff4d044db29a827c01d6e7fc1a')\n</code></pre> <p>The implementation is lazily initialized when you first access the <code>impl</code> attribute:</p> <pre><code>&gt;&gt;&gt; print(run.impl)\n&gt;&gt;&gt; print(run.impl.root_dir)\nImpl('artifacts')\n/home/runner/work/hydraflow/hydraflow/examples/mlruns/3/7a3670ff4d044db29a827c01d6e7fc1a/artifacts\n</code></pre>"},{"location":"practical-tutorials/analysis/#configuration-aware-implementation","title":"Configuration-Aware Implementation","text":"<p>Implementations can also access the run's configuration:</p> <pre><code>from dataclasses import dataclass, field\n\n@dataclass\nclass Size:\n    root_dir: Path = field(repr=False)\n    cfg: Config\n\n    @property\n    def size(self) -&gt; int:\n        return self.cfg.width * self.cfg.height\n\n    def is_large(self) -&gt; bool:\n        return self.size &gt; 100000\n</code></pre> <pre><code>&gt;&gt;&gt; run = Run[Config, Size].load(run_dir, Size)\n&gt;&gt;&gt; print(run)\n&gt;&gt;&gt; print(run.impl)\n&gt;&gt;&gt; print(run.impl.size)\nRun[Size]('7a3670ff4d044db29a827c01d6e7fc1a')\nSize(cfg={'width': 350, 'height': 150})\n52500\n</code></pre> <p>This allows you to define custom analysis methods that use both the run's artifacts and its configuration.</p>"},{"location":"practical-tutorials/analysis/#working-with-multiple-runs","title":"Working with Multiple Runs","text":""},{"location":"practical-tutorials/analysis/#creating-a-run-collection","title":"Creating a Run Collection","text":"<p>The <code>RunCollection</code> class helps you analyze multiple runs:</p> <pre><code>&gt;&gt;&gt; run_dirs = iter_run_dirs()\n&gt;&gt;&gt; rc = Run[Config, Size].load(run_dirs, Size)\n&gt;&gt;&gt; print(rc)\nRunCollection(Run[Size], n=16)\n</code></pre> <p>The <code>load</code> method automatically creates a <code>RunCollection</code> when given multiple run directories.</p>"},{"location":"practical-tutorials/analysis/#basic-run-collection-operations","title":"Basic Run Collection Operations","text":"<p>You can perform basic operations on a collection:</p> <pre><code>&gt;&gt;&gt; print(rc.first())\n&gt;&gt;&gt; print(rc.last())\nRun[Size]('7a3670ff4d044db29a827c01d6e7fc1a')\nRun[Size]('3e7b4fe89b9844e5b44f7517ffe80d03')\n</code></pre>"},{"location":"practical-tutorials/analysis/#filtering-runs","title":"Filtering Runs","text":"<p>The <code>filter</code> method lets you select runs based on various criteria:</p> <pre><code>&gt;&gt;&gt; print(rc.filter(width=400))\nRunCollection(Run[Size], n=3)\n</code></pre> <p>You can use lists to filter by multiple values (OR logic):</p> <pre><code>&gt;&gt;&gt; print(rc.filter(height=[100, 300]))\nRunCollection(Run[Size], n=8)\n</code></pre> <p>Tuples create range filters (inclusive):</p> <pre><code>&gt;&gt;&gt; print(rc.filter(height=(100, 300)))\nRunCollection(Run[Size], n=16)\n</code></pre> <p>You can even use custom filter functions:</p> <pre><code>&gt;&gt;&gt; print(rc.filter(lambda r: r.impl.is_large()))\nRunCollection(Run[Size], n=1)\n</code></pre>"},{"location":"practical-tutorials/analysis/#finding-specific-runs","title":"Finding Specific Runs","text":"<p>The <code>get</code> method returns a single run matching your criteria:</p> <pre><code>&gt;&gt;&gt; run = rc.get(width=250, height=(100, 200))\n&gt;&gt;&gt; print(run)\n&gt;&gt;&gt; print(run.impl)\nRun[Size]('83bbb768556a4e9fb678d124f56ea4b9')\nSize(cfg={'width': 250, 'height': 150})\n</code></pre>"},{"location":"practical-tutorials/analysis/#converting-to-dataframes","title":"Converting to DataFrames","text":"<p>For data analysis, you can convert runs to a Polars DataFrame:</p> <pre><code>&gt;&gt;&gt; print(rc.to_frame(\"width\", \"height\", \"size\"))\nshape: (16, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 width \u2506 height \u2506 size  \u2502\n\u2502 ---   \u2506 ---    \u2506 ---   \u2502\n\u2502 i64   \u2506 i64    \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 350   \u2506 150    \u2506 52500 \u2502\n\u2502 250   \u2506 150    \u2506 37500 \u2502\n\u2502 250   \u2506 250    \u2506 62500 \u2502\n\u2502 350   \u2506 250    \u2506 87500 \u2502\n\u2502 200   \u2506 100    \u2506 20000 \u2502\n\u2502 \u2026     \u2506 \u2026      \u2506 \u2026     \u2502\n\u2502 300   \u2506 200    \u2506 60000 \u2502\n\u2502 300   \u2506 100    \u2506 30000 \u2502\n\u2502 300   \u2506 300    \u2506 90000 \u2502\n\u2502 100   \u2506 300    \u2506 30000 \u2502\n\u2502 100   \u2506 200    \u2506 20000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can add custom columns using callables:</p> <pre><code>&gt;&gt;&gt; print(rc.to_frame(\"width\", \"height\", is_large=lambda r: r.impl.is_large()))\nshape: (16, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 width \u2506 height \u2506 is_large \u2502\n\u2502 ---   \u2506 ---    \u2506 ---      \u2502\n\u2502 i64   \u2506 i64    \u2506 bool     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 350   \u2506 150    \u2506 false    \u2502\n\u2502 250   \u2506 150    \u2506 false    \u2502\n\u2502 250   \u2506 250    \u2506 false    \u2502\n\u2502 350   \u2506 250    \u2506 false    \u2502\n\u2502 200   \u2506 100    \u2506 false    \u2502\n\u2502 \u2026     \u2506 \u2026      \u2506 \u2026        \u2502\n\u2502 300   \u2506 200    \u2506 false    \u2502\n\u2502 300   \u2506 100    \u2506 false    \u2502\n\u2502 300   \u2506 300    \u2506 false    \u2502\n\u2502 100   \u2506 300    \u2506 false    \u2502\n\u2502 100   \u2506 200    \u2506 false    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Functions can return lists for multiple values:</p> <pre><code>&gt;&gt;&gt; def to_list(run: Run) -&gt; list[int]:\n...     return [2 * run.get(\"width\"), 3 * run.get(\"height\")]\n&gt;&gt;&gt; print(rc.to_frame(\"width\", from_list=to_list))\nshape: (16, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 width \u2506 from_list  \u2502\n\u2502 ---   \u2506 ---        \u2502\n\u2502 i64   \u2506 list[i64]  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 350   \u2506 [700, 450] \u2502\n\u2502 250   \u2506 [500, 450] \u2502\n\u2502 250   \u2506 [500, 750] \u2502\n\u2502 350   \u2506 [700, 750] \u2502\n\u2502 200   \u2506 [400, 300] \u2502\n\u2502 \u2026     \u2506 \u2026          \u2502\n\u2502 300   \u2506 [600, 600] \u2502\n\u2502 300   \u2506 [600, 300] \u2502\n\u2502 300   \u2506 [600, 900] \u2502\n\u2502 100   \u2506 [200, 900] \u2502\n\u2502 100   \u2506 [200, 600] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Or dictionaries for multiple named columns:</p> <pre><code>&gt;&gt;&gt; def to_dict(run: Run) -&gt; dict[int, str]:\n...     width2 = 2 * run.get(\"width\")\n...     name = f\"h{run.get('height')}\"\n...     return {\"width2\": width2, \"name\": name}\n&gt;&gt;&gt; print(rc.to_frame(\"width\", from_dict=to_dict))\nshape: (16, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 width \u2506 from_dict    \u2502\n\u2502 ---   \u2506 ---          \u2502\n\u2502 i64   \u2506 struct[2]    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 350   \u2506 {700,\"h150\"} \u2502\n\u2502 250   \u2506 {500,\"h150\"} \u2502\n\u2502 250   \u2506 {500,\"h250\"} \u2502\n\u2502 350   \u2506 {700,\"h250\"} \u2502\n\u2502 200   \u2506 {400,\"h100\"} \u2502\n\u2502 \u2026     \u2506 \u2026            \u2502\n\u2502 300   \u2506 {600,\"h200\"} \u2502\n\u2502 300   \u2506 {600,\"h100\"} \u2502\n\u2502 300   \u2506 {600,\"h300\"} \u2502\n\u2502 100   \u2506 {200,\"h300\"} \u2502\n\u2502 100   \u2506 {200,\"h200\"} \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"practical-tutorials/analysis/#grouping-runs","title":"Grouping Runs","text":"<p>The <code>group_by</code> method organizes runs by common attributes:</p> <pre><code>&gt;&gt;&gt; grouped = rc.group_by(\"width\")\n&gt;&gt;&gt; for key, group in grouped.items():\n...     print(key, group)\n350 RunCollection(Run[Size], n=2)\n250 RunCollection(Run[Size], n=2)\n200 RunCollection(Run[Size], n=3)\n400 RunCollection(Run[Size], n=3)\n100 RunCollection(Run[Size], n=3)\n300 RunCollection(Run[Size], n=3)\n</code></pre> <p>You can group by multiple keys:</p> <pre><code>&gt;&gt;&gt; grouped = rc.group_by(\"width\", \"height\")\n&gt;&gt;&gt; for key, group in grouped.items():\n...     print(key, group)\n(350, 150) RunCollection(Run[Size], n=1)\n(250, 150) RunCollection(Run[Size], n=1)\n(250, 250) RunCollection(Run[Size], n=1)\n(350, 250) RunCollection(Run[Size], n=1)\n(200, 100) RunCollection(Run[Size], n=1)\n(200, 200) RunCollection(Run[Size], n=1)\n(400, 100) RunCollection(Run[Size], n=1)\n(200, 300) RunCollection(Run[Size], n=1)\n(400, 300) RunCollection(Run[Size], n=1)\n(400, 200) RunCollection(Run[Size], n=1)\n(100, 100) RunCollection(Run[Size], n=1)\n(300, 200) RunCollection(Run[Size], n=1)\n(300, 100) RunCollection(Run[Size], n=1)\n(300, 300) RunCollection(Run[Size], n=1)\n(100, 300) RunCollection(Run[Size], n=1)\n(100, 200) RunCollection(Run[Size], n=1)\n</code></pre> <p>Adding aggregation functions using the <code>agg</code> method transforms the result into a DataFrame:</p> <pre><code>&gt;&gt;&gt; grouped = rc.group_by(\"width\")\n&gt;&gt;&gt; df = grouped.agg(n=lambda runs: len(runs))\n&gt;&gt;&gt; print(df)\nshape: (6, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 width \u2506 n   \u2502\n\u2502 ---   \u2506 --- \u2502\n\u2502 i64   \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 350   \u2506 2   \u2502\n\u2502 250   \u2506 2   \u2502\n\u2502 200   \u2506 3   \u2502\n\u2502 400   \u2506 3   \u2502\n\u2502 100   \u2506 3   \u2502\n\u2502 300   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"practical-tutorials/analysis/#summary","title":"Summary","text":"<p>In this tutorial, you've learned how to:</p> <ol> <li>Discover experiment runs in your MLflow tracking directory</li> <li>Load and access information from individual runs</li> <li>Add custom implementation classes for domain-specific analysis</li> <li>Filter, group, and analyze collections of runs</li> <li>Convert run data to DataFrames for advanced analysis</li> </ol> <p>These capabilities enable you to efficiently analyze your experiments and extract valuable insights from your machine learning workflows.</p>"},{"location":"practical-tutorials/analysis/#next-steps","title":"Next Steps","text":"<p>Now that you understand HydraFlow's analysis capabilities, you can:</p> <ul> <li>Dive deeper into the Run Class   and Run Collection documentation</li> <li>Explore advanced analysis techniques in the   Analyzing Results section</li> <li>Apply these analysis techniques to your own machine learning experiments</li> </ul>"},{"location":"practical-tutorials/applications/","title":"Creating Your First HydraFlow Application","text":"<p>This tutorial demonstrates how to create and run a basic HydraFlow application that integrates Hydra's configuration management with MLflow's experiment tracking.</p> <p></p>"},{"location":"practical-tutorials/applications/#prerequisites","title":"Prerequisites","text":"<p>Before you begin this tutorial, you should:</p> <ol> <li>Have HydraFlow installed (Installation Guide)</li> <li>Have a basic understanding of Python</li> </ol>"},{"location":"practical-tutorials/applications/#project-structure","title":"Project Structure","text":"<p>First, let's examine our project structure:</p> <pre><code>./\n\u251c\u2500\u2500 example.py\n\u251c\u2500\u2500 hydraflow.yaml\n\u2514\u2500\u2500 submit.py\n</code></pre> <p>In this tutorial, we will only use the <code>example.py</code> file.</p>"},{"location":"practical-tutorials/applications/#creating-a-basic-application","title":"Creating a Basic Application","text":"<p>Let's create a simple HydraFlow application that defines a configuration class and tracks experiment parameters:</p> example.py<pre><code>from __future__ import annotations\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING\n\nimport hydraflow\n\nif TYPE_CHECKING:\n    from mlflow.entities import Run\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Config:\n    width: int = 1024\n    height: int = 768\n\n\n@hydraflow.main(Config)\ndef app(run: Run, cfg: Config) -&gt; None:\n    logger.info(run.info.run_id)\n    logger.info(cfg)\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>"},{"location":"practical-tutorials/applications/#understanding-the-key-components","title":"Understanding the Key Components","text":"<p>Let's break down the essential parts of this application:</p> <ol> <li> <p>Configuration Class: A <code>dataclass</code> that defines the parameters for our experiment:     <pre><code>@dataclass\nclass Config:\n    width: int = 1024\n    height: int = 768\n</code></pre></p> </li> <li> <p>Main Function: The core of our application, decorated with <code>@hydraflow.main</code>:     <pre><code>@hydraflow.main(Config)\ndef app(run: Run, cfg: Config) -&gt; None:\n    logger.info(run.info.run_id)\n    logger.info(cfg)\n</code></pre></p> <p>This function is the entry point and receives two key parameters: <code>run</code> (an MLflow Run object) and <code>cfg</code> (the configuration object).</p> </li> <li> <p>Entry Point: The standard Python entry point that calls our application function:     <pre><code>if __name__ == \"__main__\":\n    app()\n</code></pre></p> </li> </ol>"},{"location":"practical-tutorials/applications/#the-power-of-the-decorator","title":"The Power of the Decorator","text":"<p>The <code>hydraflow.main</code> decorator is where the magic happens:</p> <ul> <li>It registers your configuration class with Hydra's <code>ConfigStore</code>.</li> <li>It sets the MLflow tracking URI via the <code>tracking_uri</code> if provided.</li> <li>It sets up an MLflow experiment.</li> <li>It starts an MLflow run and passes it to your function.</li> <li>It stores all Hydra configuration and logs as MLflow artifacts.</li> </ul>"},{"location":"practical-tutorials/applications/#running-the-application","title":"Running the Application","text":"<p>Now that we understand the code, let's run our application.</p>"},{"location":"practical-tutorials/applications/#single-run-mode","title":"Single-run Mode","text":"<p>First, let's run it in single-run mode:</p> <pre><code>$ python example.py\n2026/02/19 05:06:12 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n2026/02/19 05:06:12 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n2026/02/19 05:06:12 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n2026/02/19 05:06:12 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n2026/02/19 05:06:12 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n2026/02/19 05:06:12 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n2026/02/19 05:06:13 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n2026/02/19 05:06:13 INFO mlflow.store.db.utils: Updating database tables\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade  -&gt; 451aebb31d03, add metric step\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -&gt; bd07f7e963c5, create index on run_uuid\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -&gt; 0c779009ac13, add deleted_time field to runs table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -&gt; cc1f77228345, change param value length to 500\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -&gt; 97727af70f4d, Add creation_time and last_update_time to experiments table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -&gt; 3500859a5d39, Add Model Aliases table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -&gt; 7f2a7d5fae7d, add datasets inputs input_tags tables\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -&gt; 2d6e25af4d3e, increase max param val length from 500 to 8000\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -&gt; acf3f17fdcc7, add storage location field to model versions\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -&gt; 867495a8f9d4, add trace tables\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -&gt; 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -&gt; 4465047574b1, increase max dataset schema size\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -&gt; f5a4f2784254, increase run tag value limit to 8000\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -&gt; 0584bdc529eb, add cascading deletion to datasets from experiments\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -&gt; 400f98739977, add logged model tables\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 400f98739977 -&gt; 6953534de441, add step to inputs table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 6953534de441 -&gt; bda7b8c39065, increase_model_version_tag_value_limit\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -&gt; cbc13b556ace, add V3 trace schema columns\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -&gt; 770bee3ae1dd, add assessments table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -&gt; a1b2c3d4e5f6, add spans table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -&gt; de4033877273, create entity_associations table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade de4033877273 -&gt; 1a0cddfcaa16, Add webhooks and webhook_events tables\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -&gt; 534353b11cbc, add scorer tables\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -&gt; 71994744cf8e, add evaluation datasets\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -&gt; 3da73c924c2f, add outputs to dataset record\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -&gt; bf29a5ff90ea, add jobs table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea -&gt; 1bd49d398cd23, add secrets tables\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 1bd49d398cd23 -&gt; b7c8d9e0f1a2, add trace metrics table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade b7c8d9e0f1a2 -&gt; 5d2d30f0abce, update job table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 5d2d30f0abce -&gt; c9d4e5f6a7b8, add routing strategy to endpoints and linkage type to mappings\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade c9d4e5f6a7b8 -&gt; 2c33131f4dae, add online_scoring_configs table\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Running upgrade 2c33131f4dae -&gt; d3e4f5a6b7c8, add display_name to endpoint_bindings\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2026/02/19 05:06:13 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n2026/02/19 05:06:13 INFO mlflow.tracking.fluent: Experiment with name 'example' does not exist. Creating a new experiment.\n[2026-02-19 05:06:13,569][__main__][INFO] - 0edb9718d9134d9da62cb188208a3485\n[2026-02-19 05:06:13,570][__main__][INFO] - {'width': 1024, 'height': 768}\n</code></pre> <p>When you run the application, HydraFlow automatically:</p> <ol> <li>Sets the MLflow tracking URI to the <code>mlflow.db</code> SQLite database in the project root.</li> <li>Creates an MLflow experiment named after your application (in this case, \"example\").</li> <li>Starts a run with the provided configuration.</li> <li>Captures logs and artifacts.</li> </ol> <p>Let's use the MLflow CLI to verify that our experiment was created:</p> <pre><code>$ mlflow experiments search\n2026/02/19 05:06:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n2026/02/19 05:06:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n2026/02/19 05:06:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n2026/02/19 05:06:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n2026/02/19 05:06:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n2026/02/19 05:06:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n2026/02/19 05:06:16 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2026/02/19 05:06:16 INFO alembic.runtime.migration: Will assume non-transactional DDL.\nExperiment Id    Name     Artifact Location                                      \n---------------  -------  -------------------------------------------------------\n0                Default  /home/runner/work/hydraflow/hydraflow/examples/mlruns/0\n1                example  /home/runner/work/hydraflow/hydraflow/examples/mlruns/1\n</code></pre> <p>Now, let's examine the directory structure created by Hydra and MlFlow:</p> <pre><code>./\n\u251c\u2500\u2500 mlruns/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 0edb9718d9134d9da62cb188208a3485/\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 artifacts/\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 example.log\n\u251c\u2500\u2500 outputs/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 2026-02-19/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 05-06-12/\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 config.yaml\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 hydra.yaml\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 overrides.yaml\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 example.log\n\u251c\u2500\u2500 example.py\n\u251c\u2500\u2500 hydraflow.yaml\n\u251c\u2500\u2500 mlflow.db\n\u2514\u2500\u2500 submit.py\n</code></pre> <p>The directory structure shows:</p> <ul> <li><code>outputs</code> directory: Created by Hydra to store the run's outputs</li> <li><code>mlflow.db</code> file: Created by MLflow to store the experiment tracking database</li> <li><code>mlruns</code> directory: Created by MLflow to store experiment artifacts</li> <li><code>artifacts</code> directory: Contains configuration files and logs managed by HydraFlow</li> </ul>"},{"location":"practical-tutorials/applications/#multi-run-mode-parameter-sweeps","title":"Multi-run Mode (Parameter Sweeps)","text":"<p>One of Hydra's most powerful features is the ability to run parameter sweeps. Let's try this by overriding our configuration parameters:</p> <pre><code>$ python example.py -m width=400,600 height=100,200\n[2026-02-19 05:06:18,356][HYDRA] Launching 4 jobs locally\n[2026-02-19 05:06:18,356][HYDRA]    #0 : width=400 height=100\n2026/02/19 05:06:18 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n2026/02/19 05:06:18 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n2026/02/19 05:06:18 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n2026/02/19 05:06:18 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n2026/02/19 05:06:18 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n2026/02/19 05:06:18 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n2026/02/19 05:06:18 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2026/02/19 05:06:18 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n[2026-02-19 05:06:18,933][__main__][INFO] - 413d3f5a402f4b38ada7627ba36cf848\n[2026-02-19 05:06:18,934][__main__][INFO] - {'width': 400, 'height': 100}\n[2026-02-19 05:06:18,942][HYDRA]    #1 : width=400 height=200\n[2026-02-19 05:06:19,009][__main__][INFO] - 223aef23ecd64b3d9031fce8ff811e10\n[2026-02-19 05:06:19,009][__main__][INFO] - {'width': 400, 'height': 200}\n[2026-02-19 05:06:19,016][HYDRA]    #2 : width=600 height=100\n[2026-02-19 05:06:19,083][__main__][INFO] - 615d2d56f5984d7aab6355eb801a26e0\n[2026-02-19 05:06:19,083][__main__][INFO] - {'width': 600, 'height': 100}\n[2026-02-19 05:06:19,088][HYDRA]    #3 : width=600 height=200\n[2026-02-19 05:06:19,154][__main__][INFO] - 7c50be551ebc46f0ac7dce3dfb308aab\n[2026-02-19 05:06:19,155][__main__][INFO] - {'width': 600, 'height': 200}\n</code></pre> <p>The <code>-m</code> flag (or <code>--multirun</code>) tells Hydra to run all combinations of the specified parameters. In this case, we'll run 4 combinations:</p> <ul> <li>width=400, height=100</li> <li>width=400, height=200</li> <li>width=600, height=100</li> <li>width=600, height=200</li> </ul> <p>Let's see the updated directory structure:</p> <pre><code>./\n\u251c\u2500\u2500 mlruns/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 0edb9718d9134d9da62cb188208a3485/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 artifacts/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 example.log\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 223aef23ecd64b3d9031fce8ff811e10/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 artifacts/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 example.log\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 413d3f5a402f4b38ada7627ba36cf848/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 artifacts/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 example.log\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 615d2d56f5984d7aab6355eb801a26e0/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 artifacts/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 example.log\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 7c50be551ebc46f0ac7dce3dfb308aab/\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 artifacts/\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 example.log\n\u251c\u2500\u2500 multirun/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 2026-02-19/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 05-06-18/\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 0/\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 example.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 1/\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 example.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 2/\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 example.log\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 3/\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 example.log\n\u251c\u2500\u2500 outputs/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 2026-02-19/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 05-06-12/\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 .hydra/\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 example.log\n\u251c\u2500\u2500 example.py\n\u251c\u2500\u2500 mlflow.db\n\u2514\u2500\u2500 submit.py\n</code></pre> <p>Notice that all runs are added to the same MLflow experiment, making it easy to compare results across parameter combinations.</p>"},{"location":"practical-tutorials/applications/#cleanup","title":"Cleanup","text":"<p>With HydraFlow, all important data is stored in MLflow, so you can safely delete the Hydra output directories:</p> <pre><code>$ rm -rf outputs multirun\n</code></pre> <p>After cleanup, the directory structure is much simpler:</p> <pre><code>./\n\u251c\u2500\u2500 mlruns/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 0edb9718d9134d9da62cb188208a3485/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 223aef23ecd64b3d9031fce8ff811e10/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 413d3f5a402f4b38ada7627ba36cf848/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 615d2d56f5984d7aab6355eb801a26e0/\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 7c50be551ebc46f0ac7dce3dfb308aab/\n\u251c\u2500\u2500 example.py\n\u251c\u2500\u2500 hydraflow.yaml\n\u251c\u2500\u2500 mlflow.db\n\u2514\u2500\u2500 submit.py\n</code></pre> <p>All experiment data remains safely stored in the MLflow directory.</p>"},{"location":"practical-tutorials/applications/#summary","title":"Summary","text":"<p>In this tutorial, you've learned how to:</p> <ol> <li>Create a simple HydraFlow application using the <code>@hydraflow.main</code> decorator</li> <li>Define configuration using Python dataclasses</li> <li>Run experiments with default and overridden parameters</li> <li>Perform parameter sweeps using Hydra's multi-run capabilities</li> </ol> <p>This basic pattern forms the foundation for all HydraFlow applications. As your machine learning workflows grow in complexity, you can build upon this foundation to create more sophisticated experiments.</p>"},{"location":"practical-tutorials/applications/#next-steps","title":"Next Steps","text":"<p>Now that you've learned how to create and run a basic application, try:</p> <ul> <li>Creating more complex configurations with nested parameters</li> <li>Adding actual machine learning code to your application</li> <li>Exploring Automated Workflows with HydraFlow</li> <li>Learning how to Analyze Results from your experiments</li> </ul> <p>For detailed documentation, refer to:</p> <ul> <li>Part 1: Running Applications</li> <li>Part 2: Automating Workflows</li> <li>Part 3: Analyzing Results</li> </ul>"},{"location":"api/hydraflow/","title":"hydraflow","text":"hydraflow<p> source package hydraflow </p> <p>Integrate Hydra and MLflow to manage and track machine learning experiments.</p> <p> Classes </p> <ul> <li> <p>Collection \u2014 A collection of items that implements the Sequence protocol.</p> </li> <li> <p>Run \u2014 Represent an MLflow Run in HydraFlow.</p> </li> <li> <p>RunCollection \u2014 A collection of Run instances that implements the Sequence protocol.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>chdir_artifact \u2014 Change the current working directory to the artifact directory of the given run.</p> </li> <li> <p>get_artifact_dir \u2014 Retrieve the artifact directory for the given run.</p> </li> <li> <p>get_experiment_names \u2014 Get the experiment names.</p> </li> <li> <p>iter_artifact_paths \u2014 Iterate over the artifact paths in the tracking directory.</p> </li> <li> <p>iter_artifacts_dirs \u2014 Iterate over the artifacts directories in the tracking directory.</p> </li> <li> <p>iter_experiment_dirs \u2014 Iterate over the experiment directories.</p> </li> <li> <p>iter_run_dirs \u2014 Iterate over the run directories in the tracking directory.</p> </li> <li> <p>log_run \u2014 Log the parameters from the given configuration instance.</p> </li> <li> <p>main \u2014 Decorate a function for configuring and running MLflow experiments with Hydra.</p> </li> <li> <p>start_run \u2014 Start an MLflow run and log parameters using the provided configuration instance.</p> </li> <li> <p>apply_hydra_argparse_patch \u2014 Apply Python 3.14 compatibility patch for Hydra's argparse usage.</p> </li> </ul> <p> source class Collection[I](items: Iterable[I], get: Callable[[I, str, Any | Callable[[I], Any]], Any] | None = None) </p> <p>Bases : Sequence[I]</p> <p>A collection of items that implements the Sequence protocol.</p> <p> Methods </p> <ul> <li> <p>filter \u2014 Filter items based on criteria.</p> </li> <li> <p>exclude \u2014 Exclude items based on criteria.</p> </li> <li> <p>try_get \u2014 Try to get a single item matching the specified criteria.</p> </li> <li> <p>get \u2014 Get a single item matching the specified criteria.</p> </li> <li> <p>first \u2014 Get the first item matching the specified criteria.</p> </li> <li> <p>last \u2014 Get the last item matching the specified criteria.</p> </li> <li> <p>to_list \u2014 Extract a list of values for a specific key from all items.</p> </li> <li> <p>to_numpy \u2014 Extract values for a specific key from all items as a NumPy array.</p> </li> <li> <p>to_series \u2014 Extract values for a specific key from all items as a Polars series.</p> </li> <li> <p>unique \u2014 Get the unique values for a specific key across all items.</p> </li> <li> <p>n_unique \u2014 Count the number of unique values for a specific key across all items.</p> </li> <li> <p>sort \u2014 Sort items based on one or more keys.</p> </li> <li> <p>map \u2014 Apply a function to each item and return an iterator of results.</p> </li> <li> <p>to_frame \u2014 Convert the collection to a Polars DataFrame.</p> </li> <li> <p>group_by \u2014 Group items by one or more keys and return a GroupBy instance.</p> </li> <li> <p>sample \u2014 Sample a random subset of items from the collection.</p> </li> <li> <p>shuffle \u2014 Shuffle the items in the collection.</p> </li> <li> <p>eq \u2014 Create a predicate function that checks if two attributes are equal.</p> </li> <li> <p>ne \u2014 Create a predicate function that checks if two attributes are not equal.</p> </li> <li> <p>gt \u2014 Create a predicate function that checks if the left &gt; the right.</p> </li> <li> <p>lt \u2014 Create a predicate function that checks if the left &lt; the right.</p> </li> <li> <p>ge \u2014 Create a predicate function that checks if the left &gt;= the right.</p> </li> <li> <p>le \u2014 Create a predicate function that checks if the left &lt;= the right.</p> </li> <li> <p>startswith \u2014 Create a predicate function that checks if an attribute starts with a prefix.</p> </li> <li> <p>endswith \u2014 Create a predicate function that checks if an attribute ends with a suffix.</p> </li> <li> <p>match \u2014 Create a predicate function that checks if an attribute matches a pattern.</p> </li> </ul> <p> source method Collection.filter(*criteria: Callable[[I], bool] | Iterable[Any] | tuple[str, Any], **kwargs: Any) \u2192 Self </p> <p>Filter items based on criteria.</p> <p>This method allows filtering items using various criteria:</p> <ul> <li>Callable criteria that take an item and return a boolean</li> <li>Key-value tuples where the key is a string and the value   is compared using the <code>matches</code> function</li> <li>Keyword arguments, where the key is a string and the value   is compared using the <code>matches</code> function</li> </ul> <p>The <code>matches</code> function supports the following comparison types:</p> <ul> <li>Callable: The predicate function is called with the value</li> <li>List/Set: Checks if the value is in the list/set</li> <li>Tuple of length 2: Checks if the value is in the range [min, max]</li> <li>Other: Checks for direct equality</li> </ul> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | Iterable[Any] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new Collection containing only the items that match all criteria.</p> </li> </ul> <p> Examples </p> <pre><code># Filter using a callable\nfiltered = collection.filter(lambda x: x &gt; 5)\n\n# Filter using a key-value tuple\nfiltered = collection.filter((\"age\", 25))\n\n# Filter using an iterable\nfiltered = collection.filter([True, False, True])\n\n# Filter using keyword arguments\nfiltered = collection.filter(age=25, name=\"John\")\n\n# Filter using range\nfiltered = collection.filter((\"age\", (20, 30)))\n\n# Filter using list membership\nfiltered = collection.filter((\"name\", [\"John\", \"Jane\"]))\n</code></pre> <p> source method Collection.exclude(*criteria: Callable[[I], bool] | Iterable[Any] | tuple[str, Any], **kwargs: Any) \u2192 Self </p> <p>Exclude items based on criteria.</p> <p>This method works the opposite of <code>filter</code>. It excludes items that match any of the specified criteria.</p> <p>This method allows excluding items using various criteria:</p> <ul> <li>Callable criteria that take an item and return a boolean</li> <li>Key-value tuples where the key is a string and the value   is compared using the <code>matches</code> function</li> <li>Keyword arguments, where the key is a string and the value   is compared using the <code>matches</code> function</li> </ul> <p>The <code>matches</code> function supports the following comparison types:</p> <ul> <li>Callable: The predicate function is called with the value</li> <li>List/Set: Checks if the value is in the list/set</li> <li>Tuple of length 2: Checks if the value is in the range [min, max]</li> <li>Other: Checks for direct equality</li> </ul> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | Iterable[Any] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for exclusion.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for exclusion.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new Collection containing only the items that do not match any of the criteria.</p> </li> </ul> <p> Examples </p> <pre><code># Exclude using a callable\nexcluded = collection.exclude(lambda x: x &gt; 5)\n\n# Exclude using a key-value tuple\nexcluded = collection.exclude((\"age\", 25))\n\n# Exclude using an iterable mask\nexcluded = collection.exclude([True, False, True])\n\n# Exclude using keyword arguments\nexcluded = collection.exclude(age=25, name=\"John\")\n</code></pre> <p> source method Collection.try_get(*criteria: Callable[[I], bool] | tuple[str, Any], **kwargs: Any) \u2192 I | None </p> <p>Try to get a single item matching the specified criteria.</p> <p>This method applies filters and returns a single matching item if exactly one is found, None if no items are found, or raises ValueError if multiple items match.</p> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>I | None \u2014 A single item that matches the criteria, or None if no matches are found.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If multiple items match the criteria.</p> </li> </ul> <p> source method Collection.get(*criteria: Callable[[I], bool] | tuple[str, Any], **kwargs: Any) \u2192 I </p> <p>Get a single item matching the specified criteria.</p> <p>This method applies filters and returns a single matching item, or raises ValueError if no items or multiple items match.</p> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>I \u2014 A single item that matches the criteria.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If no items match or if multiple items match</p> </li> <li> <p>the criteria.</p> </li> <li> <p>_value_error</p> </li> </ul> <p> source method Collection.first(*criteria: Callable[[I], bool] | tuple[str, Any], **kwargs: Any) \u2192 I </p> <p>Get the first item matching the specified criteria.</p> <p>This method applies filters and returns the first matching item, or raises ValueError if no items match.</p> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>I \u2014 The first item that matches the criteria.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If no items match the criteria.</p> </li> <li> <p>_value_error</p> </li> </ul> <p> source method Collection.last(*criteria: Callable[[I], bool] | tuple[str, Any], **kwargs: Any) \u2192 I </p> <p>Get the last item matching the specified criteria.</p> <p>This method applies filters and returns the last matching item, or raises ValueError if no items match.</p> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>I \u2014 The last item that matches the criteria.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If no items match the criteria.</p> </li> <li> <p>_value_error</p> </li> </ul> <p> source method Collection.to_list(key: str, default: Any | Callable[[I], Any] = MISSING) \u2192 list[Any] </p> <p>Extract a list of values for a specific key from all items.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to extract from each item.</p> </li> <li> <p>default :  Any | Callable[[I], Any] \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[Any] \u2014 A list containing the values for the specified key from each item.</p> </li> </ul> <p> source method Collection.to_numpy(key: str, default: Any | Callable[[I], Any] = MISSING) \u2192 NDArray[Any] </p> <p>Extract values for a specific key from all items as a NumPy array.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to extract from each item.</p> </li> <li> <p>default :  Any | Callable[[I], Any] \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>NDArray \u2014 A NumPy array containing the values for the specified key from each item.</p> </li> </ul> <p> source method Collection.to_series(key: str, default: Any = MISSING, *, name: str | None = None) \u2192 Series </p> <p>Extract values for a specific key from all items as a Polars series.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to extract from each item.</p> </li> <li> <p>default :  Any \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> <li> <p>name :  str | None \u2014 The name of the series. If not provided, the key will be used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Series \u2014 A Polars series containing the values for the specified key from each item.</p> </li> </ul> <p> source method Collection.unique(key: str, default: Any | Callable[[I], Any] = MISSING) \u2192 NDArray[Any] </p> <p>Get the unique values for a specific key across all items.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to extract unique values for.</p> </li> <li> <p>default :  Any | Callable[[I], Any] \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>NDArray \u2014 A NumPy array containing the unique values for the specified key.</p> </li> </ul> <p> source method Collection.n_unique(key: str, default: Any | Callable[[I], Any] = MISSING) \u2192 int </p> <p>Count the number of unique values for a specific key across all items.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to count unique values for.</p> </li> <li> <p>default :  Any | Callable[[I], Any] \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The number of unique values for the specified key.</p> </li> </ul> <p> source method Collection.sort(*keys: str, reverse: bool = False) \u2192 Self </p> <p>Sort items based on one or more keys.</p> <p> Parameters </p> <ul> <li> <p>*keys :  str \u2014 The keys to sort by, in order of priority.</p> </li> <li> <p>reverse :  bool \u2014 Whether to sort in descending order (default is ascending).</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new Collection with the items sorted according to the specified keys.</p> </li> </ul> <p> source method Collection.map[**P, R](function: Callable[Concatenate[I, P], R], *args: P.args, **kwargs: P.kwargs) \u2192 Iterator[R] </p> <p>Apply a function to each item and return an iterator of results.</p> <p>This is a memory-efficient mapping operation that lazily evaluates results. Ideal for large collections where memory usage is a concern.</p> <p> Parameters </p> <ul> <li> <p>function :  Callable[Concatenate[I, P], R] \u2014 Function to apply to each item. The item is passed as the first argument.</p> </li> <li> <p>*args :  P.args \u2014 Additional positional arguments to pass to the function.</p> </li> <li> <p>**kwargs :  P.kwargs \u2014 Additional keyword arguments to pass to the function.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>R \u2014 The function's results.</p> </li> </ul> <p> Examples </p> <pre><code># Process results one at a time\nfor result in collection.map(process_item, additional_arg):\n    handle_result(result)\n\n# Convert to list if needed\nresults = list(collection.map(transform_item))\n</code></pre> <p> source method Collection.to_frame(*keys: str | tuple[str, Any | Callable[[I], Any]], defaults: dict[str, Any | Callable[[I], Any]] | None = None, **kwargs: Callable[[I], Any]) \u2192 DataFrame </p> <p>Convert the collection to a Polars DataFrame.</p> <p>This method converts the items in the collection into a Polars DataFrame. It allows specifying multiple keys, where each key can be a string or a tuple. If a tuple is provided, the first element is treated as the key and the second element as the default value for that key.</p> <p> Parameters </p> <ul> <li> <p>*keys :  str | tuple[str, Any | Callable[[I], Any]] \u2014 The keys to include as columns in the DataFrame. If a tuple is provided, the first element is the key and the second element is the default value.</p> </li> <li> <p>defaults :  dict[str, Any | Callable[[I], Any]] | None \u2014 Default values for the keys. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> <li> <p>**kwargs :  Callable[[I], Any] \u2014 Additional columns to compute using callables that take an item and return a value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A Polars DataFrame containing the specified data from the items.</p> </li> </ul> <p> Examples </p> <pre><code># Convert to DataFrame with single keys\ndf = collection.to_frame(\"name\", \"age\")\n\n# Convert to DataFrame with keys and default values\ndf = collection.to_frame((\"name\", \"Unknown\"), (\"age\", 0))\n</code></pre> <p> source method Collection.group_by(*by: str) \u2192 GroupBy[Self, I] </p> <p>Group items by one or more keys and return a GroupBy instance.</p> <p>This method organizes items into groups based on the specified keys and returns a GroupBy instance that contains the grouped collections. The GroupBy instance behaves like a dictionary, allowing access to collections for each group key.</p> <p> Parameters </p> <ul> <li> <p>*by :  str \u2014 The keys to group by. If a single key is provided, its value will be used as the group key. If multiple keys are provided, a tuple of their values will be used as the group key. Keys can use dot notation (e.g., \"model.type\") to access nested configuration values.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GroupBy[Self, I] \u2014 A GroupBy instance containing the grouped items. Each group is a collection of the same type as the original.</p> </li> </ul> <p> source method Collection.sample(k: int, seed: int | None = None) \u2192 Self </p> <p>Sample a random subset of items from the collection.</p> <p>This method returns a new collection containing a random sample of items from the original collection. The sample is drawn without replacement, meaning each item can only appear once in the sample.</p> <p> Parameters </p> <ul> <li> <p>k :  int \u2014 The number of items to sample.</p> </li> <li> <p>seed :  int | None \u2014 The seed for the random number generator. If provided, the sample will be reproducible.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new collection containing a random sample of items.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If the sample size is greater than the collection size.</p> </li> </ul> <p> source method Collection.shuffle(seed: int | None = None) \u2192 Self </p> <p>Shuffle the items in the collection.</p> <p>This method returns a new collection with the items in random order.</p> <p> Parameters </p> <ul> <li> <p>seed :  int | None \u2014 The seed for the random number generator. If provided, the sample will be reproducible.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new collection containing the items in random order.</p> </li> </ul> <p> source method Collection.eq(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if two attributes are equal.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value to use if either attribute is not found. If callable, it will be called with the item.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A function that takes an item and returns True if the values of the specified attributes are equal.</p> </li> </ul> <p> Examples </p> <pre><code># Find items where attribute 'a' equals attribute 'b'\nequal_items = collection.filter(collection.eq('a', 'b'))\n</code></pre> <p> source method Collection.ne(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if two attributes are not equal.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value to use if either attribute is not found. If callable, it will be called with the item.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A function that takes an item and returns True if the values of the specified attributes are not equal.</p> </li> </ul> <p> Examples </p> <pre><code># Find items where attribute 'a' is not equal to attribute 'b'\nunequal_items = collection.filter(collection.ne('a', 'b'))\n</code></pre> <p> source method Collection.gt(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if the left &gt; the right.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value to use if either attribute is not found. If callable, it will be called with the item.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A function that takes an item and returns True if the left attribute value is greater than the right attribute value.</p> </li> </ul> <p> Examples </p> <pre><code># Find items where attribute 'a' is greater than attribute 'b'\nitems = collection.filter(collection.gt('a', 'b'))\n</code></pre> <p> source method Collection.lt(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if the left &lt; the right.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value to use if either attribute is not found. If callable, it will be called with the item.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A function that takes an item and returns True if the left attribute value is less than the right attribute value.</p> </li> </ul> <p> Examples </p> <pre><code># Find items where attribute 'a' is less than attribute 'b'\nitems = collection.filter(collection.lt('a', 'b'))\n</code></pre> <p> source method Collection.ge(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if the left &gt;= the right.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source method Collection.le(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if the left &lt;= the right.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source method Collection.startswith(key: str, prefix: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if an attribute starts with a prefix.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The name of the attribute to check.</p> </li> <li> <p>prefix :  str \u2014 The prefix to check for.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source method Collection.endswith(key: str, suffix: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if an attribute ends with a suffix.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The name of the attribute to check.</p> </li> <li> <p>suffix :  str \u2014 The suffix to check for.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source method Collection.match(key: str, pattern: str | Pattern[str], *, default: Any | Callable[[I], Any] = MISSING, flags: _FlagsType = 0) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if an attribute matches a pattern.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The name of the attribute to check.</p> </li> <li> <p>pattern :  str | re.Pattern \u2014 The pattern to check for.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> <li> <p>flags :  re.RegexFlag, optional \u2014 Flags for the regex pattern.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source class Run[C, I = None](run_dir: Path, impl_factory: Callable[[Path], I] | Callable[[Path, C], I] | None = None) </p> <p>Represent an MLflow Run in HydraFlow.</p> <p>A Run contains information about the run, configuration, and implementation. The configuration type C and implementation type I are specified as type parameters.</p> <p> Attributes </p> <ul> <li> <p>info :  RunInfo \u2014 Information about the run, such as run directory, run ID, and job name.</p> </li> <li> <p>impl_factory :  Callable[[Path], I] | Callable[[Path, C], I] \u2014 Factory function to create the implementation instance.</p> </li> <li> <p>cfg :  C \u2014 The configuration instance loaded from the Hydra configuration file.</p> </li> <li> <p>impl :  I \u2014 The implementation instance created by the factory function.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>load \u2014 Load a Run from a run directory.</p> </li> <li> <p>update \u2014 Set default value(s) in the configuration if they don't already exist.</p> </li> <li> <p>get \u2014 Get a value from the information or configuration.</p> </li> <li> <p>lit \u2014 Create a Polars literal expression from a run key.</p> </li> <li> <p>to_frame \u2014 Convert the Run to a DataFrame.</p> </li> <li> <p>to_frame_impl \u2014 Convert the Run to a DataFrame.</p> </li> <li> <p>to_dict \u2014 Convert the Run to a dictionary.</p> </li> <li> <p>chdir \u2014 Change the current working directory to the artifact directory.</p> </li> <li> <p>path \u2014 Return the path relative to the artifact directory.</p> </li> <li> <p>iterdir \u2014 Iterate over the artifact directories for the run.</p> </li> <li> <p>glob \u2014 Glob the artifact directories for the run.</p> </li> </ul> <p> source property Run.cfg: C </p> <p>The configuration instance loaded from the Hydra configuration file.</p> <p> source property Run.impl: I </p> <p>The implementation instance created by the factory function.</p> <p>This property dynamically examines the signature of the impl_factory using the inspect module and calls it with the appropriate arguments:</p> <ul> <li>If the factory accepts one parameter: called with just the artifacts   directory</li> <li>If the factory accepts two parameters: called with the artifacts   directory and the configuration instance</li> </ul> <p>This allows implementation classes to be configuration-aware and utilize both the file system and configuration information.</p> <p> source classmethod Run.load(run_dir: str | Path | Iterable[str | Path], impl_factory: Callable[[Path], I] | Callable[[Path, C], I] | None = None) \u2192 Self | RunCollection[Self] </p> <p>Load a Run from a run directory.</p> <p> Parameters </p> <ul> <li> <p>run_dir :  str | Path | Iterable[str | Path] \u2014 The directory where the MLflow runs are stored, either as a string, a Path instance, or an iterable of them.</p> </li> <li> <p>impl_factory :  Callable[[Path], I] | Callable[[Path, C], I] | None \u2014 A factory function that creates the implementation instance. It can accept either just the artifacts directory path, or both the path and the configuration instance. Defaults to None, in which case a function that returns None is used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self | RunCollection[Self] \u2014 A single Run instance or a RunCollection of Run instances.</p> </li> </ul> <p> source method Run.update(key: str | tuple[str, ...], value: Any | Callable[[Self], Any], *, force: bool = False) \u2192 None </p> <p>Set default value(s) in the configuration if they don't already exist.</p> <p>This method adds a value or multiple values to the configuration, but only if the corresponding keys don't already have values. Existing values will not be modified.</p> <p> Parameters </p> <ul> <li> <p>key :  str | tuple[str, ...] \u2014 Either a string representing a single configuration path (can use dot notation like \"section.subsection.param\"), or a tuple of strings to set multiple related configuration values at once.</p> </li> <li> <p>value :  Any | Callable[[Self], Any] \u2014 The value to set. This can be:</p> <ul> <li>For string keys: Any value, or a callable that returns   a value</li> <li>For tuple keys: An iterable with the same length as the   key tuple, or a callable that returns such an iterable</li> <li>For callable values: The callable must accept a single argument   of type Run (self) and return the appropriate value type</li> </ul> </li> <li> <p>force :  bool \u2014 Whether to force the update even if the key already exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>TypeError \u2014 If a tuple key is provided but the value is not an iterable, or if the callable doesn't return an iterable.</p> </li> </ul> <p> source method Run.get(key: str, default: Any | Callable[[Self], Any] = MISSING) \u2192 Any </p> <p>Get a value from the information or configuration.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to look for. Can use dot notation for nested keys in configuration. Special keys:</p> <ul> <li>\"cfg\": Returns the configuration object</li> <li>\"impl\": Returns the implementation object</li> <li>\"info\": Returns the run information object</li> </ul> </li> <li> <p>default :  Any | Callable[[Self], Any] \u2014 Value to return if the key is not found. If a callable, it will be called with the Run instance and the value returned will be used as the default. If not provided, AttributeError will be raised.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Any \u2014 The value associated with the key, or the default value if the key is not found and a default is provided.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>AttributeError \u2014 If the key is not found and no default is provided.</p> </li> </ul> <p>Note</p> <p>The search order for keys is:</p> <ol> <li>Configuration (<code>cfg</code>)</li> <li>Implementation (<code>impl</code>)</li> <li>Run information (<code>info</code>)</li> <li>Run object itself (<code>self</code>)</li> </ol> <p> source method Run.lit(key: str, default: Any | Callable[[Self], Any] = MISSING, *, dtype: PolarsDataType | None = None) \u2192 Expr </p> <p>Create a Polars literal expression from a run key.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to look up in the run's configuration or info.</p> </li> <li> <p>default :  Any | Callable[[Run], Any], optional \u2014 Default value to use if the key is missing. If a callable is provided, it will be called with the Run instance.</p> </li> <li> <p>dtype :  PolarsDataType | None \u2014 Explicit data type for the literal expression.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Expr \u2014 A Polars literal expression aliased to the provided key.</p> </li> </ul> <p> source method Run.to_frame(function: Callable[[Self], DataFrame], *keys: str | tuple[str, Any | Callable[[Self], Any]]) \u2192 DataFrame </p> <p>Convert the Run to a DataFrame.</p> <p> Parameters </p> <ul> <li> <p>function :  Callable[[Run], DataFrame] \u2014 A function that takes a Run instance and returns a DataFrame.</p> </li> <li> <p>keys :  str | tuple[str, Any | Callable[[Run], Any]] \u2014 The keys to add to the DataFrame.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A DataFrame representation of the Run.</p> </li> </ul> <p> source method Run.to_frame_impl(function: Callable[[I], DataFrame], *keys: str | tuple[str, Any | Callable[[Self], Any]]) \u2192 DataFrame </p> <p>Convert the Run to a DataFrame.</p> <p> Parameters </p> <ul> <li> <p>function :  Callable[[I], DataFrame] \u2014 A function that takes a Run.impl instance and returns a DataFrame.</p> </li> <li> <p>keys :  str | tuple[str, Any | Callable[[Run], Any]] \u2014 The keys to add to the DataFrame.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A DataFrame representation of the Run.</p> </li> </ul> <p> source method Run.to_dict(*, flatten: bool = True) \u2192 dict[str, Any] </p> <p>Convert the Run to a dictionary.</p> <p> Parameters </p> <ul> <li> <p>flatten :  bool, optional \u2014 If True, flattens nested dictionaries. Defaults to True.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, Any] \u2014 A dictionary representation of the Run's configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>TypeError \u2014 If the configuration is not a dictionary.</p> </li> </ul> <p> source method Run.chdir(relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Change the current working directory to the artifact directory.</p> <p>This context manager changes the current working directory to the artifact directory of the run. It ensures that the directory is changed back to the original directory after the context is exited.</p> <p> Parameters </p> <ul> <li> <p>relative_dir :  str \u2014 The relative directory to the artifact directory. Defaults to an empty string.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 The artifact directory of the run.</p> </li> </ul> <p> source method Run.path(relative_path: str = '') \u2192 Path </p> <p>Return the path relative to the artifact directory.</p> <p> Parameters </p> <ul> <li> <p>relative_path :  str \u2014 The relative path to the artifact directory.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Path \u2014 The path relative to the artifact directory.</p> </li> </ul> <p> source method Run.iterdir(relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Iterate over the artifact directories for the run.</p> <p> Parameters </p> <ul> <li> <p>relative_dir :  str \u2014 The relative directory to iterate over.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 The artifact directory for the run.</p> </li> </ul> <p> source method Run.glob(pattern: str, relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Glob the artifact directories for the run.</p> <p> Parameters </p> <ul> <li> <p>pattern :  str \u2014 The pattern to glob.</p> </li> <li> <p>relative_dir :  str \u2014 The relative directory to glob.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 The existing artifact paths that match the pattern.</p> </li> </ul> <p> source class RunCollection[R: Run[Any, Any]](items: Iterable[I], get: Callable[[I, str, Any | Callable[[I], Any]], Any] | None = None) </p> <p>Bases : Collection[R]</p> <p>A collection of Run instances that implements the Sequence protocol.</p> <p>RunCollection provides methods for filtering, sorting, grouping, and analyzing runs, as well as converting run data to various formats such as DataFrames.</p> <p> Parameters </p> <ul> <li> <p>runs :  Iterable[Run] \u2014 An iterable of Run instances to include in the collection.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>update \u2014 Update configuration values for all runs in the collection.</p> </li> <li> <p>concat \u2014 Concatenate the results of a function applied to all runs in the collection.</p> </li> <li> <p>iterdir \u2014 Iterate over the artifact directories for all runs in the collection.</p> </li> <li> <p>glob \u2014 Glob the artifact directories for all runs in the collection.</p> </li> </ul> <p> source method RunCollection.update(key: str | tuple[str, ...], value: Any | Callable[[R], Any], *, force: bool = False) \u2192 None </p> <p>Update configuration values for all runs in the collection.</p> <p>This method calls the update method on each run in the collection.</p> <p> Parameters </p> <ul> <li> <p>key :  str | tuple[str, ...] \u2014 Either a string representing a single configuration path or a tuple of strings to set multiple configuration values.</p> </li> <li> <p>value :  Any | Callable[[R], Any] \u2014 The value(s) to set or a callable that returns such values.</p> </li> <li> <p>force :  bool \u2014 Whether to force updates even if the keys already exist.</p> </li> </ul> <p> source method RunCollection.concat(function: Callable[[R], DataFrame], *keys: str | tuple[str, Any | Callable[[R], Any]]) \u2192 DataFrame </p> <p>Concatenate the results of a function applied to all runs in the collection.</p> <p>This method applies the provided function to each run in the collection and concatenates the resulting DataFrames along the specified keys.</p> <p> Parameters </p> <ul> <li> <p>function :  Callable[[R], DataFrame] \u2014 A function that takes a Run instance and returns a DataFrame.</p> </li> <li> <p>keys :  str | tuple[str, Any | Callable[[R], Any]] \u2014 The keys to add to the DataFrame.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A DataFrame representation of the Run collection.</p> </li> </ul> <p> source method RunCollection.iterdir(relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Iterate over the artifact directories for all runs in the collection.</p> <p>This method yields all files and directories in the specified relative directory for each run in the collection.</p> <p> Parameters </p> <ul> <li> <p>relative_dir :  str \u2014 The relative directory within the artifacts directory to iterate over.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 Each path in the specified directory for each run in the collection.</p> </li> </ul> <p> source method RunCollection.glob(pattern: str, relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Glob the artifact directories for all runs in the collection.</p> <p>This method yields all paths matching the specified pattern in the relative directory for each run in the collection.</p> <p> Parameters </p> <ul> <li> <p>pattern :  str \u2014 The glob pattern to match files or directories.</p> </li> <li> <p>relative_dir :  str \u2014 The relative directory within the artifacts directory to search in.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 Each path matching the pattern for each run in the collection.</p> </li> </ul> <p> source chdir_artifact(run: Run) \u2192 Iterator[Path] </p> <p>Change the current working directory to the artifact directory of the given run.</p> <p>This context manager changes the current working directory to the artifact directory of the given run. It ensures that the directory is changed back to the original directory after the context is exited.</p> <p> Parameters </p> <ul> <li> <p>run :  Run | None \u2014 The run to get the artifact directory from.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>str \u2014 The artifact directory</p> </li> </ul> <p> source get_artifact_dir(run: Run) \u2192 Path </p> <p>Retrieve the artifact directory for the given run.</p> <p>This function uses MLflow to get the artifact directory for the given run.</p> <p> Parameters </p> <ul> <li> <p>run :  Run | None \u2014 The run instance. Defaults to None.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Path \u2014 The local path to the directory where the artifacts are downloaded.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source get_experiment_names() \u2192 list[str] </p> <p>Get the experiment names.</p> <p> Returns </p> <ul> <li> <p>list[str] \u2014 A list of experiment names sorted by the name.</p> </li> </ul> <p> source iter_artifact_paths(artifact_path: str | Path, experiment_names: str | list[str] | Callable[[str], bool] | None = None) \u2192 Iterator[Path] </p> <p>Iterate over the artifact paths in the tracking directory.</p> <p> source iter_artifacts_dirs(experiment_names: str | list[str] | Callable[[str], bool] | None = None) \u2192 Iterator[Path] </p> <p>Iterate over the artifacts directories in the tracking directory.</p> <p> source iter_experiment_dirs(experiment_names: str | list[str] | Callable[[str], bool] | None = None) \u2192 Iterator[Path] </p> <p>Iterate over the experiment directories.</p> <p> source iter_run_dirs(experiment_names: str | list[str] | Callable[[str], bool] | None = None) \u2192 Iterator[Path] </p> <p>Iterate over the run directories in the tracking directory.</p> <p> source log_run(run: Run) \u2192 Iterator[None] </p> <p>Log the parameters from the given configuration instance.</p> <p>This context manager logs the parameters from the provided configuration instance using MLflow. It also manages the MLflow run context, ensuring that artifacts are logged and the run is properly closed.</p> <p> Parameters </p> <ul> <li> <p>run :  Run \u2014 The run instance.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>None \u2014 None</p> </li> </ul> <p> source main[C](node: C | type[C], config_name: str = 'config', *, tracking_uri: str | Path | None = None, chdir: bool = False, force_new_run: bool = False, match_overrides: bool = False, rerun_finished: bool = False, dry_run: bool = False, update: Callable[[C], C | None] | None = None) \u2192 Callable[..., Callable[[], None]] </p> <p>Decorate a function for configuring and running MLflow experiments with Hydra.</p> <p>This decorator combines Hydra configuration management with MLflow experiment tracking. It automatically handles run deduplication and configuration storage.</p> <p> Parameters </p> <ul> <li> <p>node :  C | type[C] \u2014 Configuration node class or instance defining the structure of the configuration.</p> </li> <li> <p>config_name :  str \u2014 Name of the configuration. Defaults to \"config\".</p> </li> <li> <p>tracking_uri :  str | Path | None \u2014 The tracking URI for MLflow. If not provided, MLflow's default tracking URI is used. Defaults to None.</p> </li> <li> <p>chdir :  bool \u2014 If True, changes working directory to the artifact directory of the run. Defaults to False.</p> </li> <li> <p>force_new_run :  bool \u2014 If True, always creates a new MLflow run instead of reusing existing ones. Defaults to False.</p> </li> <li> <p>match_overrides :  bool \u2014 If True, matches runs based on Hydra CLI overrides instead of full config. Defaults to False.</p> </li> <li> <p>rerun_finished :  bool \u2014 If True, allows rerunning completed runs. Defaults to False.</p> </li> <li> <p>dry_run :  bool \u2014 If True, starts the hydra job but does not run the application itself. This allows users to preview the configuration and settings without executing the actual run. Defaults to False.</p> </li> <li> <p>update :  Callable[[C], C | None] | None \u2014 A function that takes a configuration and returns a new configuration or None. The function can modify the configuration in-place and/or return it. If the function returns None, the original (potentially modified) configuration is used. Changes made by this function are saved to the configuration file. This is useful for adding derived parameters, ensuring consistency between related values, or adding runtime information to the configuration. Defaults to None.</p> </li> </ul> <p> source start_run(*, chdir: bool = False, run_id: str | None = None, experiment_id: str | None = None, run_name: str | None = None, nested: bool = False, parent_run_id: str | None = None, tags: dict[str, str] | None = None, description: str | None = None, log_system_metrics: bool | None = None) \u2192 Iterator[Run] </p> <p>Start an MLflow run and log parameters using the provided configuration instance.</p> <p>This context manager starts an MLflow run and logs parameters using the specified configuration instance. It ensures that the run is properly closed after completion.</p> <p> Parameters </p> <ul> <li> <p>chdir :  bool \u2014 Whether to change the current working directory to the artifact directory of the current run. Defaults to False.</p> </li> <li> <p>run_id :  str | None \u2014 The existing run ID. Defaults to None.</p> </li> <li> <p>experiment_id :  str | None \u2014 The experiment ID. Defaults to None.</p> </li> <li> <p>run_name :  str | None \u2014 The name of the run. Defaults to None.</p> </li> <li> <p>nested :  bool \u2014 Whether to allow nested runs. Defaults to False.</p> </li> <li> <p>parent_run_id :  str | None \u2014 The parent run ID. Defaults to None.</p> </li> <li> <p>tags :  dict[str, str] | None \u2014 Tags to associate with the run. Defaults to None.</p> </li> <li> <p>description :  str | None \u2014 A description of the run. Defaults to None.</p> </li> <li> <p>log_system_metrics :  bool | None \u2014 Whether to log system metrics. Defaults to None.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Run \u2014 An MLflow Run instance representing the started run.</p> </li> </ul> <p> apply_hydra_argparse_patch() \u2192 None </p> <p>Apply Python 3.14 compatibility patch for Hydra's argparse usage.</p> <p>Python 3.14 added validation in argparse._check_help that checks if '%' is in the help string. Hydra's LazyCompletionHelp object doesn't support the 'in' operator, causing a TypeError during argument parser creation.</p> <p>This patch temporarily disables the validation during Hydra's parser creation, then immediately restores it. This is safe because: 1. The validation is purely for catching developer errors early 2. It's new in Python 3.14 (didn't exist in earlier versions) 3. Hydra's help strings work correctly when actually displayed 4. We restore normal behavior immediately after</p>"},{"location":"api/hydraflow/core/","title":"hydraflow.core","text":"hydraflow.core<p> source package hydraflow.core </p> <p> Modules </p> <ul> <li> <p>hydraflow.core.collection \u2014 Provide a collection of items that implements the Sequence protocol.</p> </li> <li> <p>hydraflow.core.context \u2014 Provide context managers to log parameters and manage the MLflow run context.</p> </li> <li> <p>hydraflow.core.group_by \u2014 GroupBy module for organizing and aggregating collections of items.</p> </li> <li> <p>hydraflow.core.io \u2014 Provide utility functions for HydraFlow.</p> </li> <li> <p>hydraflow.core.main \u2014 Integration of MLflow experiment tracking with Hydra configuration management.</p> </li> <li> <p>hydraflow.core.run \u2014 Run module for HydraFlow.</p> </li> <li> <p>hydraflow.core.run_collection \u2014 RunCollection module for HydraFlow.</p> </li> <li> <p>hydraflow.core.run_info \u2014 RunInfo module for HydraFlow.</p> </li> </ul>"},{"location":"api/hydraflow/core/collection/","title":"hydraflow.core.collection","text":"hydraflow.core.collection<p> source module hydraflow.core.collection </p> <p>Provide a collection of items that implements the Sequence protocol.</p> <p> Classes </p> <ul> <li> <p>Collection \u2014 A collection of items that implements the Sequence protocol.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>to_hashable \u2014 Convert a value to a hashable instance.</p> </li> <li> <p>matches \u2014 Check if a value matches the given criterion.</p> </li> </ul> <p> source class Collection[I](items: Iterable[I], get: Callable[[I, str, Any | Callable[[I], Any]], Any] | None = None) </p> <p>Bases : Sequence[I]</p> <p>A collection of items that implements the Sequence protocol.</p> <p> Methods </p> <ul> <li> <p>filter \u2014 Filter items based on criteria.</p> </li> <li> <p>exclude \u2014 Exclude items based on criteria.</p> </li> <li> <p>try_get \u2014 Try to get a single item matching the specified criteria.</p> </li> <li> <p>get \u2014 Get a single item matching the specified criteria.</p> </li> <li> <p>first \u2014 Get the first item matching the specified criteria.</p> </li> <li> <p>last \u2014 Get the last item matching the specified criteria.</p> </li> <li> <p>to_list \u2014 Extract a list of values for a specific key from all items.</p> </li> <li> <p>to_numpy \u2014 Extract values for a specific key from all items as a NumPy array.</p> </li> <li> <p>to_series \u2014 Extract values for a specific key from all items as a Polars series.</p> </li> <li> <p>unique \u2014 Get the unique values for a specific key across all items.</p> </li> <li> <p>n_unique \u2014 Count the number of unique values for a specific key across all items.</p> </li> <li> <p>sort \u2014 Sort items based on one or more keys.</p> </li> <li> <p>map \u2014 Apply a function to each item and return an iterator of results.</p> </li> <li> <p>to_frame \u2014 Convert the collection to a Polars DataFrame.</p> </li> <li> <p>group_by \u2014 Group items by one or more keys and return a GroupBy instance.</p> </li> <li> <p>sample \u2014 Sample a random subset of items from the collection.</p> </li> <li> <p>shuffle \u2014 Shuffle the items in the collection.</p> </li> <li> <p>eq \u2014 Create a predicate function that checks if two attributes are equal.</p> </li> <li> <p>ne \u2014 Create a predicate function that checks if two attributes are not equal.</p> </li> <li> <p>gt \u2014 Create a predicate function that checks if the left &gt; the right.</p> </li> <li> <p>lt \u2014 Create a predicate function that checks if the left &lt; the right.</p> </li> <li> <p>ge \u2014 Create a predicate function that checks if the left &gt;= the right.</p> </li> <li> <p>le \u2014 Create a predicate function that checks if the left &lt;= the right.</p> </li> <li> <p>startswith \u2014 Create a predicate function that checks if an attribute starts with a prefix.</p> </li> <li> <p>endswith \u2014 Create a predicate function that checks if an attribute ends with a suffix.</p> </li> <li> <p>match \u2014 Create a predicate function that checks if an attribute matches a pattern.</p> </li> </ul> <p> source method Collection.filter(*criteria: Callable[[I], bool] | Iterable[Any] | tuple[str, Any], **kwargs: Any) \u2192 Self </p> <p>Filter items based on criteria.</p> <p>This method allows filtering items using various criteria:</p> <ul> <li>Callable criteria that take an item and return a boolean</li> <li>Key-value tuples where the key is a string and the value   is compared using the <code>matches</code> function</li> <li>Keyword arguments, where the key is a string and the value   is compared using the <code>matches</code> function</li> </ul> <p>The <code>matches</code> function supports the following comparison types:</p> <ul> <li>Callable: The predicate function is called with the value</li> <li>List/Set: Checks if the value is in the list/set</li> <li>Tuple of length 2: Checks if the value is in the range [min, max]</li> <li>Other: Checks for direct equality</li> </ul> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | Iterable[Any] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new Collection containing only the items that match all criteria.</p> </li> </ul> <p> Examples </p> <pre><code># Filter using a callable\nfiltered = collection.filter(lambda x: x &gt; 5)\n\n# Filter using a key-value tuple\nfiltered = collection.filter((\"age\", 25))\n\n# Filter using an iterable\nfiltered = collection.filter([True, False, True])\n\n# Filter using keyword arguments\nfiltered = collection.filter(age=25, name=\"John\")\n\n# Filter using range\nfiltered = collection.filter((\"age\", (20, 30)))\n\n# Filter using list membership\nfiltered = collection.filter((\"name\", [\"John\", \"Jane\"]))\n</code></pre> <p> source method Collection.exclude(*criteria: Callable[[I], bool] | Iterable[Any] | tuple[str, Any], **kwargs: Any) \u2192 Self </p> <p>Exclude items based on criteria.</p> <p>This method works the opposite of <code>filter</code>. It excludes items that match any of the specified criteria.</p> <p>This method allows excluding items using various criteria:</p> <ul> <li>Callable criteria that take an item and return a boolean</li> <li>Key-value tuples where the key is a string and the value   is compared using the <code>matches</code> function</li> <li>Keyword arguments, where the key is a string and the value   is compared using the <code>matches</code> function</li> </ul> <p>The <code>matches</code> function supports the following comparison types:</p> <ul> <li>Callable: The predicate function is called with the value</li> <li>List/Set: Checks if the value is in the list/set</li> <li>Tuple of length 2: Checks if the value is in the range [min, max]</li> <li>Other: Checks for direct equality</li> </ul> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | Iterable[Any] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for exclusion.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for exclusion.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new Collection containing only the items that do not match any of the criteria.</p> </li> </ul> <p> Examples </p> <pre><code># Exclude using a callable\nexcluded = collection.exclude(lambda x: x &gt; 5)\n\n# Exclude using a key-value tuple\nexcluded = collection.exclude((\"age\", 25))\n\n# Exclude using an iterable mask\nexcluded = collection.exclude([True, False, True])\n\n# Exclude using keyword arguments\nexcluded = collection.exclude(age=25, name=\"John\")\n</code></pre> <p> source method Collection.try_get(*criteria: Callable[[I], bool] | tuple[str, Any], **kwargs: Any) \u2192 I | None </p> <p>Try to get a single item matching the specified criteria.</p> <p>This method applies filters and returns a single matching item if exactly one is found, None if no items are found, or raises ValueError if multiple items match.</p> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>I | None \u2014 A single item that matches the criteria, or None if no matches are found.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If multiple items match the criteria.</p> </li> </ul> <p> source method Collection.get(*criteria: Callable[[I], bool] | tuple[str, Any], **kwargs: Any) \u2192 I </p> <p>Get a single item matching the specified criteria.</p> <p>This method applies filters and returns a single matching item, or raises ValueError if no items or multiple items match.</p> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>I \u2014 A single item that matches the criteria.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If no items match or if multiple items match</p> </li> <li> <p>the criteria.</p> </li> <li> <p>_value_error</p> </li> </ul> <p> source method Collection.first(*criteria: Callable[[I], bool] | tuple[str, Any], **kwargs: Any) \u2192 I </p> <p>Get the first item matching the specified criteria.</p> <p>This method applies filters and returns the first matching item, or raises ValueError if no items match.</p> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>I \u2014 The first item that matches the criteria.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If no items match the criteria.</p> </li> <li> <p>_value_error</p> </li> </ul> <p> source method Collection.last(*criteria: Callable[[I], bool] | tuple[str, Any], **kwargs: Any) \u2192 I </p> <p>Get the last item matching the specified criteria.</p> <p>This method applies filters and returns the last matching item, or raises ValueError if no items match.</p> <p> Parameters </p> <ul> <li> <p>*criteria :  Callable[[I], bool] | tuple[str, Any] \u2014 Callable criteria or (key, value) tuples for filtering.</p> </li> <li> <p>**kwargs :  Any \u2014 Additional key-value pairs for filtering.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>I \u2014 The last item that matches the criteria.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If no items match the criteria.</p> </li> <li> <p>_value_error</p> </li> </ul> <p> source method Collection.to_list(key: str, default: Any | Callable[[I], Any] = MISSING) \u2192 list[Any] </p> <p>Extract a list of values for a specific key from all items.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to extract from each item.</p> </li> <li> <p>default :  Any | Callable[[I], Any] \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[Any] \u2014 A list containing the values for the specified key from each item.</p> </li> </ul> <p> source method Collection.to_numpy(key: str, default: Any | Callable[[I], Any] = MISSING) \u2192 NDArray[Any] </p> <p>Extract values for a specific key from all items as a NumPy array.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to extract from each item.</p> </li> <li> <p>default :  Any | Callable[[I], Any] \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>NDArray \u2014 A NumPy array containing the values for the specified key from each item.</p> </li> </ul> <p> source method Collection.to_series(key: str, default: Any = MISSING, *, name: str | None = None) \u2192 Series </p> <p>Extract values for a specific key from all items as a Polars series.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to extract from each item.</p> </li> <li> <p>default :  Any \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> <li> <p>name :  str | None \u2014 The name of the series. If not provided, the key will be used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Series \u2014 A Polars series containing the values for the specified key from each item.</p> </li> </ul> <p> source method Collection.unique(key: str, default: Any | Callable[[I], Any] = MISSING) \u2192 NDArray[Any] </p> <p>Get the unique values for a specific key across all items.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to extract unique values for.</p> </li> <li> <p>default :  Any | Callable[[I], Any] \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>NDArray \u2014 A NumPy array containing the unique values for the specified key.</p> </li> </ul> <p> source method Collection.n_unique(key: str, default: Any | Callable[[I], Any] = MISSING) \u2192 int </p> <p>Count the number of unique values for a specific key across all items.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to count unique values for.</p> </li> <li> <p>default :  Any | Callable[[I], Any] \u2014 The default value to return if the key is not found. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The number of unique values for the specified key.</p> </li> </ul> <p> source method Collection.sort(*keys: str, reverse: bool = False) \u2192 Self </p> <p>Sort items based on one or more keys.</p> <p> Parameters </p> <ul> <li> <p>*keys :  str \u2014 The keys to sort by, in order of priority.</p> </li> <li> <p>reverse :  bool \u2014 Whether to sort in descending order (default is ascending).</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new Collection with the items sorted according to the specified keys.</p> </li> </ul> <p> source method Collection.map[**P, R](function: Callable[Concatenate[I, P], R], *args: P.args, **kwargs: P.kwargs) \u2192 Iterator[R] </p> <p>Apply a function to each item and return an iterator of results.</p> <p>This is a memory-efficient mapping operation that lazily evaluates results. Ideal for large collections where memory usage is a concern.</p> <p> Parameters </p> <ul> <li> <p>function :  Callable[Concatenate[I, P], R] \u2014 Function to apply to each item. The item is passed as the first argument.</p> </li> <li> <p>*args :  P.args \u2014 Additional positional arguments to pass to the function.</p> </li> <li> <p>**kwargs :  P.kwargs \u2014 Additional keyword arguments to pass to the function.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>R \u2014 The function's results.</p> </li> </ul> <p> Examples </p> <pre><code># Process results one at a time\nfor result in collection.map(process_item, additional_arg):\n    handle_result(result)\n\n# Convert to list if needed\nresults = list(collection.map(transform_item))\n</code></pre> <p> source method Collection.to_frame(*keys: str | tuple[str, Any | Callable[[I], Any]], defaults: dict[str, Any | Callable[[I], Any]] | None = None, **kwargs: Callable[[I], Any]) \u2192 DataFrame </p> <p>Convert the collection to a Polars DataFrame.</p> <p>This method converts the items in the collection into a Polars DataFrame. It allows specifying multiple keys, where each key can be a string or a tuple. If a tuple is provided, the first element is treated as the key and the second element as the default value for that key.</p> <p> Parameters </p> <ul> <li> <p>*keys :  str | tuple[str, Any | Callable[[I], Any]] \u2014 The keys to include as columns in the DataFrame. If a tuple is provided, the first element is the key and the second element is the default value.</p> </li> <li> <p>defaults :  dict[str, Any | Callable[[I], Any]] | None \u2014 Default values for the keys. If a callable, it will be called with the item and the value returned will be used as the default.</p> </li> <li> <p>**kwargs :  Callable[[I], Any] \u2014 Additional columns to compute using callables that take an item and return a value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A Polars DataFrame containing the specified data from the items.</p> </li> </ul> <p> Examples </p> <pre><code># Convert to DataFrame with single keys\ndf = collection.to_frame(\"name\", \"age\")\n\n# Convert to DataFrame with keys and default values\ndf = collection.to_frame((\"name\", \"Unknown\"), (\"age\", 0))\n</code></pre> <p> source method Collection.group_by(*by: str) \u2192 GroupBy[Self, I] </p> <p>Group items by one or more keys and return a GroupBy instance.</p> <p>This method organizes items into groups based on the specified keys and returns a GroupBy instance that contains the grouped collections. The GroupBy instance behaves like a dictionary, allowing access to collections for each group key.</p> <p> Parameters </p> <ul> <li> <p>*by :  str \u2014 The keys to group by. If a single key is provided, its value will be used as the group key. If multiple keys are provided, a tuple of their values will be used as the group key. Keys can use dot notation (e.g., \"model.type\") to access nested configuration values.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GroupBy[Self, I] \u2014 A GroupBy instance containing the grouped items. Each group is a collection of the same type as the original.</p> </li> </ul> <p> source method Collection.sample(k: int, seed: int | None = None) \u2192 Self </p> <p>Sample a random subset of items from the collection.</p> <p>This method returns a new collection containing a random sample of items from the original collection. The sample is drawn without replacement, meaning each item can only appear once in the sample.</p> <p> Parameters </p> <ul> <li> <p>k :  int \u2014 The number of items to sample.</p> </li> <li> <p>seed :  int | None \u2014 The seed for the random number generator. If provided, the sample will be reproducible.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new collection containing a random sample of items.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If the sample size is greater than the collection size.</p> </li> </ul> <p> source method Collection.shuffle(seed: int | None = None) \u2192 Self </p> <p>Shuffle the items in the collection.</p> <p>This method returns a new collection with the items in random order.</p> <p> Parameters </p> <ul> <li> <p>seed :  int | None \u2014 The seed for the random number generator. If provided, the sample will be reproducible.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self \u2014 A new collection containing the items in random order.</p> </li> </ul> <p> source method Collection.eq(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if two attributes are equal.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value to use if either attribute is not found. If callable, it will be called with the item.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A function that takes an item and returns True if the values of the specified attributes are equal.</p> </li> </ul> <p> Examples </p> <pre><code># Find items where attribute 'a' equals attribute 'b'\nequal_items = collection.filter(collection.eq('a', 'b'))\n</code></pre> <p> source method Collection.ne(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if two attributes are not equal.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value to use if either attribute is not found. If callable, it will be called with the item.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A function that takes an item and returns True if the values of the specified attributes are not equal.</p> </li> </ul> <p> Examples </p> <pre><code># Find items where attribute 'a' is not equal to attribute 'b'\nunequal_items = collection.filter(collection.ne('a', 'b'))\n</code></pre> <p> source method Collection.gt(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if the left &gt; the right.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value to use if either attribute is not found. If callable, it will be called with the item.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A function that takes an item and returns True if the left attribute value is greater than the right attribute value.</p> </li> </ul> <p> Examples </p> <pre><code># Find items where attribute 'a' is greater than attribute 'b'\nitems = collection.filter(collection.gt('a', 'b'))\n</code></pre> <p> source method Collection.lt(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if the left &lt; the right.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value to use if either attribute is not found. If callable, it will be called with the item.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A function that takes an item and returns True if the left attribute value is less than the right attribute value.</p> </li> </ul> <p> Examples </p> <pre><code># Find items where attribute 'a' is less than attribute 'b'\nitems = collection.filter(collection.lt('a', 'b'))\n</code></pre> <p> source method Collection.ge(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if the left &gt;= the right.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source method Collection.le(left: str, right: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if the left &lt;= the right.</p> <p> Parameters </p> <ul> <li> <p>left :  str \u2014 The name of the left attribute to compare.</p> </li> <li> <p>right :  str \u2014 The name of the right attribute to compare.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source method Collection.startswith(key: str, prefix: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if an attribute starts with a prefix.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The name of the attribute to check.</p> </li> <li> <p>prefix :  str \u2014 The prefix to check for.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source method Collection.endswith(key: str, suffix: str, *, default: Any | Callable[[I], Any] = MISSING) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if an attribute ends with a suffix.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The name of the attribute to check.</p> </li> <li> <p>suffix :  str \u2014 The suffix to check for.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source method Collection.match(key: str, pattern: str | Pattern[str], *, default: Any | Callable[[I], Any] = MISSING, flags: _FlagsType = 0) \u2192 Callable[[I], bool] </p> <p>Create a predicate function that checks if an attribute matches a pattern.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The name of the attribute to check.</p> </li> <li> <p>pattern :  str | re.Pattern \u2014 The pattern to check for.</p> </li> <li> <p>default :  Any | Callable[[I], Any], optional \u2014 The default value.</p> </li> <li> <p>flags :  re.RegexFlag, optional \u2014 Flags for the regex pattern.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Callable[[I], bool] \u2014 A predicate function for filtering.</p> </li> </ul> <p> source to_hashable(value: Any) \u2192 Hashable </p> <p>Convert a value to a hashable instance.</p> <p>This function handles various types of values and converts them to hashable equivalents for use in dictionaries and sets.</p> <p> Parameters </p> <ul> <li> <p>value :  Any \u2014 The value to convert to a hashable instance.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Hashable \u2014 A hashable version of the input value.</p> </li> </ul> <p> source matches(value: Any, criterion: Any) \u2192 bool </p> <p>Check if a value matches the given criterion.</p> <p>This function compares the value with the given criteria according to the following rules:</p> <ul> <li>If criterion is callable: Call it with the value and return   the boolean result</li> <li>If criterion is a list or set: Check if the value is in the list/set</li> <li>If criterion is a tuple of length 2: Check if the value is   in the range [criterion[0], criterion[1]]. Both sides are   inclusive</li> <li>Otherwise: Check if the value equals the criteria</li> </ul> <p> Parameters </p> <ul> <li> <p>value :  Any \u2014 The value to be compared with the criterion.</p> </li> <li> <p>criterion :  Any \u2014 The criterion to match against. Can be:</p> <ul> <li>A callable that takes the value and returns a boolean</li> <li>A list or set to check membership</li> <li>A tuple of length 2 to check range inclusion</li> <li>Any other value for direct equality comparison</li> </ul> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 True if the value matches the criterion according to the rules above, False otherwise.</p> </li> </ul> <p> Examples </p> <pre><code>matches(5, lambda x: x &gt; 3)\n</code></pre> <pre><code>True\n</code></pre> <pre><code>matches(2, [1, 2, 3])\n</code></pre> <pre><code>True\n</code></pre> <pre><code>matches(4, (1, 5))\n</code></pre> <pre><code>True\n</code></pre> <pre><code>matches(3, 3)\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"api/hydraflow/core/context/","title":"hydraflow.core.context","text":"hydraflow.core.context<p> source module hydraflow.core.context </p> <p>Provide context managers to log parameters and manage the MLflow run context.</p> <p> Functions </p> <ul> <li> <p>log_run \u2014 Log the parameters from the given configuration instance.</p> </li> <li> <p>start_run \u2014 Start an MLflow run and log parameters using the provided configuration instance.</p> </li> <li> <p>chdir_artifact \u2014 Change the current working directory to the artifact directory of the given run.</p> </li> </ul> <p> source log_run(run: Run) \u2192 Iterator[None] </p> <p>Log the parameters from the given configuration instance.</p> <p>This context manager logs the parameters from the provided configuration instance using MLflow. It also manages the MLflow run context, ensuring that artifacts are logged and the run is properly closed.</p> <p> Parameters </p> <ul> <li> <p>run :  Run \u2014 The run instance.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>None \u2014 None</p> </li> </ul> <p> source start_run(*, chdir: bool = False, run_id: str | None = None, experiment_id: str | None = None, run_name: str | None = None, nested: bool = False, parent_run_id: str | None = None, tags: dict[str, str] | None = None, description: str | None = None, log_system_metrics: bool | None = None) \u2192 Iterator[Run] </p> <p>Start an MLflow run and log parameters using the provided configuration instance.</p> <p>This context manager starts an MLflow run and logs parameters using the specified configuration instance. It ensures that the run is properly closed after completion.</p> <p> Parameters </p> <ul> <li> <p>chdir :  bool \u2014 Whether to change the current working directory to the artifact directory of the current run. Defaults to False.</p> </li> <li> <p>run_id :  str | None \u2014 The existing run ID. Defaults to None.</p> </li> <li> <p>experiment_id :  str | None \u2014 The experiment ID. Defaults to None.</p> </li> <li> <p>run_name :  str | None \u2014 The name of the run. Defaults to None.</p> </li> <li> <p>nested :  bool \u2014 Whether to allow nested runs. Defaults to False.</p> </li> <li> <p>parent_run_id :  str | None \u2014 The parent run ID. Defaults to None.</p> </li> <li> <p>tags :  dict[str, str] | None \u2014 Tags to associate with the run. Defaults to None.</p> </li> <li> <p>description :  str | None \u2014 A description of the run. Defaults to None.</p> </li> <li> <p>log_system_metrics :  bool | None \u2014 Whether to log system metrics. Defaults to None.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Run \u2014 An MLflow Run instance representing the started run.</p> </li> </ul> <p> source chdir_artifact(run: Run) \u2192 Iterator[Path] </p> <p>Change the current working directory to the artifact directory of the given run.</p> <p>This context manager changes the current working directory to the artifact directory of the given run. It ensures that the directory is changed back to the original directory after the context is exited.</p> <p> Parameters </p> <ul> <li> <p>run :  Run | None \u2014 The run to get the artifact directory from.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>str \u2014 The artifact directory</p> </li> </ul>"},{"location":"api/hydraflow/core/group_by/","title":"hydraflow.core.group_by","text":"hydraflow.core.group_by<p> source module hydraflow.core.group_by </p> <p>GroupBy module for organizing and aggregating collections of items.</p> <p>This module provides the GroupBy class, which represents the result of a group_by operation on a Collection. It organizes items into groups based on specified keys and enables aggregation operations across those groups.</p> <p>The GroupBy class implements a dictionary-like interface, allowing access to individual groups through key lookup, iteration, and standard dictionary methods like keys(), values(), and items().</p> <p> Example </p> <pre><code># Group runs by model type\ngrouped = runs.group_by(\"model.type\")\n\n# Access a specific group\ntransformer_runs = grouped[\"transformer\"]\n\n# Iterate through groups\nfor model_type, group in grouped.items():\n    print(f\"Model: {model_type}, Runs: {len(group)}\")\n\n# Perform aggregations\nstats = grouped.agg(\n    \"accuracy\",\n    \"loss\",\n    avg_time=lambda g: sum(r.get(\"runtime\") for r in g) / len(g)\n)\n</code></pre> <p>The GroupBy class supports aggregation through the agg() method, which can compute both predefined metrics from the grouped items and custom aggregations specified as callables.</p> <p> Classes </p> <ul> <li> <p>GroupBy \u2014 Represents the result of a group_by operation on a Collection.</p> </li> </ul> <p> source class GroupBy[C: Collection[Any], I](by: tuple[str, ...], groups: dict[Any, C]) </p> <p>Represents the result of a group_by operation on a Collection.</p> <p>The GroupBy class organizes items from a Collection into groups based on specified keys. It provides a dictionary-like interface for accessing the groups and methods for aggregating data across the groups.</p> <p>Initialize a GroupBy instance.</p> <p> Attributes </p> <ul> <li> <p>by :  tuple[str, ...] \u2014 The keys used for grouping.</p> </li> <li> <p>groups :  dict[Any, C] \u2014 A dictionary mapping group keys to Collection instances.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>by :  tuple[str, ...] \u2014 The keys used for grouping.</p> </li> <li> <p>groups :  dict[Any, C] \u2014 A dictionary mapping group keys to Collection instances.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>keys \u2014 Get the keys of the groups.</p> </li> <li> <p>values \u2014 Get the values (Collections) of the groups.</p> </li> <li> <p>items \u2014 Get the (key, value) pairs of the groups.</p> </li> <li> <p>agg \u2014 Aggregate data across groups.</p> </li> </ul> <p> source method GroupBy.keys() \u2192 KeysView[Any] </p> <p>Get the keys of the groups.</p> <p> Returns </p> <ul> <li> <p>KeysView[Any] \u2014 A view of the group keys.</p> </li> </ul> <p> source method GroupBy.values() \u2192 ValuesView[C] </p> <p>Get the values (Collections) of the groups.</p> <p> Returns </p> <ul> <li> <p>ValuesView[C] \u2014 A view of the group values.</p> </li> </ul> <p> source method GroupBy.items() \u2192 ItemsView[Any, C] </p> <p>Get the (key, value) pairs of the groups.</p> <p> Returns </p> <ul> <li> <p>ItemsView[Any, C] \u2014 A view of the (key, value) pairs.</p> </li> </ul> <p> source method GroupBy.agg(*aggs: str, **named_aggs: Callable[[C | Sequence[I]], Any]) \u2192 DataFrame </p> <p>Aggregate data across groups.</p> <p>This method computes aggregations for each group and returns the results as a DataFrame. There are two ways to specify aggregations:</p> <ol> <li>String keys: These are interpreted as attributes to extract from each    item in the group.</li> <li>Callables: Functions that take a Collection or Sequence of items and    return an aggregated value.</li> </ol> <p> Parameters </p> <ul> <li> <p>*aggs :  str \u2014 String keys to aggregate.</p> </li> <li> <p>**named_aggs :  Callable[[C | Sequence[I]], Any] \u2014 Named aggregation functions.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A DataFrame with group keys and aggregated values.</p> </li> </ul> <p> Example </p> <pre><code># Aggregate by accuracy and loss, and compute average runtime\nstats = grouped.agg(\n    \"accuracy\",\n    \"loss\",\n    avg_runtime=lambda g: sum(r.get(\"runtime\") for r in g) / len(g)\n)\n</code></pre>"},{"location":"api/hydraflow/core/io/","title":"hydraflow.core.io","text":"hydraflow.core.io<p> source module hydraflow.core.io </p> <p>Provide utility functions for HydraFlow.</p> <p> Functions </p> <ul> <li> <p>file_uri_to_path \u2014 Convert a file URI to a local path.</p> </li> <li> <p>get_artifact_dir \u2014 Retrieve the artifact directory for the given run.</p> </li> <li> <p>log_text \u2014 Log text files in the given directory as artifacts.</p> </li> <li> <p>predicate_experiment \u2014 Predicate an experiment based on the experiment names.</p> </li> <li> <p>get_experiment_name \u2014 Get the job name from an experiment directory.</p> </li> <li> <p>get_experiment_names \u2014 Get the experiment names.</p> </li> <li> <p>iter_experiment_dirs \u2014 Iterate over the experiment directories.</p> </li> <li> <p>iter_run_dirs \u2014 Iterate over the run directories in the tracking directory.</p> </li> <li> <p>iter_artifacts_dirs \u2014 Iterate over the artifacts directories in the tracking directory.</p> </li> <li> <p>iter_artifact_paths \u2014 Iterate over the artifact paths in the tracking directory.</p> </li> </ul> <p> source file_uri_to_path(uri: str) \u2192 Path </p> <p>Convert a file URI to a local path.</p> <p> source get_artifact_dir(run: Run) \u2192 Path </p> <p>Retrieve the artifact directory for the given run.</p> <p>This function uses MLflow to get the artifact directory for the given run.</p> <p> Parameters </p> <ul> <li> <p>run :  Run | None \u2014 The run instance. Defaults to None.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Path \u2014 The local path to the directory where the artifacts are downloaded.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source log_text(run: Run, from_dir: Path, pattern: str = '*.log') \u2192 None </p> <p>Log text files in the given directory as artifacts.</p> <p>Append the text files to the existing text file in the artifact directory.</p> <p> Parameters </p> <ul> <li> <p>run :  Run \u2014 The run instance.</p> </li> <li> <p>from_dir :  Path \u2014 The directory to find the logs in.</p> </li> <li> <p>pattern :  str \u2014 The pattern to match the logs.</p> </li> </ul> <p> source predicate_experiment(experiment: Experiment, experiment_names: list[str] | Callable[[str], bool] | None = None) \u2192 bool </p> <p>Predicate an experiment based on the experiment names.</p> <p> source get_experiment_name(experiment_dir: Path) \u2192 str </p> <p>Get the job name from an experiment directory.</p> <p>Extracts the job name from the meta.yaml file. Returns an empty string if the file does not exist or if the job name cannot be found.</p> <p> Parameters </p> <ul> <li> <p>experiment_dir :  Path \u2014 Path to the experiment directory containing the meta.yaml file</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The job name as a string, or an empty string if the file does not exist</p> </li> </ul> <p> source get_experiment_names() \u2192 list[str] </p> <p>Get the experiment names.</p> <p> Returns </p> <ul> <li> <p>list[str] \u2014 A list of experiment names sorted by the name.</p> </li> </ul> <p> source iter_experiment_dirs(experiment_names: str | list[str] | Callable[[str], bool] | None = None) \u2192 Iterator[Path] </p> <p>Iterate over the experiment directories.</p> <p> source iter_run_dirs(experiment_names: str | list[str] | Callable[[str], bool] | None = None) \u2192 Iterator[Path] </p> <p>Iterate over the run directories in the tracking directory.</p> <p> source iter_artifacts_dirs(experiment_names: str | list[str] | Callable[[str], bool] | None = None) \u2192 Iterator[Path] </p> <p>Iterate over the artifacts directories in the tracking directory.</p> <p> source iter_artifact_paths(artifact_path: str | Path, experiment_names: str | list[str] | Callable[[str], bool] | None = None) \u2192 Iterator[Path] </p> <p>Iterate over the artifact paths in the tracking directory.</p>"},{"location":"api/hydraflow/core/main/","title":"hydraflow.core.main","text":"hydraflow.core.main<p> source module hydraflow.core.main </p> <p>Integration of MLflow experiment tracking with Hydra configuration management.</p> <p>This module provides decorators and utilities to seamlessly combine Hydra's configuration management with MLflow's experiment tracking capabilities. It enables automatic run deduplication, configuration storage, and experiment management.</p> <p>The main functionality is provided through the <code>main</code> decorator, which can be used to wrap experiment entry points. This decorator handles:</p> <ul> <li>Configuration management via Hydra</li> <li>Experiment tracking via MLflow</li> <li>Run deduplication based on configurations</li> <li>Working directory management</li> <li>Automatic configuration storage</li> </ul> <p> Example </p> <pre><code>import hydraflow\nfrom dataclasses import dataclass\nfrom mlflow.entities import Run\n\n@dataclass\nclass Config:\n    learning_rate: float\n    batch_size: int\n\n@hydraflow.main(Config)\ndef train(run: Run, config: Config):\n    # Your training code here\n    pass\n</code></pre> <p> Functions </p> <ul> <li> <p>main \u2014 Decorate a function for configuring and running MLflow experiments with Hydra.</p> </li> <li> <p>set_experiment \u2014 Set the MLflow tracking URI if provided and experiment.</p> </li> <li> <p>get_run_id \u2014 Try to get the run ID for the given configuration.</p> </li> <li> <p>equals \u2014 Check if the run directory matches the given configuration or overrides.</p> </li> </ul> <p> source main[C](node: C | type[C], config_name: str = 'config', *, tracking_uri: str | Path | None = None, chdir: bool = False, force_new_run: bool = False, match_overrides: bool = False, rerun_finished: bool = False, dry_run: bool = False, update: Callable[[C], C | None] | None = None) \u2192 Callable[..., Callable[[], None]] </p> <p>Decorate a function for configuring and running MLflow experiments with Hydra.</p> <p>This decorator combines Hydra configuration management with MLflow experiment tracking. It automatically handles run deduplication and configuration storage.</p> <p> Parameters </p> <ul> <li> <p>node :  C | type[C] \u2014 Configuration node class or instance defining the structure of the configuration.</p> </li> <li> <p>config_name :  str \u2014 Name of the configuration. Defaults to \"config\".</p> </li> <li> <p>tracking_uri :  str | Path | None \u2014 The tracking URI for MLflow. If not provided, MLflow's default tracking URI is used. Defaults to None.</p> </li> <li> <p>chdir :  bool \u2014 If True, changes working directory to the artifact directory of the run. Defaults to False.</p> </li> <li> <p>force_new_run :  bool \u2014 If True, always creates a new MLflow run instead of reusing existing ones. Defaults to False.</p> </li> <li> <p>match_overrides :  bool \u2014 If True, matches runs based on Hydra CLI overrides instead of full config. Defaults to False.</p> </li> <li> <p>rerun_finished :  bool \u2014 If True, allows rerunning completed runs. Defaults to False.</p> </li> <li> <p>dry_run :  bool \u2014 If True, starts the hydra job but does not run the application itself. This allows users to preview the configuration and settings without executing the actual run. Defaults to False.</p> </li> <li> <p>update :  Callable[[C], C | None] | None \u2014 A function that takes a configuration and returns a new configuration or None. The function can modify the configuration in-place and/or return it. If the function returns None, the original (potentially modified) configuration is used. Changes made by this function are saved to the configuration file. This is useful for adding derived parameters, ensuring consistency between related values, or adding runtime information to the configuration. Defaults to None.</p> </li> </ul> <p> source set_experiment(hc: HydraConf, tracking_uri: str | Path | None) \u2192 Experiment </p> <p>Set the MLflow tracking URI if provided and experiment.</p> <p> Parameters </p> <ul> <li> <p>hc :  HydraConf \u2014 The Hydra configuration instance.</p> </li> <li> <p>tracking_uri :  str | Path | None \u2014 The tracking URI for MLflow. If None, MLflow's default tracking URI is used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Experiment \u2014 The MLflow experiment instance.</p> </li> </ul> <p> source get_run_id(uri: str, config: Any, overrides: list[str] | None) \u2192 str | None </p> <p>Try to get the run ID for the given configuration.</p> <p>If the run is not found, the function will return None.</p> <p> Parameters </p> <ul> <li> <p>uri :  str \u2014 The URI of the experiment.</p> </li> <li> <p>config :  object \u2014 The configuration instance.</p> </li> <li> <p>overrides :  list[str] | None \u2014 The task overrides.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | None \u2014 The run ID for the given configuration or overrides. Returns None if no run ID is found.</p> </li> </ul> <p> source equals(run_dir: Path, config: Any, overrides: list[str] | None) \u2192 bool </p> <p>Check if the run directory matches the given configuration or overrides.</p> <p> Parameters </p> <ul> <li> <p>run_dir :  Path \u2014 The run directory.</p> </li> <li> <p>config :  object \u2014 The configuration instance.</p> </li> <li> <p>overrides :  list[str] | None \u2014 The task overrides.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 True if the run directory matches the given configuration or overrides, False otherwise.</p> </li> </ul>"},{"location":"api/hydraflow/core/run/","title":"hydraflow.core.run","text":"hydraflow.core.run<p> source module hydraflow.core.run </p> <p>Run module for HydraFlow.</p> <p>This module provides the Run class, which represents an MLflow Run in HydraFlow. A Run contains three main components:</p> <ol> <li>info: Information about the run, which includes the run directory,    run ID, and job name.</li> <li>cfg: Configuration loaded from the Hydra configuration file.</li> <li>impl: Implementation instance created by the provided    factory function.</li> </ol> <p>The Run class allows accessing these components through a unified interface, and provides methods for setting default configuration values and filtering runs.</p> <p>The implementation instance (impl) can be created using a factory function that accepts either just the artifacts directory path, or both the artifacts directory path and the configuration instance. This flexibility allows implementation classes to be configuration-aware and adjust their behavior based on the run's configuration.</p> <p> Classes </p> <ul> <li> <p>Run \u2014 Represent an MLflow Run in HydraFlow.</p> </li> </ul> <p> source class Run[C, I = None](run_dir: Path, impl_factory: Callable[[Path], I] | Callable[[Path, C], I] | None = None) </p> <p>Represent an MLflow Run in HydraFlow.</p> <p>A Run contains information about the run, configuration, and implementation. The configuration type C and implementation type I are specified as type parameters.</p> <p> Attributes </p> <ul> <li> <p>info :  RunInfo \u2014 Information about the run, such as run directory, run ID, and job name.</p> </li> <li> <p>impl_factory :  Callable[[Path], I] | Callable[[Path, C], I] \u2014 Factory function to create the implementation instance.</p> </li> <li> <p>cfg :  C \u2014 The configuration instance loaded from the Hydra configuration file.</p> </li> <li> <p>impl :  I \u2014 The implementation instance created by the factory function.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>load \u2014 Load a Run from a run directory.</p> </li> <li> <p>update \u2014 Set default value(s) in the configuration if they don't already exist.</p> </li> <li> <p>get \u2014 Get a value from the information or configuration.</p> </li> <li> <p>lit \u2014 Create a Polars literal expression from a run key.</p> </li> <li> <p>to_frame \u2014 Convert the Run to a DataFrame.</p> </li> <li> <p>to_frame_impl \u2014 Convert the Run to a DataFrame.</p> </li> <li> <p>to_dict \u2014 Convert the Run to a dictionary.</p> </li> <li> <p>chdir \u2014 Change the current working directory to the artifact directory.</p> </li> <li> <p>path \u2014 Return the path relative to the artifact directory.</p> </li> <li> <p>iterdir \u2014 Iterate over the artifact directories for the run.</p> </li> <li> <p>glob \u2014 Glob the artifact directories for the run.</p> </li> </ul> <p> source property Run.cfg: C </p> <p>The configuration instance loaded from the Hydra configuration file.</p> <p> source property Run.impl: I </p> <p>The implementation instance created by the factory function.</p> <p>This property dynamically examines the signature of the impl_factory using the inspect module and calls it with the appropriate arguments:</p> <ul> <li>If the factory accepts one parameter: called with just the artifacts   directory</li> <li>If the factory accepts two parameters: called with the artifacts   directory and the configuration instance</li> </ul> <p>This allows implementation classes to be configuration-aware and utilize both the file system and configuration information.</p> <p> source classmethod Run.load(run_dir: str | Path | Iterable[str | Path], impl_factory: Callable[[Path], I] | Callable[[Path, C], I] | None = None) \u2192 Self | RunCollection[Self] </p> <p>Load a Run from a run directory.</p> <p> Parameters </p> <ul> <li> <p>run_dir :  str | Path | Iterable[str | Path] \u2014 The directory where the MLflow runs are stored, either as a string, a Path instance, or an iterable of them.</p> </li> <li> <p>impl_factory :  Callable[[Path], I] | Callable[[Path, C], I] | None \u2014 A factory function that creates the implementation instance. It can accept either just the artifacts directory path, or both the path and the configuration instance. Defaults to None, in which case a function that returns None is used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Self | RunCollection[Self] \u2014 A single Run instance or a RunCollection of Run instances.</p> </li> </ul> <p> source method Run.update(key: str | tuple[str, ...], value: Any | Callable[[Self], Any], *, force: bool = False) \u2192 None </p> <p>Set default value(s) in the configuration if they don't already exist.</p> <p>This method adds a value or multiple values to the configuration, but only if the corresponding keys don't already have values. Existing values will not be modified.</p> <p> Parameters </p> <ul> <li> <p>key :  str | tuple[str, ...] \u2014 Either a string representing a single configuration path (can use dot notation like \"section.subsection.param\"), or a tuple of strings to set multiple related configuration values at once.</p> </li> <li> <p>value :  Any | Callable[[Self], Any] \u2014 The value to set. This can be:</p> <ul> <li>For string keys: Any value, or a callable that returns   a value</li> <li>For tuple keys: An iterable with the same length as the   key tuple, or a callable that returns such an iterable</li> <li>For callable values: The callable must accept a single argument   of type Run (self) and return the appropriate value type</li> </ul> </li> <li> <p>force :  bool \u2014 Whether to force the update even if the key already exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>TypeError \u2014 If a tuple key is provided but the value is not an iterable, or if the callable doesn't return an iterable.</p> </li> </ul> <p> source method Run.get(key: str, default: Any | Callable[[Self], Any] = MISSING) \u2192 Any </p> <p>Get a value from the information or configuration.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to look for. Can use dot notation for nested keys in configuration. Special keys:</p> <ul> <li>\"cfg\": Returns the configuration object</li> <li>\"impl\": Returns the implementation object</li> <li>\"info\": Returns the run information object</li> </ul> </li> <li> <p>default :  Any | Callable[[Self], Any] \u2014 Value to return if the key is not found. If a callable, it will be called with the Run instance and the value returned will be used as the default. If not provided, AttributeError will be raised.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Any \u2014 The value associated with the key, or the default value if the key is not found and a default is provided.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>AttributeError \u2014 If the key is not found and no default is provided.</p> </li> </ul> <p>Note</p> <p>The search order for keys is:</p> <ol> <li>Configuration (<code>cfg</code>)</li> <li>Implementation (<code>impl</code>)</li> <li>Run information (<code>info</code>)</li> <li>Run object itself (<code>self</code>)</li> </ol> <p> source method Run.lit(key: str, default: Any | Callable[[Self], Any] = MISSING, *, dtype: PolarsDataType | None = None) \u2192 Expr </p> <p>Create a Polars literal expression from a run key.</p> <p> Parameters </p> <ul> <li> <p>key :  str \u2014 The key to look up in the run's configuration or info.</p> </li> <li> <p>default :  Any | Callable[[Run], Any], optional \u2014 Default value to use if the key is missing. If a callable is provided, it will be called with the Run instance.</p> </li> <li> <p>dtype :  PolarsDataType | None \u2014 Explicit data type for the literal expression.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Expr \u2014 A Polars literal expression aliased to the provided key.</p> </li> </ul> <p> source method Run.to_frame(function: Callable[[Self], DataFrame], *keys: str | tuple[str, Any | Callable[[Self], Any]]) \u2192 DataFrame </p> <p>Convert the Run to a DataFrame.</p> <p> Parameters </p> <ul> <li> <p>function :  Callable[[Run], DataFrame] \u2014 A function that takes a Run instance and returns a DataFrame.</p> </li> <li> <p>keys :  str | tuple[str, Any | Callable[[Run], Any]] \u2014 The keys to add to the DataFrame.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A DataFrame representation of the Run.</p> </li> </ul> <p> source method Run.to_frame_impl(function: Callable[[I], DataFrame], *keys: str | tuple[str, Any | Callable[[Self], Any]]) \u2192 DataFrame </p> <p>Convert the Run to a DataFrame.</p> <p> Parameters </p> <ul> <li> <p>function :  Callable[[I], DataFrame] \u2014 A function that takes a Run.impl instance and returns a DataFrame.</p> </li> <li> <p>keys :  str | tuple[str, Any | Callable[[Run], Any]] \u2014 The keys to add to the DataFrame.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A DataFrame representation of the Run.</p> </li> </ul> <p> source method Run.to_dict(*, flatten: bool = True) \u2192 dict[str, Any] </p> <p>Convert the Run to a dictionary.</p> <p> Parameters </p> <ul> <li> <p>flatten :  bool, optional \u2014 If True, flattens nested dictionaries. Defaults to True.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, Any] \u2014 A dictionary representation of the Run's configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>TypeError \u2014 If the configuration is not a dictionary.</p> </li> </ul> <p> source method Run.chdir(relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Change the current working directory to the artifact directory.</p> <p>This context manager changes the current working directory to the artifact directory of the run. It ensures that the directory is changed back to the original directory after the context is exited.</p> <p> Parameters </p> <ul> <li> <p>relative_dir :  str \u2014 The relative directory to the artifact directory. Defaults to an empty string.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 The artifact directory of the run.</p> </li> </ul> <p> source method Run.path(relative_path: str = '') \u2192 Path </p> <p>Return the path relative to the artifact directory.</p> <p> Parameters </p> <ul> <li> <p>relative_path :  str \u2014 The relative path to the artifact directory.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Path \u2014 The path relative to the artifact directory.</p> </li> </ul> <p> source method Run.iterdir(relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Iterate over the artifact directories for the run.</p> <p> Parameters </p> <ul> <li> <p>relative_dir :  str \u2014 The relative directory to iterate over.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 The artifact directory for the run.</p> </li> </ul> <p> source method Run.glob(pattern: str, relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Glob the artifact directories for the run.</p> <p> Parameters </p> <ul> <li> <p>pattern :  str \u2014 The pattern to glob.</p> </li> <li> <p>relative_dir :  str \u2014 The relative directory to glob.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 The existing artifact paths that match the pattern.</p> </li> </ul>"},{"location":"api/hydraflow/core/run_collection/","title":"hydraflow.core.run_collection","text":"hydraflow.core.run_collection<p> source module hydraflow.core.run_collection </p> <p>RunCollection module for HydraFlow.</p> <p>This module provides the RunCollection class, which represents a collection of MLflow Runs in HydraFlow. RunCollection offers functionality for filtering, sorting, grouping, and analyzing runs, as well as converting run data to various formats such as DataFrames.</p> <p>The RunCollection class implements the Sequence protocol, allowing it to be used like a standard Python list while providing specialized methods for working with Run instances.</p> <p> Example </p> <pre><code># Create a collection from a list of runs\nruns = RunCollection([run1, run2, run3])\n\n# Filter runs based on criteria\nfiltered = runs.filter((\"metrics.accuracy\", lambda acc: acc &gt; 0.9))\n\n# Sort runs by specific keys\nsorted_runs = runs.sort(\"metrics.accuracy\", reverse=True)\n\n# Group runs by model type\ngrouped = runs.group_by(\"model.type\")\n\n# Compute aggregates on grouped data\nmetrics_df = grouped.agg(\n    avg_acc=lambda rc: sum(r.get(\"metrics.accuracy\") for r in rc) / len(rc)\n)\n\n# Convert runs to a DataFrame for analysis\ndf = runs.to_frame(\"run_id\", \"model.type\", \"metrics.accuracy\")\n</code></pre> <p>Note</p> <p>This module requires Polars and NumPy for DataFrame operations and numerical computations.</p> <p> Classes </p> <ul> <li> <p>RunCollection \u2014 A collection of Run instances that implements the Sequence protocol.</p> </li> </ul> <p> source class RunCollection[R: Run[Any, Any]](items: Iterable[I], get: Callable[[I, str, Any | Callable[[I], Any]], Any] | None = None) </p> <p>Bases : Collection[R]</p> <p>A collection of Run instances that implements the Sequence protocol.</p> <p>RunCollection provides methods for filtering, sorting, grouping, and analyzing runs, as well as converting run data to various formats such as DataFrames.</p> <p> Parameters </p> <ul> <li> <p>runs :  Iterable[Run] \u2014 An iterable of Run instances to include in the collection.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>update \u2014 Update configuration values for all runs in the collection.</p> </li> <li> <p>concat \u2014 Concatenate the results of a function applied to all runs in the collection.</p> </li> <li> <p>iterdir \u2014 Iterate over the artifact directories for all runs in the collection.</p> </li> <li> <p>glob \u2014 Glob the artifact directories for all runs in the collection.</p> </li> </ul> <p> source method RunCollection.update(key: str | tuple[str, ...], value: Any | Callable[[R], Any], *, force: bool = False) \u2192 None </p> <p>Update configuration values for all runs in the collection.</p> <p>This method calls the update method on each run in the collection.</p> <p> Parameters </p> <ul> <li> <p>key :  str | tuple[str, ...] \u2014 Either a string representing a single configuration path or a tuple of strings to set multiple configuration values.</p> </li> <li> <p>value :  Any | Callable[[R], Any] \u2014 The value(s) to set or a callable that returns such values.</p> </li> <li> <p>force :  bool \u2014 Whether to force updates even if the keys already exist.</p> </li> </ul> <p> source method RunCollection.concat(function: Callable[[R], DataFrame], *keys: str | tuple[str, Any | Callable[[R], Any]]) \u2192 DataFrame </p> <p>Concatenate the results of a function applied to all runs in the collection.</p> <p>This method applies the provided function to each run in the collection and concatenates the resulting DataFrames along the specified keys.</p> <p> Parameters </p> <ul> <li> <p>function :  Callable[[R], DataFrame] \u2014 A function that takes a Run instance and returns a DataFrame.</p> </li> <li> <p>keys :  str | tuple[str, Any | Callable[[R], Any]] \u2014 The keys to add to the DataFrame.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DataFrame \u2014 A DataFrame representation of the Run collection.</p> </li> </ul> <p> source method RunCollection.iterdir(relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Iterate over the artifact directories for all runs in the collection.</p> <p>This method yields all files and directories in the specified relative directory for each run in the collection.</p> <p> Parameters </p> <ul> <li> <p>relative_dir :  str \u2014 The relative directory within the artifacts directory to iterate over.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 Each path in the specified directory for each run in the collection.</p> </li> </ul> <p> source method RunCollection.glob(pattern: str, relative_dir: str = '') \u2192 Iterator[Path] </p> <p>Glob the artifact directories for all runs in the collection.</p> <p>This method yields all paths matching the specified pattern in the relative directory for each run in the collection.</p> <p> Parameters </p> <ul> <li> <p>pattern :  str \u2014 The glob pattern to match files or directories.</p> </li> <li> <p>relative_dir :  str \u2014 The relative directory within the artifacts directory to search in.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>Path \u2014 Each path matching the pattern for each run in the collection.</p> </li> </ul>"},{"location":"api/hydraflow/core/run_info/","title":"hydraflow.core.run_info","text":"hydraflow.core.run_info<p> source module hydraflow.core.run_info </p> <p>RunInfo module for HydraFlow.</p> <p>This module provides the RunInfo class, which represents a MLflow Run in HydraFlow. RunInfo contains information about a run, such as the run directory, run ID, and job name. The job name is extracted from the Hydra configuration file and represents the MLflow Experiment name that was used when the run was created.</p> <p> Classes </p> <ul> <li> <p>RunInfo \u2014 Information about a MLflow Run in HydraFlow.</p> </li> </ul> <p> source dataclass RunInfo(run_dir: Path) </p> <p>Information about a MLflow Run in HydraFlow.</p> <p>This class represents a MLflow Run and contains information such as the run directory, run ID, and job name. The job name is extracted from the Hydra configuration file and represents the MLflow Experiment name that was used when the run was created.</p> <p> Attributes </p> <ul> <li> <p>run_dir :  Path \u2014 The MLflow Run directory, which contains metrics, parameters, and artifacts.</p> </li> <li> <p>run_id :  str \u2014 The MLflow run ID, which is the name of the run directory.</p> </li> <li> <p>job_name :  str \u2014 The Hydra job name, which was used as the MLflow Experiment name.</p> </li> </ul> <p> source property RunInfo.run_id: str </p> <p>The MLflow run ID, which is the name of the run directory.</p> <p> source property RunInfo.job_name: str </p> <p>The Hydra job name, which was used as the MLflow Experiment name.</p> <p>An empty string if the job name cannot be extracted from the Hydra configuration file (e.g., if the file does not exist or does not contain the expected format).</p>"},{"location":"api/hydraflow/executor/","title":"hydraflow.executor","text":"hydraflow.executor<p> source package hydraflow.executor </p> <p> Modules </p> <ul> <li> <p>hydraflow.executor.aio \u2014 Asynchronous execution.</p> </li> <li> <p>hydraflow.executor.conf \u2014 Configuration dataclasses for Hydraflow executor.</p> </li> <li> <p>hydraflow.executor.io \u2014 Hydraflow jobs IO.</p> </li> <li> <p>hydraflow.executor.job \u2014 Job execution and argument handling for HydraFlow.</p> </li> <li> <p>hydraflow.executor.parser \u2014 Parse and convert string representations of numbers and ranges.</p> </li> </ul>"},{"location":"api/hydraflow/executor/aio/","title":"hydraflow.executor.aio","text":"hydraflow.executor.aio<p> source module hydraflow.executor.aio </p> <p>Asynchronous execution.</p> <p> Functions </p> <ul> <li> <p>run \u2014 Run multiple tasks.</p> </li> <li> <p>arun \u2014 Run a command asynchronously.</p> </li> <li> <p>alog \u2014 Log a stream of output asynchronously.</p> </li> </ul> <p> source run(iterable: Iterable[Task]) \u2192 int | None </p> <p>Run multiple tasks.</p> <p> source async arun(args: list[str], stdout: Callable[[str], None], stderr: Callable[[str], None]) \u2192 int | None </p> <p>Run a command asynchronously.</p> <p> source async alog(reader: StreamReader, write: Callable[[str], None]) \u2192 None </p> <p>Log a stream of output asynchronously.</p>"},{"location":"api/hydraflow/executor/conf/","title":"hydraflow.executor.conf","text":"hydraflow.executor.conf<p> source module hydraflow.executor.conf </p> <p>Configuration dataclasses for Hydraflow executor.</p> <p> Classes </p> <ul> <li> <p>Set \u2014 A set of commands or actions to be executed.</p> </li> <li> <p>Job \u2014 A job configuration containing various commands and sets.</p> </li> <li> <p>HydraflowConf \u2014 Configuration for Hydraflow executor.</p> </li> </ul> <p> source dataclass Set(each: str = '', all: str = '', add: str = '') </p> <p>A set of commands or actions to be executed.</p> <p> source dataclass Job(name: str = '', run: str = '', call: str = '', submit: str = '', add: str = '', sets: list[Set] = field(default_factory=list)) </p> <p>A job configuration containing various commands and sets.</p> <p> source dataclass HydraflowConf(jobs: dict[str, Job] = field(default_factory=dict)) </p> <p>Configuration for Hydraflow executor.</p>"},{"location":"api/hydraflow/executor/io/","title":"hydraflow.executor.io","text":"hydraflow.executor.io<p> source module hydraflow.executor.io </p> <p>Hydraflow jobs IO.</p> <p> Functions </p> <ul> <li> <p>load_config \u2014 Load the hydraflow config.</p> </li> <li> <p>get_job \u2014 Get a job from the config.</p> </li> </ul> <p> source load_config(config_file: str | Path = 'hydraflow.yaml') \u2192 HydraflowConf </p> <p>Load the hydraflow config.</p> <p> source get_job(name: str, config_file: str | Path = 'hydraflow.yaml') \u2192 Job </p> <p>Get a job from the config.</p>"},{"location":"api/hydraflow/executor/job/","title":"hydraflow.executor.job","text":"hydraflow.executor.job<p> source module hydraflow.executor.job </p> <p>Job execution and argument handling for HydraFlow.</p> <p>This module provides functionality for executing jobs in HydraFlow, including:</p> <ul> <li>Argument parsing and expansion for job parameter sets</li> <li>Batch processing of Hydra configurations</li> <li>Execution of jobs via shell commands or Python functions</li> </ul> <p>The module supports two execution modes:</p> <ol> <li>Shell command execution</li> <li>Python function calls</li> </ol> <p>Each job can consist of multiple parameter sets, and each parameter set can have its own arguments and configurations that will be expanded into multiple runs.</p> <p> Classes </p> <ul> <li> <p>Task \u2014 A task to be executed.</p> </li> <li> <p>Call \u2014 A call to be executed.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>iter_args \u2014 Iterate over combinations generated from parsed arguments.</p> </li> <li> <p>iter_batches \u2014 Generate Hydra application arguments for a job.</p> </li> <li> <p>merge_args \u2014 Merge two lists of arguments.</p> </li> <li> <p>iter_tasks \u2014 Yield tasks of a job to be executed using a shell command.</p> </li> <li> <p>iter_calls \u2014 Yield calls of a job to be executed using a Python function.</p> </li> <li> <p>submit \u2014 Submit entire job using a shell command.</p> </li> <li> <p>get_callable \u2014 Get a callable from a function name.</p> </li> </ul> <p> source iter_args(each: str, all_: str) \u2192 Iterator[list[str]] </p> <p>Iterate over combinations generated from parsed arguments.</p> <p>Generate all possible combinations of arguments by parsing and expanding each one, yielding them as an iterator.</p> <p> Parameters </p> <ul> <li> <p>each :  str \u2014 The 'each' parameter to parse.</p> </li> <li> <p>all_ :  str \u2014 The 'all' parameter to parse.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>list[str] \u2014 a list of the parsed argument combinations.</p> </li> </ul> <p> source iter_batches(job: Job) \u2192 Iterator[list[str]] </p> <p>Generate Hydra application arguments for a job.</p> <p>This function generates a list of Hydra application arguments for a given job, including the job name and the root directory for the sweep.</p> <p> Parameters </p> <ul> <li> <p>job :  Job \u2014 The job to generate the Hydra configuration for.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>str \u2014 Hydra configuration strings.</p> </li> </ul> <p> source merge_args(first: list[str], second: list[str]) \u2192 list[str] </p> <p>Merge two lists of arguments.</p> <p>This function merges two lists of arguments by checking for conflicts and resolving them by keeping the values from the second list.</p> <p> Parameters </p> <ul> <li> <p>first :  list[str] \u2014 The first list of arguments.</p> </li> <li> <p>second :  list[str] \u2014 The second list of arguments.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 A merged list of arguments.</p> </li> </ul> <p> source dataclass Task(args: list[str], total: int, index: int) </p> <p>A task to be executed.</p> <p> source dataclass Call(args: list[str], total: int, index: int, func: Callable[[], Any]) </p> <p>Bases : Task</p> <p>A call to be executed.</p> <p> source iter_tasks(args: list[str], iterable: Iterable[list[str]]) \u2192 Iterator[Task] </p> <p>Yield tasks of a job to be executed using a shell command.</p> <p> source iter_calls(args: list[str], iterable: Iterable[list[str]]) \u2192 Iterator[Call] </p> <p>Yield calls of a job to be executed using a Python function.</p> <p> source submit(args: list[str], iterable: Iterable[list[str]], *, dry_run: bool = False) \u2192 CompletedProcess[bytes] | tuple[list[str], str] </p> <p>Submit entire job using a shell command.</p> <p> source get_callable(name: str) \u2192 Callable[[list[str]], Any] </p> <p>Get a callable from a function name.</p> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If the name of the function is invalid.</p> </li> </ul>"},{"location":"api/hydraflow/executor/parser/","title":"hydraflow.executor.parser","text":"hydraflow.executor.parser<p> source module hydraflow.executor.parser </p> <p>Parse and convert string representations of numbers and ranges.</p> <p>This module provides utility functions for parsing and converting string representations of numbers and ranges. It includes functions to convert strings to numbers, count decimal places, handle numeric ranges, and expand values from string arguments.</p> <p> Functions </p> <ul> <li> <p>to_number \u2014 Convert a string to an integer or float.</p> </li> <li> <p>count_decimal_digits \u2014 Count decimal places in a string.</p> </li> <li> <p>count_integer_digits \u2014 Count the number of digits in the integer part of a number.</p> </li> <li> <p>is_number \u2014 Check if a string represents a valid number.</p> </li> <li> <p>split_suffix \u2014 Split a string into the prefix and suffix.</p> </li> <li> <p>add_exponent \u2014 Append an exponent to a value string.</p> </li> <li> <p>split_parentheses \u2014 Split a string with parentheses into a list of strings.</p> </li> <li> <p>collect_parentheses \u2014 Collect values from a string with parentheses.</p> </li> <li> <p>collect_values \u2014 Collect a list of values from a range argument.</p> </li> <li> <p>split \u2014 Split a string by top-level commas.</p> </li> <li> <p>expand_values \u2014 Expand a string argument into a list of values.</p> </li> <li> <p>split_arg \u2014 Split an argument into a key, suffix, and value.</p> </li> <li> <p>collect_arg \u2014 Collect a string of expanded key-value pairs.</p> </li> <li> <p>expand_arg \u2014 Parse a string argument into a list of values.</p> </li> <li> <p>collect \u2014 Collect a list of arguments into a list of strings.</p> </li> <li> <p>expand \u2014 Expand a list of arguments into a list of lists of strings.</p> </li> </ul> <p> source to_number(x: str) \u2192 int | float </p> <p>Convert a string to an integer or float.</p> <p>Attempts to convert a string to an integer or a float, returning 0 if the string is empty or cannot be converted.</p> <p> Parameters </p> <ul> <li> <p>x :  str \u2014 The string to convert.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int | float \u2014 The converted number as an integer or float.</p> </li> </ul> <p> Examples </p> <pre><code>type(to_number(\"1\"))\n</code></pre> <pre><code>&lt;class 'int'&gt;\n</code></pre> <pre><code>type(to_number(\"1.2\"))\n</code></pre> <pre><code>&lt;class 'float'&gt;\n</code></pre> <pre><code>to_number(\"\")\n</code></pre> <pre><code>0\n</code></pre> <pre><code>to_number(\"1e-3\")\n</code></pre> <pre><code>0.001\n</code></pre> <p> source count_decimal_digits(x: str) \u2192 int </p> <p>Count decimal places in a string.</p> <p>Examine a string representing a number and returns the count of decimal places present after the decimal point. Return 0 if no decimal point is found.</p> <p> Parameters </p> <ul> <li> <p>x :  str \u2014 The string to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The number of decimal places.</p> </li> </ul> <p> Examples </p> <pre><code>count_decimal_digits(\"1\")\n</code></pre> <pre><code>0\n</code></pre> <pre><code>count_decimal_digits(\"-1.2\")\n</code></pre> <pre><code>1\n</code></pre> <pre><code>count_decimal_digits(\"1.234\")\n</code></pre> <pre><code>3\n</code></pre> <pre><code>count_decimal_digits(\"-1.234e-10\")\n</code></pre> <pre><code>3\n</code></pre> <p> source count_integer_digits(num_str: str) \u2192 int </p> <p>Count the number of digits in the integer part of a number.</p> <p>Consider only the integer part of a number, even if it is in scientific notation.</p> <p> Parameters </p> <ul> <li> <p>num_str :  str \u2014 The string representing a number.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The number of digits in the integer part of a number. (excluding the sign)</p> </li> </ul> <p> Examples </p> <pre><code>count_integer_digits(\"123\")\n</code></pre> <pre><code>3\n</code></pre> <pre><code>count_integer_digits(\"-123.45\")\n</code></pre> <pre><code>3\n</code></pre> <pre><code>count_integer_digits(\"+0.00123\")\n</code></pre> <pre><code>1\n</code></pre> <pre><code>count_integer_digits(\"-1.200\")\n</code></pre> <pre><code>1\n</code></pre> <pre><code>count_integer_digits(\"+1.20e3\")\n</code></pre> <pre><code>1\n</code></pre> <pre><code>count_integer_digits(\"-0.120e-3\")\n</code></pre> <pre><code>1\n</code></pre> <pre><code>count_integer_digits(\".123\")\n</code></pre> <pre><code>0\n</code></pre> <p> source is_number(x: str) \u2192 bool </p> <p>Check if a string represents a valid number.</p> <p> Parameters </p> <ul> <li> <p>x :  str \u2014 The string to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 True if the string is a number, False otherwise.</p> </li> </ul> <p> Examples </p> <pre><code>is_number(\"1\")\n</code></pre> <pre><code>True\n</code></pre> <pre><code>is_number(\"-1.2\")\n</code></pre> <pre><code>True\n</code></pre> <pre><code>is_number(\"1.2.3\")\n</code></pre> <pre><code>False\n</code></pre> <p> source split_suffix(arg: str) \u2192 tuple[str, str] </p> <p>Split a string into the prefix and suffix.</p> <p>The suffix is the part of the string that starts with a colon (:). The prefix is the part of the string that precedes the suffix.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The string to split.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple containing the prefix and suffix.</p> </li> </ul> <p> Examples </p> <pre><code>split_suffix(\"1:k\")\n</code></pre> <pre><code>('1', 'e3')\n</code></pre> <pre><code>split_suffix(\"1:2:k\")\n</code></pre> <pre><code>('1:2', 'e3')\n</code></pre> <pre><code>split_suffix(\"1:2:M\")\n</code></pre> <pre><code>('1:2', 'e6')\n</code></pre> <pre><code>split_suffix(\":1:2:M\")\n</code></pre> <pre><code>(':1:2', 'e6')\n</code></pre> <p> source add_exponent(value: str, exponent: str) \u2192 str </p> <p>Append an exponent to a value string.</p> <p> Parameters </p> <ul> <li> <p>value :  str \u2014 The value to modify.</p> </li> <li> <p>exponent :  str \u2014 The exponent to append.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The value with the exponent added.</p> </li> </ul> <p> Examples </p> <pre><code>add_exponent(\"1\", \"e3\")\n</code></pre> <pre><code>'1e3'\n</code></pre> <pre><code>add_exponent(\"1\", \"\")\n</code></pre> <pre><code>'1'\n</code></pre> <pre><code>add_exponent(\"0\", \"e-3\")\n</code></pre> <pre><code>'0'\n</code></pre> <pre><code>add_exponent(\"0.0\", \"e-3\")\n</code></pre> <pre><code>'0.0'\n</code></pre> <p> source split_parentheses(arg: str) \u2192 Iterator[str] </p> <p>Split a string with parentheses into a list of strings.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The string to split.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>str \u2014 The split strings.</p> </li> </ul> <p> Examples </p> <pre><code>list(split_parentheses(\"a(b,c)m(e:f)k\"))\n</code></pre> <pre><code>['a', 'b,c', 'e-3', 'e:f', 'e3']\n</code></pre> <pre><code>list(split_parentheses(\"(b,c)d(e:f)\"))\n</code></pre> <pre><code>['b,c', 'd', 'e:f']\n</code></pre> <p> source collect_parentheses(arg: str) \u2192 list[str] </p> <p>Collect values from a string with parentheses.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The string to collect values from.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 A list of the collected values.</p> </li> </ul> <p> Examples </p> <pre><code>collect_parentheses(\"(1:3,5:9:2,20)k\")\n</code></pre> <pre><code>['1e3', '2e3', '3e3', '5e3', '7e3', '9e3', '20e3']\n</code></pre> <pre><code>collect_parentheses(\"2e(-1,-2,-3)\")\n</code></pre> <pre><code>['2e-1', '2e-2', '2e-3']\n</code></pre> <pre><code>collect_parentheses(\"(1:3)e(3,5)\")\n</code></pre> <pre><code>['1e3', '2e3', '3e3', '1e5', '2e5', '3e5']\n</code></pre> <p> source collect_values(arg: str) \u2192 list[str] </p> <p>Collect a list of values from a range argument.</p> <p>Collect all individual values within a numeric range represented by a string (e.g., <code>1:4</code>) and return them as a list of strings. Support both integer and floating-point ranges.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The argument to collect.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 A list of the collected values.</p> </li> </ul> <p> Examples </p> <pre><code>collect_values(\"1:4\")\n</code></pre> <pre><code>['1', '2', '3', '4']\n</code></pre> <pre><code>collect_values(\"1.2:1.4:0.1:k\")\n</code></pre> <pre><code>['1.2e3', '1.3e3', '1.4e3']\n</code></pre> <pre><code>collect_values(\"0.1\")\n</code></pre> <pre><code>['0.1']\n</code></pre> <pre><code>collect_values(\"4:M\")\n</code></pre> <pre><code>['4e6']\n</code></pre> <pre><code>collect_values(\"(1:3,5:7)M\")\n</code></pre> <pre><code>['1e6', '2e6', '3e6', '5e6', '6e6', '7e6']\n</code></pre> <pre><code>collect_values(\"(1,5)e-(1:3)\")\n</code></pre> <pre><code>['1e-1', '5e-1', '1e-2', '5e-2', '1e-3', '5e-3']\n</code></pre> <p> source split(arg: str) \u2192 list[str] </p> <p>Split a string by top-level commas.</p> <p>Splits a string by commas while respecting nested structures. Commas inside brackets and quotes are ignored, only splitting at the top-level commas.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The string to split.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 A list of split strings.</p> </li> </ul> <p> Examples </p> <pre><code>split(\"[a,1],[b,2]\")\n</code></pre> <pre><code>['[a,1]', '[b,2]']\n</code></pre> <pre><code>split('\"x,y\",z')\n</code></pre> <pre><code>['\"x,y\"', 'z']\n</code></pre> <pre><code>split(\"'p,q',r\")\n</code></pre> <pre><code>[\"'p,q'\", 'r']\n</code></pre> <pre><code>split(\"(a,b)m,(1,2:4)k\")\n</code></pre> <pre><code>['(a,b)m', '(1,2:4)k']\n</code></pre> <p> source expand_values(arg: str, suffix: str = '') \u2192 Iterator[str] </p> <p>Expand a string argument into a list of values.</p> <p>Take a string containing comma-separated values or ranges and return a list of all individual values. Handle numeric ranges and special characters.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The argument to expand.</p> </li> <li> <p>suffix :  str \u2014 The suffix to append to each value.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>str \u2014 The expanded values.</p> </li> </ul> <p> source split_arg(arg: str) \u2192 tuple[str, str, str] </p> <p>Split an argument into a key, suffix, and value.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The argument to split.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str, str] \u2014 A tuple containing the key, suffix, and value.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 If the argument is invalid.</p> </li> </ul> <p> source collect_arg(arg: str) \u2192 str </p> <p>Collect a string of expanded key-value pairs.</p> <p>Take a key-value pair argument and concatenates all expanded values with commas, returning a single string suitable for command-line usage.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The argument to collect.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 A string of the collected key and values.</p> </li> </ul> <p> source expand_arg(arg: str) \u2192 Iterator[str] </p> <p>Parse a string argument into a list of values.</p> <p>Responsible for parsing a string that may contain multiple arguments separated by pipes (\"|\") and returns a list of all expanded arguments.</p> <p> Parameters </p> <ul> <li> <p>arg :  str \u2014 The argument to parse.</p> </li> </ul> <p> Yields </p> <ul> <li> <p>str \u2014 Parsed arguments.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014 xx</p> </li> </ul> <p> source collect(args: str | list[str]) \u2192 list[str] </p> <p>Collect a list of arguments into a list of strings.</p> <p> Parameters </p> <ul> <li> <p>args :  list[str] \u2014 The arguments to collect.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 A list of the collected arguments.</p> </li> </ul> <p> source expand(args: str | list[str]) \u2192 list[list[str]] </p> <p>Expand a list of arguments into a list of lists of strings.</p> <p> Parameters </p> <ul> <li> <p>args :  list[str] \u2014 The arguments to expand.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[list[str]] \u2014 A list of the expanded arguments.</p> </li> </ul>"},{"location":"api/hydraflow/cli/","title":"hydraflow.cli","text":"hydraflow.cli<p> source module hydraflow.cli </p> <p>Hydraflow CLI.</p> <p> Functions </p> <ul> <li> <p>run \u2014 Run a job.</p> </li> <li> <p>call \u2014 Call a job.</p> </li> <li> <p>submit \u2014 Submit a job.</p> </li> <li> <p>show \u2014 Show the hydraflow config.</p> </li> <li> <p>callback \u2014 Hydraflow CLI.</p> </li> </ul> <p> source run(job: Job, args: list[str], *, dry_run: bool) \u2192 None </p> <p>Run a job.</p> <p> Raises </p> <ul> <li> <p>Exit</p> </li> </ul> <p> source call(job: Job, args: list[str], *, dry_run: bool) \u2192 None </p> <p>Call a job.</p> <p> Raises </p> <ul> <li> <p>Exit</p> </li> </ul> <p> source submit(job: Job, args: list[str], *, dry_run: bool) \u2192 None </p> <p>Submit a job.</p> <p> source show(name: Annotated[str, Argument(help='Job name.', show_default=False)] = '') \u2192 None </p> <p>Show the hydraflow config.</p> <p> source callback(*, version: Annotated[bool, Option('--version', help='Show the version and exit.')] = False) \u2192 None </p> <p>Hydraflow CLI.</p> <p> Raises </p> <ul> <li> <p>Exit</p> </li> </ul>"}]}